{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Self-attention is the core mechanism that allows language models to understand relationships between words in text.\n",
    "\n",
    "1. Each word in a sentence gets converted to a vector (embedding)\n",
    "2. The model calculates how much attention each word should pay to every other word\n",
    "3. This creates an \"attention matrix\" showing these relationships\n",
    "4. Words get updated based on their connections to other words\n",
    "\n",
    "For example, in \"The cat sat on the mat\", when processing \"cat\", the model pays attention to \"sat\" to understand what the cat is doing, creating these meaningful connections automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 #hyperparameter\n",
    "\n",
    "#every single token emits two vector, one for key and one for query\n",
    "key = nn.Linear(C, head_size, bias=False)#what do i contain?\n",
    "query = nn.Linear(C, head_size, bias=False)#what am i looking for?\n",
    "#my query dot products with all the keys of other tokens\n",
    "#if key and wuery are aligned they'll interact to a very hihg value\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "\n",
    "k = key(x)   # (B, T, 16)\n",
    "q = query(x) # (B, T, 16)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1)# transpose last two dimension # (B, T, 16) @ (B, 16, T) ---> (B, T, T)# for every row of B, we have a matrix affinities of TxT\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x) #why doing this?\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

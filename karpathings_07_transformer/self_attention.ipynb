{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Attention\n",
    "\n",
    "Self-attention is the core mechanism that allows language models to understand relationships between words in text.\n",
    "\n",
    "1. Each word in a sentence gets converted to a vector (embedding)\n",
    "2. The model calculates how much attention each word should pay to every other word\n",
    "3. This creates an \"attention matrix\" showing these relationships\n",
    "4. Words get updated based on their connections to other words\n",
    "\n",
    "For example, in \"The cat sat on the mat\", when processing \"cat\", the model pays attention to \"sat\" to understand what the cat is doing, creating these meaningful connections automatically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 16])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 4: self-attention!\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time, channels\n",
    "x = torch.randn(B,T,C)\n",
    "\n",
    "# let's see a single Head perform self-attention\n",
    "head_size = 16 #hyperparameter\n",
    "\n",
    "#every single token emits two vector, one for key and one for query\n",
    "key = nn.Linear(C, head_size, bias=False)   # what information do i contain?\n",
    "query = nn.Linear(C, head_size, bias=False)  # what am i looking for in other tokens?\n",
    "value = nn.Linear(C, head_size, bias=False) # what information do i contribute?\n",
    "\n",
    "\n",
    "# Each query vectors dot product with all key vectors to calculate attention scores\n",
    "# If a query and key are aligned (similar direction), they'll produce a high score\n",
    "\n",
    "k = key(x)   # (B, T, 16)- keys for all tokens\n",
    "q = query(x) # (B, T, 16)- queries for all tokens\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1)# transpose last two dimension # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
    "# This gives us a BxTxT tensor where each value at position (i,j) \n",
    "# represents how much token i should attend to token j\n",
    "# For each batch, we get a TxT attention matrix (affinity matrix)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))# mask out future positions\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # normalize scores to sum to 1\n",
    "\n",
    "v = value(x)\n",
    "out = wei @ v\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
    "\n",
    "- There is no notion of space. Attention simply acts over a set of vectors. This is why **we need to positionally encode tokens**.\n",
    "\n",
    "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
    "\n",
    "- In an **\"encoder\"** attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. Useful for example for sentiment analysis. We are **able to see the \"future\"**\n",
    "\n",
    "- **This implementation here is called a \"decoder\"** attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
    "\n",
    "\n",
    "- **\"Self-attention\"** just means that the keys, queries and values are produced from the same source.\n",
    "- In **\"Cross-attention\"** the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
    "\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "\n",
    "\n",
    "This is the standard self-attention formula where:\n",
    "- $Q$ is the query matrix\n",
    "- $K$ is the key matrix\n",
    "- $K^T$ is the transpose of the key matrix\n",
    "- $V$ is the value matrix\n",
    "- $d_k$ is the dimensionality of the keys (scaling factor)\n",
    "- The softmax normalizes the attention weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B,T,head_size)\n",
    "q = torch.randn(B,T,head_size)\n",
    "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "**Why can't LLM spell words**  \n",
    "LLMs see words as token chunks, not letter by letter, so unusual spellings may split into strange token combinations the model hasn't seen together often.  \n",
    "\n",
    "**Why can't LLM do super simple string processing tasks like reversing a string**  \n",
    "LLMs process text as tokens, not individual characters, making character-by-character operations like string reversal difficult.  \n",
    "\n",
    "**Why is LLM worse at non-English languages (e.g. Japanese)**  \n",
    "Languages like Japanese tokenize inefficiently, often requiring more tokens per word than English, giving the model less context to work with.  \n",
    "\n",
    "**Why is LLM bad at simple arithmetic**  \n",
    "Numbers get split into multiple tokens based on digit patterns, making it harder for the model to understand numerical relationships.  \n",
    "\n",
    "**Why did GPT-2 have more than necessary trouble coding in Python**  \n",
    "Code tokens were less common in GPT-2's training data, and programming syntax often gets split in unintuitive ways. For example, indentation spaces occupy a huge part of the context window.  \n",
    "\n",
    "**Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"**  \n",
    "This specific string is a special token used during training to mark the end of documents, so models interpret it as a signal to stop generating.  \n",
    "\n",
    "**Why does the LLM break if I ask it about \"SolidGoldMagikarp\"**  \n",
    "This specific string became famous because in early GPT models, it tokenized into an unusual pattern that caused the model to produce repetitive or nonsensical outputs. A hypothesis is that it's related to a Reddit username.  \n",
    "\n",
    "**Why should I prefer to use YAML over JSON with LLMs**  \n",
    "YAML formatting creates more natural language-like tokens than JSON's special characters and strict syntax.  \n",
    "\n",
    "\n",
    "\n",
    "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six times—probably six times—a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer fooling—this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine times—well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten times—the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\n",
      "length: 1688\n",
      "---\n",
      "[70, 111, 111, 108, 32, 109, 101, 32, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 104, 114, 101, 101, 32, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 101, 32, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 101, 32, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 102, 105, 114, 115, 116, 32, 116, 105, 109, 101, 32, 110, 111, 116, 32, 116, 111, 32, 98, 101, 32, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 101, 32, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 32, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 101, 32, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 101, 32, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 105, 118, 101, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 101, 32, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 111, 32, 116, 97, 107, 101, 32, 115, 111, 109, 101, 32, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 101, 32, 103, 111, 116, 32, 116, 119, 101, 108, 118, 101, 32, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 101, 32, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 101, 32, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 101, 32, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 101, 32, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 101, 32, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 101, 32, 115, 101, 110, 115, 101, 32, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 104, 101, 32, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 101, 32, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 101, 32, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 101, 32, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 101, 32, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 101, 32, 110, 97, 116, 117, 114, 101, 32, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 101, 32, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 101, 32, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 110, 105, 110, 101, 32, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 101, 32, 116, 105, 109, 101, 32, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 101, 32, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 101, 32, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 101, 32, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 101, 32, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 101, 32, 111, 117, 116, 32, 116, 104, 101, 32, 112, 101, 111, 112, 108, 101, 32, 119, 104, 111, 32, 104, 97, 118, 101, 32, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 101, 32, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1698\n"
     ]
    }
   ],
   "source": [
    "#As usual, i change the original text just for fun\n",
    "# Using this awesome bit of James Donald Forbes McCann --> https://www.youtube.com/watch?v=udSMZG_L-S0\n",
    "text=\"Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six times—probably six times—a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer fooling—this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine times—well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten times—the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\"\n",
    "\n",
    "\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, (101, 32)), (55, (32, 116)), (42, (109, 101)), (33, (116, 104)), (30, (32, 97)), (27, (111, 111)), (27, (105, 110)), (25, (111, 108)), (25, (44, 32)), (25, (32, 102)), (23, (104, 101)), (23, (32, 115)), (23, (32, 109)), (22, (111, 117)), (20, (116, 105)), (20, (32, 111)), (19, (116, 32)), (19, (111, 110)), (19, (46, 32)), (18, (115, 32)), (18, (110, 103)), (18, (108, 32)), (18, (102, 111)), (18, (100, 32)), (17, (104, 97)), (15, (114, 101)), (15, (110, 32)), (15, (105, 109)), (15, (101, 115)), (15, (32, 98)), (14, (118, 101)), (14, (111, 32)), (14, (97, 116)), (13, (121, 111)), (13, (103, 32)), (13, (32, 121)), (12, (116, 111)), (12, (114, 32)), (12, (108, 105)), (12, (101, 114)), (12, (97, 110)), (11, (108, 108)), (10, (117, 32)), (10, (110, 100)), (10, (97, 108)), (10, (97, 32)), (10, (70, 111)), (10, (32, 73)), (9, (115, 104)), (9, (115, 101)), (9, (108, 101)), (9, (105, 116)), (9, (32, 119)), (9, (32, 108)), (9, (32, 105)), (9, (32, 70)), (8, (121, 32)), (8, (117, 116)), (8, (108, 121)), (8, (98, 101)), (8, (97, 109)), (7, (119, 105)), (7, (114, 105)), (7, (111, 114)), (7, (105, 115)), (7, (101, 108)), (7, (32, 112)), (7, (32, 103)), (6, (117, 114)), (6, (116, 97)), (6, (115, 105)), (6, (111, 102)), (6, (110, 101)), (6, (107, 105)), (6, (105, 99)), (6, (102, 32)), (6, (101, 110)), (6, (98, 117)), (6, (32, 110)), (5, (226, 128)), (5, (128, 148)), (5, (117, 115)), (5, (117, 108)), (5, (115, 116)), (5, (115, 111)), (5, (115, 44)), (5, (110, 111)), (5, (108, 111)), (5, (105, 118)), (5, (104, 105)), (5, (104, 32)), (5, (103, 111)), (5, (101, 118)), (5, (101, 100)), (5, (101, 46)), (5, (97, 114)), (5, (73, 32)), (5, (32, 114)), (5, (32, 104)), (4, (121, 105)), (4, (121, 44)), (4, (117, 97)), (4, (116, 121)), (4, (116, 117)), (4, (116, 101)), (4, (115, 226)), (4, (114, 115)), (4, (114, 111)), (4, (114, 97)), (4, (112, 114)), (4, (112, 101)), (4, (111, 116)), (4, (111, 109)), (4, (110, 115)), (4, (109, 97)), (4, (107, 101)), (4, (105, 111)), (4, (105, 108)), (4, (105, 100)), (4, (101, 101)), (4, (101, 44)), (4, (99, 107)), (4, (98, 108)), (4, (97, 118)), (4, (97, 98)), (4, (73, 39)), (4, (39, 115)), (4, (32, 117)), (4, (32, 99)), (3, (120, 32)), (3, (119, 101)), (3, (118, 105)), (3, (116, 119)), (3, (114, 121)), (3, (112, 32)), (3, (110, 97)), (3, (110, 46)), (3, (109, 32)), (3, (108, 46)), (3, (108, 44)), (3, (105, 120)), (3, (105, 112)), (3, (105, 107)), (3, (103, 104)), (3, (103, 101)), (3, (102, 105)), (3, (101, 116)), (3, (101, 109)), (3, (101, 99)), (3, (100, 105)), (3, (99, 104)), (3, (99, 101)), (3, (99, 97)), (3, (97, 115)), (3, (97, 105)), (3, (97, 99)), (3, (39, 114)), (3, (32, 107)), (3, (32, 101)), (2, (148, 116)), (2, (121, 46)), (2, (119, 114)), (2, (119, 104)), (2, (119, 32)), (2, (118, 117)), (2, (117, 39)), (2, (116, 114)), (2, (116, 46)), (2, (116, 44)), (2, (116, 39)), (2, (114, 116)), (2, (112, 111)), (2, (111, 119)), (2, (111, 118)), (2, (111, 98)), (2, (110, 116)), (2, (110, 110)), (2, (110, 105)), (2, (110, 44)), (2, (109, 121)), (2, (108, 117)), (2, (108, 115)), (2, (108, 110)), (2, (108, 100)), (2, (107, 32)), (2, (105, 114)), (2, (105, 103)), (2, (105, 101)), (2, (104, 116)), (2, (104, 114)), (2, (104, 111)), (2, (103, 105)), (2, (103, 97)), (2, (102, 114)), (2, (102, 97)), (2, (101, 105)), (2, (101, 97)), (2, (101, 39)), (2, (100, 46)), (2, (99, 117)), (2, (99, 116)), (2, (99, 114)), (2, (98, 97)), (2, (97, 119)), (2, (97, 107)), (2, (97, 103)), (2, (89, 111)), (2, (66, 117)), (2, (65, 110)), (2, (63, 32)), (2, (59, 32)), (2, (45, 116)), (2, (39, 109)), (2, (32, 118)), (2, (32, 89)), (2, (32, 66)), (2, (32, 65)), (1, (148, 119)), (1, (148, 112)), (1, (148, 97)), (1, (121, 115)), (1, (121, 112)), (1, (121, 39)), (1, (119, 111)), (1, (118, 111)), (1, (118, 97)), (1, (117, 112)), (1, (117, 110)), (1, (117, 105)), (1, (117, 103)), (1, (117, 101)), (1, (117, 100)), (1, (117, 99)), (1, (117, 46)), (1, (117, 44)), (1, (116, 116)), (1, (115, 121)), (1, (115, 112)), (1, (115, 108)), (1, (115, 99)), (1, (115, 97)), (1, (115, 63)), (1, (115, 46)), (1, (114, 117)), (1, (114, 110)), (1, (114, 108)), (1, (114, 45)), (1, (112, 108)), (1, (112, 105)), (1, (112, 97)), (1, (112, 46)), (1, (112, 44)), (1, (111, 112)), (1, (111, 107)), (1, (111, 105)), (1, (111, 101)), (1, (111, 100)), (1, (111, 99)), (1, (110, 121)), (1, (110, 112)), (1, (110, 108)), (1, (110, 99)), (1, (110, 98)), (1, (110, 59)), (1, (110, 45)), (1, (109, 117)), (1, (109, 112)), (1, (109, 111)), (1, (109, 98)), (1, (108, 118)), (1, (108, 116)), (1, (108, 99)), (1, (107, 44)), (1, (106, 117)), (1, (105, 98)), (1, (105, 97)), (1, (103, 226)), (1, (103, 45)), (1, (103, 44)), (1, (102, 108)), (1, (101, 121)), (1, (101, 111)), (1, (101, 103)), (1, (101, 102)), (1, (101, 63)), (1, (101, 59)), (1, (101, 45)), (1, (100, 117)), (1, (100, 44)), (1, (99, 111)), (1, (99, 32)), (1, (98, 121)), (1, (98, 118)), (1, (98, 114)), (1, (98, 105)), (1, (97, 117)), (1, (97, 112)), (1, (85, 110)), (1, (85, 83)), (1, (84, 104)), (1, (83, 116)), (1, (83, 83)), (1, (83, 82)), (1, (82, 32)), (1, (73, 116)), (1, (68, 101)), (1, (45, 111)), (1, (45, 85)), (1, (39, 118)), (1, (39, 100)), (1, (32, 106)), (1, (32, 100)), (1, (32, 85)), (1, (32, 84)), (1, (32, 83)), (1, (32, 68))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\"\n",
    "    Counts occurrences of consecutive element pairs in the given list.\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {}  # Dictionary to store the count of each consecutive pair\n",
    "    \n",
    "    # Iterate through consecutive elements\n",
    "    for pair in zip(ids, ids[1:]):  \n",
    "        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for the pair, initializing to 0 if not present\n",
    "    \n",
    "    return counts \n",
    "\n",
    "stats = get_stats(tokens)\n",
    "#print(stats)\n",
    "print(sorted(((value,key) for key,value in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 111, 111, 108, 32, 109, 256, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 104, 114, 101, 256, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 256, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 256, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 256, 102, 105, 114, 115, 116, 32, 116, 105, 109, 256, 110, 111, 116, 32, 116, 111, 32, 98, 256, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 256, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 256, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 256, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 256, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 105, 118, 256, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 256, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 111, 32, 116, 97, 107, 256, 115, 111, 109, 256, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 256, 103, 111, 116, 32, 116, 119, 101, 108, 118, 256, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 256, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 256, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 256, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 256, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 256, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 256, 115, 101, 110, 115, 256, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 256, 116, 104, 256, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 104, 256, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 256, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 256, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 256, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 256, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 256, 110, 97, 116, 117, 114, 256, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 256, 116, 104, 256, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 256, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 256, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 110, 105, 110, 256, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 256, 116, 105, 109, 256, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 256, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 256, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 256, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 256, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 256, 111, 117, 116, 32, 116, 104, 256, 112, 101, 111, 112, 108, 256, 119, 104, 111, 32, 104, 97, 118, 256, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 256, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1624\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i keep the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #0 (101, 32) `e ` --> 256 `Ā`\n",
      "Merge #1 (32, 116) ` t` --> 257 `ā`\n",
      "Merge #2 (111, 111) `oo` --> 258 `Ă`\n",
      "Merge #3 (105, 110) `in` --> 259 `ă`\n",
      "Merge #4 (32, 97) ` a` --> 260 `Ą`\n",
      "Merge #5 (258, 108) `Ăl` --> 261 `ą`\n",
      "Merge #6 (32, 109) ` m` --> 262 `Ć`\n",
      "Merge #7 (111, 117) `ou` --> 263 `ć`\n",
      "Merge #8 (111, 110) `on` --> 264 `Ĉ`\n",
      "Merge #9 (46, 32) `. ` --> 265 `ĉ`\n",
      "Merge #10 (257, 104) `āh` --> 266 `Ċ`\n",
      "Merge #11 (44, 32) `, ` --> 267 `ċ`\n",
      "Merge #12 (100, 32) `d ` --> 268 `Č`\n",
      "Merge #13 (259, 103) `ăg` --> 269 `č`\n",
      "Merge #14 (104, 97) `ha` --> 270 `Ď`\n",
      "Merge #15 (105, 109) `im` --> 271 `ď`\n",
      "Merge #16 (101, 115) `es` --> 272 `Đ`\n",
      "Merge #17 (256, 116) `Āt` --> 273 `đ`\n",
      "Merge #18 (102, 261) `fą` --> 274 `Ē`\n",
      "Merge #19 (121, 263) `yć` --> 275 `ē`\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"Merge #{i} {pair} `{chr(pair[0])}{chr(pair[1])}` --> {idx} `{chr(idx)}`\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1698\n",
      "ids length: 1243\n",
      "compression ratio: 1.37X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### decoding\n",
    "\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids) #concatenate bytes\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")# errors=\"strict\"\n",
    "  return text\n",
    "\n",
    "print(decode([97]))\n",
    "\n",
    "#not every "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### encoding\n",
    "\n",
    "The other way around: Given a string, what are the tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Encodes a given string into a list of integer tokens using a merge-based encoding scheme.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input string into a list of UTF-8 byte values (tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    # Continue merging tokens until no more valid merges exist\n",
    "    while len(tokens) >= 2:  # Need at least 2 tokens to merge\n",
    "        # Get the frequency of all consecutive token pairs\n",
    "        stats = get_stats(tokens)\n",
    "        # Find the pair with the lowest index in the `merges` dictionary (or inf if not in merges)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "\n",
    "        # If the selected pair is not in the `merges` dictionary, stop merging\n",
    "        if pair not in merges:\n",
    "            break  \n",
    "\n",
    "        # Get the index assigned to this pair from the `merges` dictionary\n",
    "        idx = merges[pair]\n",
    "\n",
    "        # Merge the selected pair into a single token\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "\n",
    "    return tokens  # Return the final list of merged tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 261, 262, 256, 264, 99, 101]\n",
      "FąĆĀĈce\n",
      "Fool me once\n"
     ]
    }
   ],
   "source": [
    "# Example usages:\n",
    "print(encode(\"Fool me once\"))  # Should return an empty list since input is empty\n",
    "print(\"\".join(chr(c) for c in encode(\"Fool me once\"))) \n",
    "print(decode(encode(\"Fool me once\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# trying with another bit https://www.youtube.com/watch?v=7akJJ3Ddxb0\n",
    "valtext = \"I would never call a woman crazy to her face; they hate it, and it makes them go insane. The word 'crazy' can mean too many things. When a man calls a former girlfriend crazy, it could either mean she ripped the head off a rabbit and threw it at my door, threatened to kill me, and burnt all of my things, or it could mean she got a little too upset when she found out I was sleeping with a prostitute. It could mean anything to a man. We have a term for us as well that's too broad: 'creepy.' Creepy could be a man who has trouble looking you in the eye and stands a bit too close on a bus, or it could be a man who rapes his whole family in a dungeon prison under his house. It's too broad a term. Ladies, 'he's a creepy guy'—does he not wash, or does he kill? It's the sort of crazy use of language we've been complaining about.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced splits using regex patterns (GPT series)\n",
    "\n",
    "\n",
    "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "We observed BPE including many versions of common words like ```dog``` since they occur in many variations such as ```dog.``` ```dog!```\n",
    "```dog?```. This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding\n",
    "only minimal fragmentation of words across multiple vocab tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gpt-2\n",
    "\n",
    "\n",
    "``` python\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. **Contractions**\n",
    "\n",
    "```python\n",
    "'s|'t|'re|'ve|'m|'ll|'d\n",
    "```\n",
    "\n",
    "Matches: Common English contractions like 's, 't, 're, etc.\n",
    "\n",
    "Purpose: Ensures contractions are split into separate tokens (e.g., \"don't\" → [\"don\", \"'t\"]).\n",
    "\n",
    "\n",
    "#### 2. **Words**  \n",
    "```python\n",
    "?\\p{L}+\n",
    "```\n",
    "\n",
    "?: Optional leading space (matches 0 or 1 space).\n",
    "\n",
    "\\p{L}+: One or more Unicode letters.\n",
    "\n",
    "Matches: Words with or without a leading space (e.g., \"hello\" or \" hello\").\n",
    "\n",
    "#### 3. **Numbers**  \n",
    "\n",
    "```python\n",
    "?\\p{N}+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "\\p{N}+: One or more Unicode numbers.\n",
    "\n",
    "Matches: Numbers with or without a leading space (e.g., \"123\" or \" 456\").\n",
    "\n",
    "\n",
    "### 4 Symbols/Punctuation\n",
    "\n",
    "```python\n",
    "?[^\\s\\p{L}\\p{N}]+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "[^\\s\\p{L}\\p{N}]: Any character that is not whitespace, a letter, or a number.\n",
    "\n",
    "Matches: Symbols, punctuation, or emojis with optional leading space (e.g., \"!\", \" 😀\").\n",
    "\n",
    "### 5 Trailing whitespace\n",
    "\n",
    "```python\n",
    "\\s+(?!\\S)\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "(?!\\S): Negative lookahead to ensure whitespace is not followed by non-whitespace.\n",
    "\n",
    "Matches: Whitespace at the end of a string or line (e.g., \" \" in \"hello \").\n",
    "\n",
    "### 6 General Whitespace\n",
    "\n",
    "```python\n",
    "\\s+\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "Matches: Remaining whitespace not captured by earlier rules (e.g., spaces between words).\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "**Order Matters**: The regex engine tries alternatives from left to right. Contractions are prioritized first.\n",
    "\n",
    "**Single vs. Multiple Spaces**:\n",
    "\n",
    "A single leading space is included with a word/number/symbol (e.g., \" hello\" → one token).\n",
    "\n",
    "Multiple spaces are split into separate tokens (e.g., \" hello\" → [\" \", \"hello\"]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install regex\n",
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ve\", ' eating', ' 3', ' apples']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I've eating 3 apples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'VE', ' EATING', ' 3', ' APPLES']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I'VE EATING 3 APPLES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'example', ' =', ' \"', '\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n       ', ' \"', '\\n', 'print', '(', 're', '.', 'findall', '(', 'gpt', '2', 'pat', ',', ' example', '))', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "example = \"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "        \"\n",
    "print(re.findall(gpt2pat, example))\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download vocab.bpe and encoder.json from the gpt2 repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 11:29:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: ‘vocab.bpe’\n",
      "\n",
      "vocab.bpe           100%[===================>] 445,62K   550KB/s    in 0,8s    \n",
      "\n",
      "2025-03-11 11:29:46 (550 KB/s) - ‘vocab.bpe’ saved [456318/456318]\n",
      "\n",
      "--2025-03-11 11:29:46--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: ‘encoder.json’\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   802KB/s    in 1,3s    \n",
      "\n",
      "2025-03-11 11:29:48 (802 KB/s) - ‘encoder.json’ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\"\n",
    "\n",
    "\n",
    "#with enconder and merges we can represent a tokenizer, encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " '¡': 94,\n",
       " '¢': 95,\n",
       " '£': 96,\n",
       " '¤': 97,\n",
       " '¥': 98,\n",
       " '¦': 99,\n",
       " '§': 100,\n",
       " '¨': 101,\n",
       " '©': 102,\n",
       " 'ª': 103,\n",
       " '«': 104,\n",
       " '¬': 105,\n",
       " '®': 106,\n",
       " '¯': 107,\n",
       " '°': 108,\n",
       " '±': 109,\n",
       " '²': 110,\n",
       " '³': 111,\n",
       " '´': 112,\n",
       " 'µ': 113,\n",
       " '¶': 114,\n",
       " '·': 115,\n",
       " '¸': 116,\n",
       " '¹': 117,\n",
       " 'º': 118,\n",
       " '»': 119,\n",
       " '¼': 120,\n",
       " '½': 121,\n",
       " '¾': 122,\n",
       " '¿': 123,\n",
       " 'À': 124,\n",
       " 'Á': 125,\n",
       " 'Â': 126,\n",
       " 'Ã': 127,\n",
       " 'Ä': 128,\n",
       " 'Å': 129,\n",
       " 'Æ': 130,\n",
       " 'Ç': 131,\n",
       " 'È': 132,\n",
       " 'É': 133,\n",
       " 'Ê': 134,\n",
       " 'Ë': 135,\n",
       " 'Ì': 136,\n",
       " 'Í': 137,\n",
       " 'Î': 138,\n",
       " 'Ï': 139,\n",
       " 'Ð': 140,\n",
       " 'Ñ': 141,\n",
       " 'Ò': 142,\n",
       " 'Ó': 143,\n",
       " 'Ô': 144,\n",
       " 'Õ': 145,\n",
       " 'Ö': 146,\n",
       " '×': 147,\n",
       " 'Ø': 148,\n",
       " 'Ù': 149,\n",
       " 'Ú': 150,\n",
       " 'Û': 151,\n",
       " 'Ü': 152,\n",
       " 'Ý': 153,\n",
       " 'Þ': 154,\n",
       " 'ß': 155,\n",
       " 'à': 156,\n",
       " 'á': 157,\n",
       " 'â': 158,\n",
       " 'ã': 159,\n",
       " 'ä': 160,\n",
       " 'å': 161,\n",
       " 'æ': 162,\n",
       " 'ç': 163,\n",
       " 'è': 164,\n",
       " 'é': 165,\n",
       " 'ê': 166,\n",
       " 'ë': 167,\n",
       " 'ì': 168,\n",
       " 'í': 169,\n",
       " 'î': 170,\n",
       " 'ï': 171,\n",
       " 'ð': 172,\n",
       " 'ñ': 173,\n",
       " 'ò': 174,\n",
       " 'ó': 175,\n",
       " 'ô': 176,\n",
       " 'õ': 177,\n",
       " 'ö': 178,\n",
       " '÷': 179,\n",
       " 'ø': 180,\n",
       " 'ù': 181,\n",
       " 'ú': 182,\n",
       " 'û': 183,\n",
       " 'ü': 184,\n",
       " 'ý': 185,\n",
       " 'þ': 186,\n",
       " 'ÿ': 187,\n",
       " 'Ā': 188,\n",
       " 'ā': 189,\n",
       " 'Ă': 190,\n",
       " 'ă': 191,\n",
       " 'Ą': 192,\n",
       " 'ą': 193,\n",
       " 'Ć': 194,\n",
       " 'ć': 195,\n",
       " 'Ĉ': 196,\n",
       " 'ĉ': 197,\n",
       " 'Ċ': 198,\n",
       " 'ċ': 199,\n",
       " 'Č': 200,\n",
       " 'č': 201,\n",
       " 'Ď': 202,\n",
       " 'ď': 203,\n",
       " 'Đ': 204,\n",
       " 'đ': 205,\n",
       " 'Ē': 206,\n",
       " 'ē': 207,\n",
       " 'Ĕ': 208,\n",
       " 'ĕ': 209,\n",
       " 'Ė': 210,\n",
       " 'ė': 211,\n",
       " 'Ę': 212,\n",
       " 'ę': 213,\n",
       " 'Ě': 214,\n",
       " 'ě': 215,\n",
       " 'Ĝ': 216,\n",
       " 'ĝ': 217,\n",
       " 'Ğ': 218,\n",
       " 'ğ': 219,\n",
       " 'Ġ': 220,\n",
       " 'ġ': 221,\n",
       " 'Ģ': 222,\n",
       " 'ģ': 223,\n",
       " 'Ĥ': 224,\n",
       " 'ĥ': 225,\n",
       " 'Ħ': 226,\n",
       " 'ħ': 227,\n",
       " 'Ĩ': 228,\n",
       " 'ĩ': 229,\n",
       " 'Ī': 230,\n",
       " 'ī': 231,\n",
       " 'Ĭ': 232,\n",
       " 'ĭ': 233,\n",
       " 'Į': 234,\n",
       " 'į': 235,\n",
       " 'İ': 236,\n",
       " 'ı': 237,\n",
       " 'Ĳ': 238,\n",
       " 'ĳ': 239,\n",
       " 'Ĵ': 240,\n",
       " 'ĵ': 241,\n",
       " 'Ķ': 242,\n",
       " 'ķ': 243,\n",
       " 'ĸ': 244,\n",
       " 'Ĺ': 245,\n",
       " 'ĺ': 246,\n",
       " 'Ļ': 247,\n",
       " 'ļ': 248,\n",
       " 'Ľ': 249,\n",
       " 'ľ': 250,\n",
       " 'Ŀ': 251,\n",
       " 'ŀ': 252,\n",
       " 'Ł': 253,\n",
       " 'ł': 254,\n",
       " 'Ń': 255,\n",
       " 'Ġt': 256,\n",
       " 'Ġa': 257,\n",
       " 'he': 258,\n",
       " 'in': 259,\n",
       " 're': 260,\n",
       " 'on': 261,\n",
       " 'Ġthe': 262,\n",
       " 'er': 263,\n",
       " 'Ġs': 264,\n",
       " 'at': 265,\n",
       " 'Ġw': 266,\n",
       " 'Ġo': 267,\n",
       " 'en': 268,\n",
       " 'Ġc': 269,\n",
       " 'it': 270,\n",
       " 'is': 271,\n",
       " 'an': 272,\n",
       " 'or': 273,\n",
       " 'es': 274,\n",
       " 'Ġb': 275,\n",
       " 'ed': 276,\n",
       " 'Ġf': 277,\n",
       " 'ing': 278,\n",
       " 'Ġp': 279,\n",
       " 'ou': 280,\n",
       " 'Ġan': 281,\n",
       " 'al': 282,\n",
       " 'ar': 283,\n",
       " 'Ġto': 284,\n",
       " 'Ġm': 285,\n",
       " 'Ġof': 286,\n",
       " 'Ġin': 287,\n",
       " 'Ġd': 288,\n",
       " 'Ġh': 289,\n",
       " 'Ġand': 290,\n",
       " 'ic': 291,\n",
       " 'as': 292,\n",
       " 'le': 293,\n",
       " 'Ġth': 294,\n",
       " 'ion': 295,\n",
       " 'om': 296,\n",
       " 'll': 297,\n",
       " 'ent': 298,\n",
       " 'Ġn': 299,\n",
       " 'Ġl': 300,\n",
       " 'st': 301,\n",
       " 'Ġre': 302,\n",
       " 've': 303,\n",
       " 'Ġe': 304,\n",
       " 'ro': 305,\n",
       " 'ly': 306,\n",
       " 'Ġbe': 307,\n",
       " 'Ġg': 308,\n",
       " 'ĠT': 309,\n",
       " 'ct': 310,\n",
       " 'ĠS': 311,\n",
       " 'id': 312,\n",
       " 'ot': 313,\n",
       " 'ĠI': 314,\n",
       " 'ut': 315,\n",
       " 'et': 316,\n",
       " 'ĠA': 317,\n",
       " 'Ġis': 318,\n",
       " 'Ġon': 319,\n",
       " 'im': 320,\n",
       " 'am': 321,\n",
       " 'ow': 322,\n",
       " 'ay': 323,\n",
       " 'ad': 324,\n",
       " 'se': 325,\n",
       " 'Ġthat': 326,\n",
       " 'ĠC': 327,\n",
       " 'ig': 328,\n",
       " 'Ġfor': 329,\n",
       " 'ac': 330,\n",
       " 'Ġy': 331,\n",
       " 'ver': 332,\n",
       " 'ur': 333,\n",
       " 'Ġu': 334,\n",
       " 'ld': 335,\n",
       " 'Ġst': 336,\n",
       " 'ĠM': 337,\n",
       " \"'s\": 338,\n",
       " 'Ġhe': 339,\n",
       " 'Ġit': 340,\n",
       " 'ation': 341,\n",
       " 'ith': 342,\n",
       " 'ir': 343,\n",
       " 'ce': 344,\n",
       " 'Ġyou': 345,\n",
       " 'il': 346,\n",
       " 'ĠB': 347,\n",
       " 'Ġwh': 348,\n",
       " 'ol': 349,\n",
       " 'ĠP': 350,\n",
       " 'Ġwith': 351,\n",
       " 'Ġ1': 352,\n",
       " 'ter': 353,\n",
       " 'ch': 354,\n",
       " 'Ġas': 355,\n",
       " 'Ġwe': 356,\n",
       " 'Ġ(': 357,\n",
       " 'nd': 358,\n",
       " 'ill': 359,\n",
       " 'ĠD': 360,\n",
       " 'if': 361,\n",
       " 'Ġ2': 362,\n",
       " 'ag': 363,\n",
       " 'ers': 364,\n",
       " 'ke': 365,\n",
       " 'Ġ\"': 366,\n",
       " 'ĠH': 367,\n",
       " 'em': 368,\n",
       " 'Ġcon': 369,\n",
       " 'ĠW': 370,\n",
       " 'ĠR': 371,\n",
       " 'her': 372,\n",
       " 'Ġwas': 373,\n",
       " 'Ġr': 374,\n",
       " 'od': 375,\n",
       " 'ĠF': 376,\n",
       " 'ul': 377,\n",
       " 'ate': 378,\n",
       " 'Ġat': 379,\n",
       " 'ri': 380,\n",
       " 'pp': 381,\n",
       " 'ore': 382,\n",
       " 'ĠThe': 383,\n",
       " 'Ġse': 384,\n",
       " 'us': 385,\n",
       " 'Ġpro': 386,\n",
       " 'Ġha': 387,\n",
       " 'um': 388,\n",
       " 'Ġare': 389,\n",
       " 'Ġde': 390,\n",
       " 'ain': 391,\n",
       " 'and': 392,\n",
       " 'Ġor': 393,\n",
       " 'igh': 394,\n",
       " 'est': 395,\n",
       " 'ist': 396,\n",
       " 'ab': 397,\n",
       " 'rom': 398,\n",
       " 'ĠN': 399,\n",
       " 'th': 400,\n",
       " 'Ġcom': 401,\n",
       " 'ĠG': 402,\n",
       " 'un': 403,\n",
       " 'op': 404,\n",
       " '00': 405,\n",
       " 'ĠL': 406,\n",
       " 'Ġnot': 407,\n",
       " 'ess': 408,\n",
       " 'Ġex': 409,\n",
       " 'Ġv': 410,\n",
       " 'res': 411,\n",
       " 'ĠE': 412,\n",
       " 'ew': 413,\n",
       " 'ity': 414,\n",
       " 'ant': 415,\n",
       " 'Ġby': 416,\n",
       " 'el': 417,\n",
       " 'os': 418,\n",
       " 'ort': 419,\n",
       " 'oc': 420,\n",
       " 'qu': 421,\n",
       " 'Ġfrom': 422,\n",
       " 'Ġhave': 423,\n",
       " 'Ġsu': 424,\n",
       " 'ive': 425,\n",
       " 'ould': 426,\n",
       " 'Ġsh': 427,\n",
       " 'Ġthis': 428,\n",
       " 'nt': 429,\n",
       " 'ra': 430,\n",
       " 'pe': 431,\n",
       " 'ight': 432,\n",
       " 'art': 433,\n",
       " 'ment': 434,\n",
       " 'Ġal': 435,\n",
       " 'ust': 436,\n",
       " 'end': 437,\n",
       " '--': 438,\n",
       " 'all': 439,\n",
       " 'ĠO': 440,\n",
       " 'ack': 441,\n",
       " 'Ġch': 442,\n",
       " 'Ġle': 443,\n",
       " 'ies': 444,\n",
       " 'red': 445,\n",
       " 'ard': 446,\n",
       " 'âĢ': 447,\n",
       " 'out': 448,\n",
       " 'ĠJ': 449,\n",
       " 'Ġab': 450,\n",
       " 'ear': 451,\n",
       " 'iv': 452,\n",
       " 'ally': 453,\n",
       " 'our': 454,\n",
       " 'ost': 455,\n",
       " 'gh': 456,\n",
       " 'pt': 457,\n",
       " 'Ġpl': 458,\n",
       " 'ast': 459,\n",
       " 'Ġcan': 460,\n",
       " 'ak': 461,\n",
       " 'ome': 462,\n",
       " 'ud': 463,\n",
       " 'The': 464,\n",
       " 'Ġhis': 465,\n",
       " 'Ġdo': 466,\n",
       " 'Ġgo': 467,\n",
       " 'Ġhas': 468,\n",
       " 'ge': 469,\n",
       " \"'t\": 470,\n",
       " 'ĠU': 471,\n",
       " 'rou': 472,\n",
       " 'Ġsa': 473,\n",
       " 'Ġj': 474,\n",
       " 'Ġbut': 475,\n",
       " 'Ġwor': 476,\n",
       " 'Ġall': 477,\n",
       " 'ect': 478,\n",
       " 'Ġk': 479,\n",
       " 'ame': 480,\n",
       " 'Ġwill': 481,\n",
       " 'ok': 482,\n",
       " 'Ġwhe': 483,\n",
       " 'Ġthey': 484,\n",
       " 'ide': 485,\n",
       " '01': 486,\n",
       " 'ff': 487,\n",
       " 'ich': 488,\n",
       " 'pl': 489,\n",
       " 'ther': 490,\n",
       " 'Ġtr': 491,\n",
       " '..': 492,\n",
       " 'Ġint': 493,\n",
       " 'ie': 494,\n",
       " 'ure': 495,\n",
       " 'age': 496,\n",
       " 'Ġne': 497,\n",
       " 'ial': 498,\n",
       " 'ap': 499,\n",
       " 'ine': 500,\n",
       " 'ice': 501,\n",
       " 'Ġme': 502,\n",
       " 'Ġout': 503,\n",
       " 'ans': 504,\n",
       " 'one': 505,\n",
       " 'ong': 506,\n",
       " 'ions': 507,\n",
       " 'Ġwho': 508,\n",
       " 'ĠK': 509,\n",
       " 'Ġup': 510,\n",
       " 'Ġtheir': 511,\n",
       " 'Ġad': 512,\n",
       " 'Ġ3': 513,\n",
       " 'Ġus': 514,\n",
       " 'ated': 515,\n",
       " 'ous': 516,\n",
       " 'Ġmore': 517,\n",
       " 'ue': 518,\n",
       " 'og': 519,\n",
       " 'ĠSt': 520,\n",
       " 'ind': 521,\n",
       " 'ike': 522,\n",
       " 'Ġso': 523,\n",
       " 'ime': 524,\n",
       " 'per': 525,\n",
       " '.\"': 526,\n",
       " 'ber': 527,\n",
       " 'iz': 528,\n",
       " 'act': 529,\n",
       " 'Ġone': 530,\n",
       " 'Ġsaid': 531,\n",
       " 'Ġ-': 532,\n",
       " 'are': 533,\n",
       " 'Ġyour': 534,\n",
       " 'cc': 535,\n",
       " 'ĠTh': 536,\n",
       " 'Ġcl': 537,\n",
       " 'ep': 538,\n",
       " 'ake': 539,\n",
       " 'able': 540,\n",
       " 'ip': 541,\n",
       " 'Ġcont': 542,\n",
       " 'Ġwhich': 543,\n",
       " 'ia': 544,\n",
       " 'Ġim': 545,\n",
       " 'Ġabout': 546,\n",
       " 'Ġwere': 547,\n",
       " 'very': 548,\n",
       " 'ub': 549,\n",
       " 'Ġhad': 550,\n",
       " 'Ġen': 551,\n",
       " 'Ġcomp': 552,\n",
       " ',\"': 553,\n",
       " 'ĠIn': 554,\n",
       " 'Ġun': 555,\n",
       " 'Ġag': 556,\n",
       " 'ire': 557,\n",
       " 'ace': 558,\n",
       " 'au': 559,\n",
       " 'ary': 560,\n",
       " 'Ġwould': 561,\n",
       " 'ass': 562,\n",
       " 'ry': 563,\n",
       " 'ĠâĢ': 564,\n",
       " 'cl': 565,\n",
       " 'ook': 566,\n",
       " 'ere': 567,\n",
       " 'so': 568,\n",
       " 'ĠV': 569,\n",
       " 'ign': 570,\n",
       " 'ib': 571,\n",
       " 'Ġoff': 572,\n",
       " 'Ġte': 573,\n",
       " 'ven': 574,\n",
       " 'ĠY': 575,\n",
       " 'ile': 576,\n",
       " 'ose': 577,\n",
       " 'ite': 578,\n",
       " 'orm': 579,\n",
       " 'Ġ201': 580,\n",
       " 'Ġres': 581,\n",
       " 'Ġman': 582,\n",
       " 'Ġper': 583,\n",
       " 'Ġother': 584,\n",
       " 'ord': 585,\n",
       " 'ult': 586,\n",
       " 'Ġbeen': 587,\n",
       " 'Ġlike': 588,\n",
       " 'ase': 589,\n",
       " 'ance': 590,\n",
       " 'ks': 591,\n",
       " 'ays': 592,\n",
       " 'own': 593,\n",
       " 'ence': 594,\n",
       " 'Ġdis': 595,\n",
       " 'ction': 596,\n",
       " 'Ġany': 597,\n",
       " 'Ġapp': 598,\n",
       " 'Ġsp': 599,\n",
       " 'int': 600,\n",
       " 'ress': 601,\n",
       " 'ations': 602,\n",
       " 'ail': 603,\n",
       " 'Ġ4': 604,\n",
       " 'ical': 605,\n",
       " 'Ġthem': 606,\n",
       " 'Ġher': 607,\n",
       " 'ount': 608,\n",
       " 'ĠCh': 609,\n",
       " 'Ġar': 610,\n",
       " 'Ġif': 611,\n",
       " 'Ġthere': 612,\n",
       " 'Ġpe': 613,\n",
       " 'Ġyear': 614,\n",
       " 'av': 615,\n",
       " 'Ġmy': 616,\n",
       " 'Ġsome': 617,\n",
       " 'Ġwhen': 618,\n",
       " 'ough': 619,\n",
       " 'ach': 620,\n",
       " 'Ġthan': 621,\n",
       " 'ru': 622,\n",
       " 'ond': 623,\n",
       " 'ick': 624,\n",
       " 'Ġover': 625,\n",
       " 'vel': 626,\n",
       " 'Ġqu': 627,\n",
       " 'ĊĊ': 628,\n",
       " 'Ġsc': 629,\n",
       " 'reat': 630,\n",
       " 'ree': 631,\n",
       " 'ĠIt': 632,\n",
       " 'ound': 633,\n",
       " 'port': 634,\n",
       " 'Ġalso': 635,\n",
       " 'Ġpart': 636,\n",
       " 'fter': 637,\n",
       " 'Ġkn': 638,\n",
       " 'Ġbec': 639,\n",
       " 'Ġtime': 640,\n",
       " 'ens': 641,\n",
       " 'Ġ5': 642,\n",
       " 'ople': 643,\n",
       " 'Ġwhat': 644,\n",
       " 'Ġno': 645,\n",
       " 'du': 646,\n",
       " 'mer': 647,\n",
       " 'ang': 648,\n",
       " 'Ġnew': 649,\n",
       " '----': 650,\n",
       " 'Ġget': 651,\n",
       " 'ory': 652,\n",
       " 'ition': 653,\n",
       " 'ings': 654,\n",
       " 'Ġjust': 655,\n",
       " 'Ġinto': 656,\n",
       " 'Ġ0': 657,\n",
       " 'ents': 658,\n",
       " 'ove': 659,\n",
       " 'te': 660,\n",
       " 'Ġpeople': 661,\n",
       " 'Ġpre': 662,\n",
       " 'Ġits': 663,\n",
       " 'Ġrec': 664,\n",
       " 'Ġtw': 665,\n",
       " 'ian': 666,\n",
       " 'irst': 667,\n",
       " 'ark': 668,\n",
       " 'ors': 669,\n",
       " 'Ġwork': 670,\n",
       " 'ade': 671,\n",
       " 'ob': 672,\n",
       " 'Ġshe': 673,\n",
       " 'Ġour': 674,\n",
       " 'wn': 675,\n",
       " 'ink': 676,\n",
       " 'lic': 677,\n",
       " 'Ġ19': 678,\n",
       " 'ĠHe': 679,\n",
       " 'ish': 680,\n",
       " 'nder': 681,\n",
       " 'ause': 682,\n",
       " 'Ġhim': 683,\n",
       " 'ons': 684,\n",
       " 'Ġ[': 685,\n",
       " 'Ġro': 686,\n",
       " 'form': 687,\n",
       " 'ild': 688,\n",
       " 'ates': 689,\n",
       " 'vers': 690,\n",
       " 'Ġonly': 691,\n",
       " 'oll': 692,\n",
       " 'Ġspe': 693,\n",
       " 'ck': 694,\n",
       " 'ell': 695,\n",
       " 'amp': 696,\n",
       " 'Ġacc': 697,\n",
       " 'Ġbl': 698,\n",
       " 'ious': 699,\n",
       " 'urn': 700,\n",
       " 'ft': 701,\n",
       " 'ood': 702,\n",
       " 'Ġhow': 703,\n",
       " 'hed': 704,\n",
       " \"Ġ'\": 705,\n",
       " 'Ġafter': 706,\n",
       " 'aw': 707,\n",
       " 'Ġatt': 708,\n",
       " 'ov': 709,\n",
       " 'ne': 710,\n",
       " 'Ġplay': 711,\n",
       " 'erv': 712,\n",
       " 'ict': 713,\n",
       " 'Ġcould': 714,\n",
       " 'itt': 715,\n",
       " 'Ġam': 716,\n",
       " 'Ġfirst': 717,\n",
       " 'Ġ6': 718,\n",
       " 'Ġact': 719,\n",
       " 'Ġ$': 720,\n",
       " 'ec': 721,\n",
       " 'hing': 722,\n",
       " 'ual': 723,\n",
       " 'ull': 724,\n",
       " 'Ġcomm': 725,\n",
       " 'oy': 726,\n",
       " 'old': 727,\n",
       " 'ces': 728,\n",
       " 'ater': 729,\n",
       " 'Ġfe': 730,\n",
       " 'Ġbet': 731,\n",
       " 'we': 732,\n",
       " 'iff': 733,\n",
       " 'Ġtwo': 734,\n",
       " 'ock': 735,\n",
       " 'Ġback': 736,\n",
       " ').': 737,\n",
       " 'ident': 738,\n",
       " 'Ġunder': 739,\n",
       " 'rough': 740,\n",
       " 'sel': 741,\n",
       " 'xt': 742,\n",
       " 'Ġmay': 743,\n",
       " 'round': 744,\n",
       " 'Ġpo': 745,\n",
       " 'ph': 746,\n",
       " 'iss': 747,\n",
       " 'Ġdes': 748,\n",
       " 'Ġmost': 749,\n",
       " 'Ġdid': 750,\n",
       " 'Ġadd': 751,\n",
       " 'ject': 752,\n",
       " 'Ġinc': 753,\n",
       " 'fore': 754,\n",
       " 'Ġpol': 755,\n",
       " 'ont': 756,\n",
       " 'Ġagain': 757,\n",
       " 'clud': 758,\n",
       " 'tern': 759,\n",
       " 'Ġknow': 760,\n",
       " 'Ġneed': 761,\n",
       " 'Ġcons': 762,\n",
       " 'Ġco': 763,\n",
       " 'Ġ.': 764,\n",
       " 'Ġwant': 765,\n",
       " 'Ġsee': 766,\n",
       " 'Ġ7': 767,\n",
       " 'ning': 768,\n",
       " 'iew': 769,\n",
       " 'ĠThis': 770,\n",
       " 'ced': 771,\n",
       " 'Ġeven': 772,\n",
       " 'Ġind': 773,\n",
       " 'ty': 774,\n",
       " 'ĠWe': 775,\n",
       " 'ath': 776,\n",
       " 'Ġthese': 777,\n",
       " 'Ġpr': 778,\n",
       " 'Ġuse': 779,\n",
       " 'Ġbecause': 780,\n",
       " 'Ġfl': 781,\n",
       " 'ng': 782,\n",
       " 'Ġnow': 783,\n",
       " 'ĠâĢĵ': 784,\n",
       " 'com': 785,\n",
       " 'ise': 786,\n",
       " 'Ġmake': 787,\n",
       " 'Ġthen': 788,\n",
       " 'ower': 789,\n",
       " 'Ġevery': 790,\n",
       " 'ĠUn': 791,\n",
       " 'Ġsec': 792,\n",
       " 'oss': 793,\n",
       " 'uch': 794,\n",
       " 'Ġem': 795,\n",
       " 'Ġ=': 796,\n",
       " 'ĠRe': 797,\n",
       " 'ied': 798,\n",
       " 'rit': 799,\n",
       " 'Ġinv': 800,\n",
       " 'lect': 801,\n",
       " 'Ġsupp': 802,\n",
       " 'ating': 803,\n",
       " 'Ġlook': 804,\n",
       " 'man': 805,\n",
       " 'pect': 806,\n",
       " 'Ġ8': 807,\n",
       " 'row': 808,\n",
       " 'Ġbu': 809,\n",
       " 'Ġwhere': 810,\n",
       " 'ific': 811,\n",
       " 'Ġyears': 812,\n",
       " 'ily': 813,\n",
       " 'Ġdiff': 814,\n",
       " 'Ġshould': 815,\n",
       " 'Ġrem': 816,\n",
       " 'Th': 817,\n",
       " 'In': 818,\n",
       " 'Ġev': 819,\n",
       " 'day': 820,\n",
       " \"'re\": 821,\n",
       " 'rib': 822,\n",
       " 'Ġrel': 823,\n",
       " 'ss': 824,\n",
       " 'Ġdef': 825,\n",
       " 'Ġright': 826,\n",
       " 'Ġsy': 827,\n",
       " '),': 828,\n",
       " 'les': 829,\n",
       " '000': 830,\n",
       " 'hen': 831,\n",
       " 'Ġthrough': 832,\n",
       " 'ĠTr': 833,\n",
       " '__': 834,\n",
       " 'Ġway': 835,\n",
       " 'Ġdon': 836,\n",
       " 'Ġ,': 837,\n",
       " 'Ġ10': 838,\n",
       " 'ased': 839,\n",
       " 'Ġass': 840,\n",
       " 'ublic': 841,\n",
       " 'Ġreg': 842,\n",
       " 'ĠAnd': 843,\n",
       " 'ix': 844,\n",
       " 'Ġvery': 845,\n",
       " 'Ġinclud': 846,\n",
       " 'other': 847,\n",
       " 'Ġimp': 848,\n",
       " 'oth': 849,\n",
       " 'Ġsub': 850,\n",
       " 'ĠâĢĶ': 851,\n",
       " 'Ġbeing': 852,\n",
       " 'arg': 853,\n",
       " 'ĠWh': 854,\n",
       " '==': 855,\n",
       " 'ible': 856,\n",
       " 'Ġdoes': 857,\n",
       " 'ange': 858,\n",
       " 'ram': 859,\n",
       " 'Ġ9': 860,\n",
       " 'ert': 861,\n",
       " 'ps': 862,\n",
       " 'ited': 863,\n",
       " 'ational': 864,\n",
       " 'Ġbr': 865,\n",
       " 'Ġdown': 866,\n",
       " 'Ġmany': 867,\n",
       " 'aking': 868,\n",
       " 'Ġcall': 869,\n",
       " 'uring': 870,\n",
       " 'ities': 871,\n",
       " 'Ġph': 872,\n",
       " 'ics': 873,\n",
       " 'als': 874,\n",
       " 'Ġdec': 875,\n",
       " 'ative': 876,\n",
       " 'ener': 877,\n",
       " 'Ġbefore': 878,\n",
       " 'ility': 879,\n",
       " 'Ġwell': 880,\n",
       " 'Ġmuch': 881,\n",
       " 'erson': 882,\n",
       " 'Ġthose': 883,\n",
       " 'Ġsuch': 884,\n",
       " 'Ġke': 885,\n",
       " 'Ġend': 886,\n",
       " 'ĠBut': 887,\n",
       " 'ason': 888,\n",
       " 'ting': 889,\n",
       " 'Ġlong': 890,\n",
       " 'ef': 891,\n",
       " 'Ġthink': 892,\n",
       " 'ys': 893,\n",
       " 'Ġbel': 894,\n",
       " 'Ġsm': 895,\n",
       " 'its': 896,\n",
       " 'ax': 897,\n",
       " 'Ġown': 898,\n",
       " 'Ġprov': 899,\n",
       " 'Ġset': 900,\n",
       " 'ife': 901,\n",
       " 'ments': 902,\n",
       " 'ble': 903,\n",
       " 'ward': 904,\n",
       " 'Ġshow': 905,\n",
       " 'Ġpres': 906,\n",
       " 'ms': 907,\n",
       " 'omet': 908,\n",
       " 'Ġob': 909,\n",
       " 'Ġsay': 910,\n",
       " 'ĠSh': 911,\n",
       " 'ts': 912,\n",
       " 'ful': 913,\n",
       " 'Ġeff': 914,\n",
       " 'Ġgu': 915,\n",
       " 'Ġinst': 916,\n",
       " 'und': 917,\n",
       " 'ren': 918,\n",
       " 'cess': 919,\n",
       " 'Ġent': 920,\n",
       " 'ĠYou': 921,\n",
       " 'Ġgood': 922,\n",
       " 'Ġstart': 923,\n",
       " 'ince': 924,\n",
       " 'Ġmade': 925,\n",
       " 'tt': 926,\n",
       " 'stem': 927,\n",
       " 'olog': 928,\n",
       " 'up': 929,\n",
       " 'Ġ|': 930,\n",
       " 'ump': 931,\n",
       " 'Ġhel': 932,\n",
       " 'vern': 933,\n",
       " 'ular': 934,\n",
       " 'ually': 935,\n",
       " 'Ġac': 936,\n",
       " 'Ġmon': 937,\n",
       " 'Ġlast': 938,\n",
       " 'Ġ200': 939,\n",
       " '10': 940,\n",
       " 'Ġstud': 941,\n",
       " 'ures': 942,\n",
       " 'ĠAr': 943,\n",
       " 'self': 944,\n",
       " 'ars': 945,\n",
       " 'meric': 946,\n",
       " 'ues': 947,\n",
       " 'cy': 948,\n",
       " 'Ġmin': 949,\n",
       " 'ollow': 950,\n",
       " 'Ġcol': 951,\n",
       " 'io': 952,\n",
       " 'Ġmod': 953,\n",
       " 'Ġcount': 954,\n",
       " 'ĠCom': 955,\n",
       " 'hes': 956,\n",
       " 'Ġfin': 957,\n",
       " 'air': 958,\n",
       " 'ier': 959,\n",
       " 'âĢĶ': 960,\n",
       " 'read': 961,\n",
       " 'ank': 962,\n",
       " 'atch': 963,\n",
       " 'ever': 964,\n",
       " 'Ġstr': 965,\n",
       " 'Ġpoint': 966,\n",
       " 'ork': 967,\n",
       " 'ĠNew': 968,\n",
       " 'Ġsur': 969,\n",
       " 'ool': 970,\n",
       " 'alk': 971,\n",
       " 'ement': 972,\n",
       " 'Ġused': 973,\n",
       " 'ract': 974,\n",
       " 'ween': 975,\n",
       " 'Ġsame': 976,\n",
       " 'oun': 977,\n",
       " 'ĠAl': 978,\n",
       " 'ci': 979,\n",
       " 'Ġdiffere': 980,\n",
       " 'Ġwhile': 981,\n",
       " '--------': 982,\n",
       " 'Ġgame': 983,\n",
       " 'cept': 984,\n",
       " 'Ġsim': 985,\n",
       " '...': 986,\n",
       " 'Ġinter': 987,\n",
       " 'ek': 988,\n",
       " 'Ġreport': 989,\n",
       " 'Ġprodu': 990,\n",
       " 'Ġstill': 991,\n",
       " 'led': 992,\n",
       " 'ah': 993,\n",
       " 'Ġhere': 994,\n",
       " 'Ġworld': 995,\n",
       " 'Ġthough': 996,\n",
       " 'Ġnum': 997,\n",
       " 'arch': 998,\n",
       " 'imes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ\n",
      "Ġ=================================================================\n",
      "Ġ----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ\n",
      "................................................................\n",
      "================================================================\n",
      "________________________________________________________________\n",
      "--------------------------------------------------------\n",
      "âĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶâĢĶ\n",
      "------------------------------------------------\n",
      "Ġ=================================\n",
      "Ġ--------------------------------\n",
      "Ġ********************************\n",
      "--------------------------------\n",
      "................................\n",
      "================================\n",
      "________________________________\n",
      "ÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤÃĥÃĤ\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "# Find the 10 longest tokens in the encoder\n",
    "longest_tokens = sorted(encoder.keys(), key=len, reverse=True)[:20]\n",
    "\n",
    "# Print the longest tokens\n",
    "for token in longest_tokens:\n",
    "    print(f\"{token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # special token in use for the GPT-2 base model. used to separate different texts in the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both **Llama** and **Mistral** series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
    "\n",
    "(Personally I think the tiktoken way is a lot cleaner...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with the start of Don Quijote\n",
    "with open(\"quixote_sentencepiece.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivía un hidalgo de los de lanza en astillero, adarga antigua, rocín flaco y galgo corredor. Una olla de algo más vaca que carnero, salpicón las más noches, duelos y quebrantos los sábados, lantejas los viernes, algún palomino de añadidura los domingos, consumían las tres partes de su hacienda. El resto della concluían sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los días de entresemana se honraba con su vellorí de lo más fino. Tenía en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que así ensillaba el rocín como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta años; era de complexión recia, seco de carnes, enjuto de rostro, gran madrugador y amigo de la caza. Quieren decir que tenía el sobrenombre de Quijada, o Quesada, que en esto hay alguna diferencia en los autores que deste caso escriben; aunque, por conjeturas verosímiles, se deja entender que se llamaba Quejana. Pero esto importa poco a nuestro cuento; basta que en la narración dél no se salga un punto de la verdad. Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba ocioso, que eran los más del año, se daba a leer libros de caballerías, con tanta afición y gusto, que olvidó casi de todo punto el ejercicio de la caza, y aun la administración de su hacienda. Y llegó a tanto su curiosidad y desatino en esto, que vendió muchas hanegas de tierra de sembradura para comprar libros de caballerías en que leer, y así, llevó a su casa todos cuantos pudo haber dellos; y de todos, ningunos le parecían tan bien como los que compuso el famoso Feliciano de Silva, porque la claridad de su prosa y aquellas entricadas razones suyas le parecían de perlas, y más cuando llegaba a leer aquellos requiebros y cartas de desafíos, donde en muchas partes hallaba escrito: La razón de la sinrazón que a mi razón se hace, de tal manera mi razón enflaquece, que con razón me quejo de la vuestra fermosura. Y también cuando leía: ...los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza. Con estas razones perdía el pobre caballero el juicio, y desvelábase por entenderlas y desentrañarles el sentido, que no se lo sacara ni las entendiera el mesmo Aristóteles, si resucitara para sólo ello. No estaba muy bien con las heridas que don Belianís daba y recebía, porque se imaginaba que, por grandes maestros que le hubiesen curado, no dejaría de tener el rostro y todo el cuerpo lleno de cicatrices y señales. Pero, con todo, alababa en su autor aquel acabar su libro con la promesa de aquella inacabable aventura, y muchas veces le vino deseo de tomar la pluma y dalle fin al pie de la letra, como allí se promete; y sin duda alguna lo hiciera, y aun saliera con ello, si otros mayores y continuos pensamientos no se lo estorbaran. Tuvo muchas veces competencia con el cura de su lugar -que era hombre docto, graduado en Sigüenza-, sobre cuál había sido mejor caballero: Palmerín de Ingalaterra o Amadís de Gaula; mas maese Nicolás, barbero del mesmo pueblo, decía que ninguno llegaba al Caballero del Febo, y que si alguno se le podía comparar, era don Galaor, hermano de Amadís de Gaula, porque tenía muy acomodada condición para todo; que no era caballero melindroso, ni tan llorón como su hermano, y que en lo de la valentía no le iba en zaga. En resolución, él se enfrascó tanto en su letura, que se le pasaban las noches leyendo de claro en claro, y los días de turbio en turbio; y así, del poco dormir y del mucho leer, se le secó el celebro, de manera que vino a perder el juicio. Llenósele la fantasía de todo aquello que leía en los libros, así de encantamentos como de pendencias, batallas, desafíos, heridas, requiebros, amores, tormentas y disparates imposibles; y asentósele de tal modo en la imaginación que era verdad toda aquella máquina de aquellas sonadas soñadas invenciones que leía, que para él no había otra historia más cierta en el mundo. Decía él que el Cid Ruy Díaz había sido muy buen caballero, pero que no tenía que ver con el Caballero de la Ardiente Espada, que de sólo un revés había partido por medio dos fieros y descomunales gigantes. Mejor estaba con Bernardo del Carpio, porque en Roncesvalles había muerto a Roldán el encantado, valiéndose de la industria de Hércules, cuando ahogó a Anteo, el hijo de la Tierra, entre los brazos. Decía mucho bien del gigante Morgante, porque, con ser de aquella generación gigantea, que todos son soberbios y descomedidos, él solo era afable y bien criado. Pero, sobre todos, estaba bien con Reinaldos de Montalbán, y más cuando le veía salir de su castillo y robar cuantos topaba, y cuando en allende robó aquel ídolo de Mahoma que era todo de oro, según dice su historia. Diera él, por dar una mano de coces al traidor de Galalón, al ama que tenía, y aun a su sobrina de añadidura. En efeto, rematado ya su juicio, vino a dar en el más estraño pensamiento que jamás dio loco en el mundo; y fue que le pareció convenible y necesario, así para el aumento de su honra como para el servicio de su república, hacerse caballero andante, y irse por todo el mundo con sus armas y caballo a buscar las aventuras y a ejercitarse en todo aquello que él había leído que los caballeros andantes se ejercitaban, deshaciendo todo género de agravio, y poniéndose en ocasiones y peligros donde, acabándolos, cobrase eterno nombre y fama...\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece Training Options 🔤✂️\n",
    "\n",
    "The training options for `spm_train` can be listed using `spm_train --help`. Since the standard pip install of sentencepiece does not always include `spm_train`, here's a comprehensive list with explanations:\n",
    "\n",
    "## Basic Usage 🚀\n",
    "```\n",
    "Usage: spm_train [options] files\n",
    "```\n",
    "\n",
    "## Essential Options ⭐\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--input` | Comma-separated list of input text files | \"\" |\n",
    "| `--model_prefix` | Output model name prefix (required!) | \"\" |\n",
    "| `--model_type` | Algorithm: `unigram`, `bpe`, `word` or `char` | \"unigram\" |\n",
    "| `--vocab_size` | Size of vocabulary to create | 8000 |\n",
    "\n",
    "## Input Processing Options 📄\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--input_format` | Format: `text` or `tsv` | \"\" |\n",
    "| `--input_sentence_size` | Maximum number of sentences to load (0 = all) | 0 |\n",
    "| `--shuffle_input_sentence` | Randomly sample sentences (when using `--input_sentence_size`) | true |\n",
    "| `--max_sentence_length` | Maximum sentence length in bytes | 4192 |\n",
    "\n",
    "## Tokenization Control 🧩\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--character_coverage` | % of characters to cover (higher = larger vocab) | 0.9995 |\n",
    "| `--max_sentencepiece_length` | Maximum length of each token | 16 |\n",
    "| `--split_by_unicode_script` | Split by Unicode script boundaries | true |\n",
    "| `--split_by_number` | Split tokens at numbers | true |\n",
    "| `--split_by_whitespace` | Use whitespace to split tokens | true |\n",
    "| `--split_digits` | Split each digit into separate pieces | false |\n",
    "| `--byte_fallback` | Decompose unknown characters into UTF-8 bytes | false |\n",
    "\n",
    "## Special Tokens & Symbols 🏷️\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--control_symbols` | Comma-separated list of control symbols | \"\" |\n",
    "| `--user_defined_symbols` | Comma-separated list of user-defined symbols | \"\" |\n",
    "| `--required_chars` | Characters always included regardless of coverage | \"\" |\n",
    "| `--unk_id` | ID for unknown token (`<unk>`) | 0 |\n",
    "| `--bos_id` | ID for beginning of sentence (`<s>`) | 1 |\n",
    "| `--eos_id` | ID for end of sentence (`</s>`) | 2 |\n",
    "| `--pad_id` | ID for padding (`<pad>`) | -1 |\n",
    "\n",
    "## Text Normalization 🧹\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--normalization_rule_name` | Rule for text normalization: `nfkc` or `identity` | \"nmt_nfkc\" |\n",
    "| `--add_dummy_prefix` | Add dummy space at text beginning | true |\n",
    "| `--remove_extra_whitespaces` | Remove extra whitespace (leading, trailing, duplicate) | true |\n",
    "\n",
    "## Advanced Training Options ⚙️\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--num_threads` | Number of training threads | 16 |\n",
    "| `--num_sub_iterations` | Number of EM sub-iterations | 2 |\n",
    "| `--seed_sentencepiece_size` | Size of seed sentencepieces | 1000000 |\n",
    "| `--shrinking_factor` | Keep top pieces based on loss | 0.75 |\n",
    "| `--hard_vocab_limit` | Strictly enforce vocab size limit | true |\n",
    "| `--random_seed` | Random seed value | 4294967295 |\n",
    "\n",
    "## Privacy Options 🔒\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--enable_differential_privacy` | Use differential privacy (DP) in training | false |\n",
    "| `--differential_privacy_noise_level` | Amount of noise for DP | 0 |\n",
    "| `--differential_privacy_clipping_threshold` | Threshold for clipping counts in DP | 0 |\n",
    "\n",
    "## Help and Debugging 🔍\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--help` | Show help message | false |\n",
    "| `--version` | Show version information | false |\n",
    "| `--minloglevel` | Minimum level for logging messages | 0 |\n",
    "| `--self_test_sample_size` | Size of self-test samples | 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: quixote_sentencepiece.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 8384\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: quixote_sentencepiece.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=5592\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 507\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=121 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=20 all=675 active=619 piece=an\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=40 all=960 active=904 piece=▁le\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=60 all=1106 active=1050 piece=▁no\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=80 all=1199 active=1143 piece=▁caball\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"quixote_sentencepiece.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=8384, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "# Check if the input file exists and is not empty\n",
    "if os.path.exists(options['input']) and os.path.getsize(options['input']) > 0:\n",
    "    spm.SentencePieceTrainer.train(**options)\n",
    "else:\n",
    "    print(f\"Error: The input file {options['input']} does not exist or is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['▁d', 259],\n",
       " ['▁de', 260],\n",
       " ['en', 261],\n",
       " ['▁l', 262],\n",
       " ['▁c', 263],\n",
       " ['er', 264],\n",
       " ['ue', 265],\n",
       " ['▁a', 266],\n",
       " ['▁s', 267],\n",
       " ['os', 268],\n",
       " ['que', 269],\n",
       " ['es', 270],\n",
       " ['ra', 271],\n",
       " ['▁p', 272],\n",
       " ['as', 273],\n",
       " ['▁y', 274],\n",
       " ['▁que', 275],\n",
       " ['ab', 276],\n",
       " ['▁m', 277],\n",
       " ['an', 278],\n",
       " ['to', 279],\n",
       " ['do', 280],\n",
       " ['el', 281],\n",
       " ['al', 282],\n",
       " ['on', 283],\n",
       " ['▁en', 284],\n",
       " ['ar', 285],\n",
       " ['▁h', 286],\n",
       " ['ci', 287],\n",
       " ['in', 288],\n",
       " ['ía', 289],\n",
       " ['or', 290],\n",
       " ['▁el', 291],\n",
       " ['om', 292],\n",
       " ['un', 293],\n",
       " ['▁n', 294],\n",
       " ['▁v', 295],\n",
       " ['▁con', 296],\n",
       " ['▁la', 297],\n",
       " ['▁le', 298],\n",
       " ['▁su', 299],\n",
       " ['ad', 300],\n",
       " ['▁se', 301],\n",
       " ['▁t', 302],\n",
       " ['br', 303],\n",
       " ['aba', 304],\n",
       " ['▁to', 305],\n",
       " ['ll', 306],\n",
       " ['ón', 307],\n",
       " ['▁r', 308],\n",
       " ['ero', 309],\n",
       " ['▁los', 310],\n",
       " ['all', 311],\n",
       " ['era', 312],\n",
       " ['▁cu', 313],\n",
       " ['ant', 314],\n",
       " ['▁es', 315],\n",
       " ['id', 316],\n",
       " ['ent', 317],\n",
       " ['▁no', 318],\n",
       " ['tr', 319],\n",
       " ['ara', 320],\n",
       " ['ch', 321],\n",
       " ['▁b', 322],\n",
       " ['▁f', 323],\n",
       " ['▁g', 324],\n",
       " ['▁al', 325],\n",
       " ['▁mu', 326],\n",
       " ['▁com', 327],\n",
       " ['▁por', 328],\n",
       " ['aball', 329],\n",
       " ['am', 330],\n",
       " ['ec', 331],\n",
       " ['ás', 332],\n",
       " ['▁des', 333],\n",
       " ['▁aque', 334],\n",
       " ['le', 335],\n",
       " ['▁so', 336],\n",
       " ['▁todo', 337],\n",
       " ['▁caball', 338],\n",
       " ['ic', 339],\n",
       " ['oc', 340],\n",
       " ['ri', 341],\n",
       " ['ui', 342],\n",
       " ['bre', 343],\n",
       " ['▁', 344],\n",
       " ['e', 345],\n",
       " ['a', 346],\n",
       " ['o', 347],\n",
       " ['s', 348],\n",
       " ['n', 349],\n",
       " ['l', 350],\n",
       " ['r', 351],\n",
       " ['d', 352],\n",
       " ['u', 353],\n",
       " ['i', 354],\n",
       " ['c', 355],\n",
       " ['t', 356],\n",
       " ['m', 357],\n",
       " [',', 358],\n",
       " ['b', 359],\n",
       " ['q', 360],\n",
       " ['p', 361],\n",
       " ['y', 362],\n",
       " ['í', 363],\n",
       " ['g', 364],\n",
       " ['h', 365],\n",
       " ['v', 366],\n",
       " ['ó', 367],\n",
       " ['.', 368],\n",
       " ['f', 369],\n",
       " ['j', 370],\n",
       " ['z', 371],\n",
       " ['á', 372],\n",
       " ['é', 373],\n",
       " [';', 374],\n",
       " ['ñ', 375],\n",
       " ['E', 376],\n",
       " ['A', 377],\n",
       " ['C', 378],\n",
       " ['M', 379],\n",
       " ['D', 380],\n",
       " ['G', 381],\n",
       " ['P', 382],\n",
       " ['Q', 383],\n",
       " ['R', 384],\n",
       " [':', 385],\n",
       " ['F', 386],\n",
       " ['T', 387],\n",
       " ['ú', 388],\n",
       " ['-', 389],\n",
       " ['B', 390],\n",
       " ['L', 391],\n",
       " ['N', 392],\n",
       " ['S', 393],\n",
       " ['Y', 394],\n",
       " ['H', 395],\n",
       " ['I', 396],\n",
       " ['U', 397],\n",
       " ['x', 398],\n",
       " ['ü', 399]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "**Why can't LLM spell words**  \n",
    "LLMs see words as token chunks, not letter by letter, so unusual spellings may split into strange token combinations the model hasn't seen together often.  \n",
    "\n",
    "**Why can't LLM do super simple string processing tasks like reversing a string**  \n",
    "LLMs process text as tokens, not individual characters, making character-by-character operations like string reversal difficult.  \n",
    "\n",
    "**Why is LLM worse at non-English languages (e.g. Japanese)**  \n",
    "Languages like Japanese tokenize inefficiently, often requiring more tokens per word than English, giving the model less context to work with.  \n",
    "\n",
    "**Why is LLM bad at simple arithmetic**  \n",
    "Numbers get split into multiple tokens based on digit patterns, making it harder for the model to understand numerical relationships.  \n",
    "\n",
    "**Why did GPT-2 have more than necessary trouble coding in Python**  \n",
    "Code tokens were less common in GPT-2's training data, and programming syntax often gets split in unintuitive ways. For example, indentation spaces occupy a huge part of the context window.  \n",
    "\n",
    "**Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"**  \n",
    "This specific string is a special token used during training to mark the end of documents, so models interpret it as a signal to stop generating.  \n",
    "\n",
    "**Why does the LLM break if I ask it about \"SolidGoldMagikarp\"**  \n",
    "This specific string became famous because in early GPT models, it tokenized into an unusual pattern that caused the model to produce repetitive or nonsensical outputs. A hypothesis is that it's related to a Reddit username.  \n",
    "\n",
    "**Why should I prefer to use YAML over JSON with LLMs**  \n",
    "YAML formatting creates more natural language-like tokens than JSON's special characters and strict syntax.  \n",
    "\n",
    "\n",
    "\n",
    "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six times—probably six times—a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer fooling—this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine times—well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten times—the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\n",
      "length: 1688\n",
      "---\n",
      "[70, 111, 111, 108, 32, 109, 101, 32, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 104, 114, 101, 101, 32, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 101, 32, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 101, 32, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 102, 105, 114, 115, 116, 32, 116, 105, 109, 101, 32, 110, 111, 116, 32, 116, 111, 32, 98, 101, 32, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 101, 32, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 32, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 101, 32, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 101, 32, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 105, 118, 101, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 101, 32, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 111, 32, 116, 97, 107, 101, 32, 115, 111, 109, 101, 32, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 101, 32, 103, 111, 116, 32, 116, 119, 101, 108, 118, 101, 32, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 101, 32, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 101, 32, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 101, 32, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 101, 32, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 101, 32, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 101, 32, 115, 101, 110, 115, 101, 32, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 104, 101, 32, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 101, 32, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 101, 32, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 101, 32, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 101, 32, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 101, 32, 110, 97, 116, 117, 114, 101, 32, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 101, 32, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 101, 32, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 110, 105, 110, 101, 32, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 101, 32, 116, 105, 109, 101, 32, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 101, 32, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 101, 32, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 101, 32, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 101, 32, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 101, 32, 111, 117, 116, 32, 116, 104, 101, 32, 112, 101, 111, 112, 108, 101, 32, 119, 104, 111, 32, 104, 97, 118, 101, 32, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 101, 32, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1698\n"
     ]
    }
   ],
   "source": [
    "#As usual, i change the original text just for fun\n",
    "# Using this awesome bit of James Donald Forbes McCann --> https://www.youtube.com/watch?v=udSMZG_L-S0\n",
    "text=\"Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six times—probably six times—a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer fooling—this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine times—well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten times—the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\"\n",
    "\n",
    "\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, (101, 32)), (55, (32, 116)), (42, (109, 101)), (33, (116, 104)), (30, (32, 97)), (27, (111, 111)), (27, (105, 110)), (25, (111, 108)), (25, (44, 32)), (25, (32, 102)), (23, (104, 101)), (23, (32, 115)), (23, (32, 109)), (22, (111, 117)), (20, (116, 105)), (20, (32, 111)), (19, (116, 32)), (19, (111, 110)), (19, (46, 32)), (18, (115, 32)), (18, (110, 103)), (18, (108, 32)), (18, (102, 111)), (18, (100, 32)), (17, (104, 97)), (15, (114, 101)), (15, (110, 32)), (15, (105, 109)), (15, (101, 115)), (15, (32, 98)), (14, (118, 101)), (14, (111, 32)), (14, (97, 116)), (13, (121, 111)), (13, (103, 32)), (13, (32, 121)), (12, (116, 111)), (12, (114, 32)), (12, (108, 105)), (12, (101, 114)), (12, (97, 110)), (11, (108, 108)), (10, (117, 32)), (10, (110, 100)), (10, (97, 108)), (10, (97, 32)), (10, (70, 111)), (10, (32, 73)), (9, (115, 104)), (9, (115, 101)), (9, (108, 101)), (9, (105, 116)), (9, (32, 119)), (9, (32, 108)), (9, (32, 105)), (9, (32, 70)), (8, (121, 32)), (8, (117, 116)), (8, (108, 121)), (8, (98, 101)), (8, (97, 109)), (7, (119, 105)), (7, (114, 105)), (7, (111, 114)), (7, (105, 115)), (7, (101, 108)), (7, (32, 112)), (7, (32, 103)), (6, (117, 114)), (6, (116, 97)), (6, (115, 105)), (6, (111, 102)), (6, (110, 101)), (6, (107, 105)), (6, (105, 99)), (6, (102, 32)), (6, (101, 110)), (6, (98, 117)), (6, (32, 110)), (5, (226, 128)), (5, (128, 148)), (5, (117, 115)), (5, (117, 108)), (5, (115, 116)), (5, (115, 111)), (5, (115, 44)), (5, (110, 111)), (5, (108, 111)), (5, (105, 118)), (5, (104, 105)), (5, (104, 32)), (5, (103, 111)), (5, (101, 118)), (5, (101, 100)), (5, (101, 46)), (5, (97, 114)), (5, (73, 32)), (5, (32, 114)), (5, (32, 104)), (4, (121, 105)), (4, (121, 44)), (4, (117, 97)), (4, (116, 121)), (4, (116, 117)), (4, (116, 101)), (4, (115, 226)), (4, (114, 115)), (4, (114, 111)), (4, (114, 97)), (4, (112, 114)), (4, (112, 101)), (4, (111, 116)), (4, (111, 109)), (4, (110, 115)), (4, (109, 97)), (4, (107, 101)), (4, (105, 111)), (4, (105, 108)), (4, (105, 100)), (4, (101, 101)), (4, (101, 44)), (4, (99, 107)), (4, (98, 108)), (4, (97, 118)), (4, (97, 98)), (4, (73, 39)), (4, (39, 115)), (4, (32, 117)), (4, (32, 99)), (3, (120, 32)), (3, (119, 101)), (3, (118, 105)), (3, (116, 119)), (3, (114, 121)), (3, (112, 32)), (3, (110, 97)), (3, (110, 46)), (3, (109, 32)), (3, (108, 46)), (3, (108, 44)), (3, (105, 120)), (3, (105, 112)), (3, (105, 107)), (3, (103, 104)), (3, (103, 101)), (3, (102, 105)), (3, (101, 116)), (3, (101, 109)), (3, (101, 99)), (3, (100, 105)), (3, (99, 104)), (3, (99, 101)), (3, (99, 97)), (3, (97, 115)), (3, (97, 105)), (3, (97, 99)), (3, (39, 114)), (3, (32, 107)), (3, (32, 101)), (2, (148, 116)), (2, (121, 46)), (2, (119, 114)), (2, (119, 104)), (2, (119, 32)), (2, (118, 117)), (2, (117, 39)), (2, (116, 114)), (2, (116, 46)), (2, (116, 44)), (2, (116, 39)), (2, (114, 116)), (2, (112, 111)), (2, (111, 119)), (2, (111, 118)), (2, (111, 98)), (2, (110, 116)), (2, (110, 110)), (2, (110, 105)), (2, (110, 44)), (2, (109, 121)), (2, (108, 117)), (2, (108, 115)), (2, (108, 110)), (2, (108, 100)), (2, (107, 32)), (2, (105, 114)), (2, (105, 103)), (2, (105, 101)), (2, (104, 116)), (2, (104, 114)), (2, (104, 111)), (2, (103, 105)), (2, (103, 97)), (2, (102, 114)), (2, (102, 97)), (2, (101, 105)), (2, (101, 97)), (2, (101, 39)), (2, (100, 46)), (2, (99, 117)), (2, (99, 116)), (2, (99, 114)), (2, (98, 97)), (2, (97, 119)), (2, (97, 107)), (2, (97, 103)), (2, (89, 111)), (2, (66, 117)), (2, (65, 110)), (2, (63, 32)), (2, (59, 32)), (2, (45, 116)), (2, (39, 109)), (2, (32, 118)), (2, (32, 89)), (2, (32, 66)), (2, (32, 65)), (1, (148, 119)), (1, (148, 112)), (1, (148, 97)), (1, (121, 115)), (1, (121, 112)), (1, (121, 39)), (1, (119, 111)), (1, (118, 111)), (1, (118, 97)), (1, (117, 112)), (1, (117, 110)), (1, (117, 105)), (1, (117, 103)), (1, (117, 101)), (1, (117, 100)), (1, (117, 99)), (1, (117, 46)), (1, (117, 44)), (1, (116, 116)), (1, (115, 121)), (1, (115, 112)), (1, (115, 108)), (1, (115, 99)), (1, (115, 97)), (1, (115, 63)), (1, (115, 46)), (1, (114, 117)), (1, (114, 110)), (1, (114, 108)), (1, (114, 45)), (1, (112, 108)), (1, (112, 105)), (1, (112, 97)), (1, (112, 46)), (1, (112, 44)), (1, (111, 112)), (1, (111, 107)), (1, (111, 105)), (1, (111, 101)), (1, (111, 100)), (1, (111, 99)), (1, (110, 121)), (1, (110, 112)), (1, (110, 108)), (1, (110, 99)), (1, (110, 98)), (1, (110, 59)), (1, (110, 45)), (1, (109, 117)), (1, (109, 112)), (1, (109, 111)), (1, (109, 98)), (1, (108, 118)), (1, (108, 116)), (1, (108, 99)), (1, (107, 44)), (1, (106, 117)), (1, (105, 98)), (1, (105, 97)), (1, (103, 226)), (1, (103, 45)), (1, (103, 44)), (1, (102, 108)), (1, (101, 121)), (1, (101, 111)), (1, (101, 103)), (1, (101, 102)), (1, (101, 63)), (1, (101, 59)), (1, (101, 45)), (1, (100, 117)), (1, (100, 44)), (1, (99, 111)), (1, (99, 32)), (1, (98, 121)), (1, (98, 118)), (1, (98, 114)), (1, (98, 105)), (1, (97, 117)), (1, (97, 112)), (1, (85, 110)), (1, (85, 83)), (1, (84, 104)), (1, (83, 116)), (1, (83, 83)), (1, (83, 82)), (1, (82, 32)), (1, (73, 116)), (1, (68, 101)), (1, (45, 111)), (1, (45, 85)), (1, (39, 118)), (1, (39, 100)), (1, (32, 106)), (1, (32, 100)), (1, (32, 85)), (1, (32, 84)), (1, (32, 83)), (1, (32, 68))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\"\n",
    "    Counts occurrences of consecutive element pairs in the given list.\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {}  # Dictionary to store the count of each consecutive pair\n",
    "    \n",
    "    # Iterate through consecutive elements\n",
    "    for pair in zip(ids, ids[1:]):  \n",
    "        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for the pair, initializing to 0 if not present\n",
    "    \n",
    "    return counts \n",
    "\n",
    "stats = get_stats(tokens)\n",
    "#print(stats)\n",
    "print(sorted(((value,key) for key,value in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 111, 111, 108, 32, 109, 256, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 104, 114, 101, 256, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 256, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 256, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 256, 102, 105, 114, 115, 116, 32, 116, 105, 109, 256, 110, 111, 116, 32, 116, 111, 32, 98, 256, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 256, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 256, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 256, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 256, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 105, 118, 256, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 256, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 111, 32, 116, 97, 107, 256, 115, 111, 109, 256, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 256, 103, 111, 116, 32, 116, 119, 101, 108, 118, 256, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 256, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 256, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 256, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 256, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 256, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 256, 115, 101, 110, 115, 256, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 256, 116, 104, 256, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 104, 256, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 256, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 256, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 256, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 256, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 256, 110, 97, 116, 117, 114, 256, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 256, 116, 104, 256, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 256, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 256, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 110, 105, 110, 256, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 256, 116, 105, 109, 256, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 256, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 256, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 256, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 256, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 256, 111, 117, 116, 32, 116, 104, 256, 112, 101, 111, 112, 108, 256, 119, 104, 111, 32, 104, 97, 118, 256, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 256, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1624\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i keep the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #0 (101, 32) `e ` --> 256 `Ā`\n",
      "Merge #1 (32, 116) ` t` --> 257 `ā`\n",
      "Merge #2 (111, 111) `oo` --> 258 `Ă`\n",
      "Merge #3 (105, 110) `in` --> 259 `ă`\n",
      "Merge #4 (32, 97) ` a` --> 260 `Ą`\n",
      "Merge #5 (258, 108) `Ăl` --> 261 `ą`\n",
      "Merge #6 (32, 109) ` m` --> 262 `Ć`\n",
      "Merge #7 (111, 117) `ou` --> 263 `ć`\n",
      "Merge #8 (111, 110) `on` --> 264 `Ĉ`\n",
      "Merge #9 (46, 32) `. ` --> 265 `ĉ`\n",
      "Merge #10 (257, 104) `āh` --> 266 `Ċ`\n",
      "Merge #11 (44, 32) `, ` --> 267 `ċ`\n",
      "Merge #12 (100, 32) `d ` --> 268 `Č`\n",
      "Merge #13 (259, 103) `ăg` --> 269 `č`\n",
      "Merge #14 (104, 97) `ha` --> 270 `Ď`\n",
      "Merge #15 (105, 109) `im` --> 271 `ď`\n",
      "Merge #16 (101, 115) `es` --> 272 `Đ`\n",
      "Merge #17 (256, 116) `Āt` --> 273 `đ`\n",
      "Merge #18 (102, 261) `fą` --> 274 `Ē`\n",
      "Merge #19 (121, 263) `yć` --> 275 `ē`\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"Merge #{i} {pair} `{chr(pair[0])}{chr(pair[1])}` --> {idx} `{chr(idx)}`\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1698\n",
      "ids length: 1243\n",
      "compression ratio: 1.37X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### decoding\n",
    "\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids) #concatenate bytes\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")# errors=\"strict\"\n",
    "  return text\n",
    "\n",
    "print(decode([97]))\n",
    "\n",
    "#not every "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### encoding\n",
    "\n",
    "The other way around: Given a string, what are the tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Encodes a given string into a list of integer tokens using a merge-based encoding scheme.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input string into a list of UTF-8 byte values (tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    # Continue merging tokens until no more valid merges exist\n",
    "    while len(tokens) >= 2:  # Need at least 2 tokens to merge\n",
    "        # Get the frequency of all consecutive token pairs\n",
    "        stats = get_stats(tokens)\n",
    "        # Find the pair with the lowest index in the `merges` dictionary (or inf if not in merges)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "\n",
    "        # If the selected pair is not in the `merges` dictionary, stop merging\n",
    "        if pair not in merges:\n",
    "            break  \n",
    "\n",
    "        # Get the index assigned to this pair from the `merges` dictionary\n",
    "        idx = merges[pair]\n",
    "\n",
    "        # Merge the selected pair into a single token\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "\n",
    "    return tokens  # Return the final list of merged tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 261, 262, 256, 264, 99, 101]\n",
      "FąĆĀĈce\n",
      "Fool me once\n"
     ]
    }
   ],
   "source": [
    "# Example usages:\n",
    "print(encode(\"Fool me once\"))  # Should return an empty list since input is empty\n",
    "print(\"\".join(chr(c) for c in encode(\"Fool me once\"))) \n",
    "print(decode(encode(\"Fool me once\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# trying with another bit https://www.youtube.com/watch?v=7akJJ3Ddxb0\n",
    "valtext = \"I would never call a woman crazy to her face; they hate it, and it makes them go insane. The word 'crazy' can mean too many things. When a man calls a former girlfriend crazy, it could either mean she ripped the head off a rabbit and threw it at my door, threatened to kill me, and burnt all of my things, or it could mean she got a little too upset when she found out I was sleeping with a prostitute. It could mean anything to a man. We have a term for us as well that's too broad: 'creepy.' Creepy could be a man who has trouble looking you in the eye and stands a bit too close on a bus, or it could be a man who rapes his whole family in a dungeon prison under his house. It's too broad a term. Ladies, 'he's a creepy guy'—does he not wash, or does he kill? It's the sort of crazy use of language we've been complaining about.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced splits using regex patterns (GPT series)\n",
    "\n",
    "\n",
    "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "We observed BPE including many versions of common words like ```dog``` since they occur in many variations such as ```dog.``` ```dog!```\n",
    "```dog?```. This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding\n",
    "only minimal fragmentation of words across multiple vocab tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gpt-2\n",
    "\n",
    "\n",
    "``` python\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. **Contractions**\n",
    "\n",
    "```python\n",
    "'s|'t|'re|'ve|'m|'ll|'d\n",
    "```\n",
    "\n",
    "Matches: Common English contractions like 's, 't, 're, etc.\n",
    "\n",
    "Purpose: Ensures contractions are split into separate tokens (e.g., \"don't\" → [\"don\", \"'t\"]).\n",
    "\n",
    "\n",
    "#### 2. **Words**  \n",
    "```python\n",
    "?\\p{L}+\n",
    "```\n",
    "\n",
    "?: Optional leading space (matches 0 or 1 space).\n",
    "\n",
    "\\p{L}+: One or more Unicode letters.\n",
    "\n",
    "Matches: Words with or without a leading space (e.g., \"hello\" or \" hello\").\n",
    "\n",
    "#### 3. **Numbers**  \n",
    "\n",
    "```python\n",
    "?\\p{N}+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "\\p{N}+: One or more Unicode numbers.\n",
    "\n",
    "Matches: Numbers with or without a leading space (e.g., \"123\" or \" 456\").\n",
    "\n",
    "\n",
    "### 4 Symbols/Punctuation\n",
    "\n",
    "```python\n",
    "?[^\\s\\p{L}\\p{N}]+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "[^\\s\\p{L}\\p{N}]: Any character that is not whitespace, a letter, or a number.\n",
    "\n",
    "Matches: Symbols, punctuation, or emojis with optional leading space (e.g., \"!\", \" 😀\").\n",
    "\n",
    "### 5 Trailing whitespace\n",
    "\n",
    "```python\n",
    "\\s+(?!\\S)\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "(?!\\S): Negative lookahead to ensure whitespace is not followed by non-whitespace.\n",
    "\n",
    "Matches: Whitespace at the end of a string or line (e.g., \" \" in \"hello \").\n",
    "\n",
    "### 6 General Whitespace\n",
    "\n",
    "```python\n",
    "\\s+\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "Matches: Remaining whitespace not captured by earlier rules (e.g., spaces between words).\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "**Order Matters**: The regex engine tries alternatives from left to right. Contractions are prioritized first.\n",
    "\n",
    "**Single vs. Multiple Spaces**:\n",
    "\n",
    "A single leading space is included with a word/number/symbol (e.g., \" hello\" → one token).\n",
    "\n",
    "Multiple spaces are split into separate tokens (e.g., \" hello\" → [\" \", \"hello\"]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install regex\n",
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ve\", ' eating', ' 3', ' apples']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I've eating 3 apples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'VE', ' EATING', ' 3', ' APPLES']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I'VE EATING 3 APPLES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'example', ' =', ' \"', '\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n       ', ' \"', '\\n', 'print', '(', 're', '.', 'findall', '(', 'gpt', '2', 'pat', ',', ' example', '))', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "example = \"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "        \"\n",
    "print(re.findall(gpt2pat, example))\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "**Why can't LLM spell words**  \n",
    "LLMs see words as token chunks, not letter by letter, so unusual spellings may split into strange token combinations the model hasn't seen together often.  \n",
    "\n",
    "**Why can't LLM do super simple string processing tasks like reversing a string**  \n",
    "LLMs process text as tokens, not individual characters, making character-by-character operations like string reversal difficult.  \n",
    "\n",
    "**Why is LLM worse at non-English languages (e.g. Japanese)**  \n",
    "Languages like Japanese tokenize inefficiently, often requiring more tokens per word than English, giving the model less context to work with.  \n",
    "\n",
    "**Why is LLM bad at simple arithmetic**  \n",
    "Numbers get split into multiple tokens based on digit patterns, making it harder for the model to understand numerical relationships.  \n",
    "\n",
    "**Why did GPT-2 have more than necessary trouble coding in Python**  \n",
    "Code tokens were less common in GPT-2's training data, and programming syntax often gets split in unintuitive ways. For example, indentation spaces occupy a huge part of the context window.  \n",
    "\n",
    "**Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"**  \n",
    "This specific string is a special token used during training to mark the end of documents, so models interpret it as a signal to stop generating.  \n",
    "\n",
    "**Why does the LLM break if I ask it about \"SolidGoldMagikarp\"**  \n",
    "This specific string became famous because in early GPT models, it tokenized into an unusual pattern that caused the model to produce repetitive or nonsensical outputs. A hypothesis is that it's related to a Reddit username.  \n",
    "\n",
    "**Why should I prefer to use YAML over JSON with LLMs**  \n",
    "YAML formatting creates more natural language-like tokens than JSON's special characters and strict syntax.  \n",
    "\n",
    "\n",
    "\n",
    "Good tokenization web app: [https://tiktokenizer.vercel.app](https://tiktokenizer.vercel.app)\n",
    "\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Byte-pair encoding\n",
    "\n",
    "https://en.wikipedia.org/wiki/Byte_pair_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six timesâ€”probably six timesâ€”a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer foolingâ€”this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine timesâ€”well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten timesâ€”the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\n",
      "length: 1688\n",
      "---\n",
      "[70, 111, 111, 108, 32, 109, 101, 32, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 116, 104, 114, 101, 101, 32, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 101, 32, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 101, 32, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 101, 32, 102, 105, 114, 115, 116, 32, 116, 105, 109, 101, 32, 110, 111, 116, 32, 116, 111, 32, 98, 101, 32, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 101, 32, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 32, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 101, 32, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 101, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 101, 32, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 101, 32, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 102, 105, 118, 101, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 101, 32, 111, 110, 32, 109, 101, 32, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 101, 32, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 111, 32, 116, 97, 107, 101, 32, 115, 111, 109, 101, 32, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 101, 32, 103, 111, 116, 32, 116, 119, 101, 108, 118, 101, 32, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 101, 32, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 101, 32, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 101, 32, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 101, 32, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 101, 32, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 101, 32, 115, 101, 110, 115, 101, 32, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 101, 32, 116, 104, 101, 32, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 101, 32, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 101, 32, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 101, 32, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 101, 32, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 101, 32, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 101, 32, 110, 97, 116, 117, 114, 101, 32, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 101, 32, 116, 104, 101, 32, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 101, 32, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 101, 32, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 110, 105, 110, 101, 32, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 101, 32, 116, 105, 109, 101, 32, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 101, 32, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 101, 32, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 101, 32, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 101, 32, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 101, 32, 111, 117, 116, 32, 116, 104, 101, 32, 112, 101, 111, 112, 108, 101, 32, 119, 104, 111, 32, 104, 97, 118, 101, 32, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 101, 32, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 101, 32, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 101, 32, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1698\n"
     ]
    }
   ],
   "source": [
    "#As usual, i change the original text just for fun\n",
    "# Using this awesome bit of James Donald Forbes McCann --> https://www.youtube.com/watch?v=udSMZG_L-S0\n",
    "text=\"Fool me once, shame on you. Fool me twice, shame on me. Fool me three times, there's twice as much shame on me. I cannot believe I allowed you to fool me again. Definitely learned from the first time not to be fooled. Fool me four times, shame back on you, actually. You are picking on a vulnerable man; something has obviously gone wrong with me. This is like bullying the kid in a wheelchair at primary school. It's like bullying the fat kid, bullying the kid with a limp. Four times? You're gonna fool me? Unbelievable. Fool me five times, shame on me again. I mean, I'm vulnerable, but at some point, you have to take some personal responsibility, for crying out loud. I've got twelve of these; they're good for eight. Fool me six timesâ€”probably six timesâ€”a fool. And I have lured you into my trap, pretending to be a fool six consecutive times to give you a false sense of security, only to flip it, and now you are the fool, and you have the shame. Fool me seven times, you saw through my trick, but there's no shame, because I'm getting fooled by the best. Fool me eight times, and this is no longer foolingâ€”this is systematic cruelty. And rather than allocating shame or even looking at you as an individual, I'd like you to unpack the nature of your fooling, remove the fool privilege that you're bringing to the situation, and build a freer world for us all. But fool me nine timesâ€”well, that's one time too many, and I will rise up with all the other members of the Foolertariat to install a dictatorship of the fools and wipe out the people who have been fooling us. But fool me ten timesâ€”the revolution goes awry, just in a sort of Stalin-taking-over-the-USSR type situation.\"\n",
    "\n",
    "\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n",
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print('---')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(74, (101, 32)), (55, (32, 116)), (42, (109, 101)), (33, (116, 104)), (30, (32, 97)), (27, (111, 111)), (27, (105, 110)), (25, (111, 108)), (25, (44, 32)), (25, (32, 102)), (23, (104, 101)), (23, (32, 115)), (23, (32, 109)), (22, (111, 117)), (20, (116, 105)), (20, (32, 111)), (19, (116, 32)), (19, (111, 110)), (19, (46, 32)), (18, (115, 32)), (18, (110, 103)), (18, (108, 32)), (18, (102, 111)), (18, (100, 32)), (17, (104, 97)), (15, (114, 101)), (15, (110, 32)), (15, (105, 109)), (15, (101, 115)), (15, (32, 98)), (14, (118, 101)), (14, (111, 32)), (14, (97, 116)), (13, (121, 111)), (13, (103, 32)), (13, (32, 121)), (12, (116, 111)), (12, (114, 32)), (12, (108, 105)), (12, (101, 114)), (12, (97, 110)), (11, (108, 108)), (10, (117, 32)), (10, (110, 100)), (10, (97, 108)), (10, (97, 32)), (10, (70, 111)), (10, (32, 73)), (9, (115, 104)), (9, (115, 101)), (9, (108, 101)), (9, (105, 116)), (9, (32, 119)), (9, (32, 108)), (9, (32, 105)), (9, (32, 70)), (8, (121, 32)), (8, (117, 116)), (8, (108, 121)), (8, (98, 101)), (8, (97, 109)), (7, (119, 105)), (7, (114, 105)), (7, (111, 114)), (7, (105, 115)), (7, (101, 108)), (7, (32, 112)), (7, (32, 103)), (6, (117, 114)), (6, (116, 97)), (6, (115, 105)), (6, (111, 102)), (6, (110, 101)), (6, (107, 105)), (6, (105, 99)), (6, (102, 32)), (6, (101, 110)), (6, (98, 117)), (6, (32, 110)), (5, (226, 128)), (5, (128, 148)), (5, (117, 115)), (5, (117, 108)), (5, (115, 116)), (5, (115, 111)), (5, (115, 44)), (5, (110, 111)), (5, (108, 111)), (5, (105, 118)), (5, (104, 105)), (5, (104, 32)), (5, (103, 111)), (5, (101, 118)), (5, (101, 100)), (5, (101, 46)), (5, (97, 114)), (5, (73, 32)), (5, (32, 114)), (5, (32, 104)), (4, (121, 105)), (4, (121, 44)), (4, (117, 97)), (4, (116, 121)), (4, (116, 117)), (4, (116, 101)), (4, (115, 226)), (4, (114, 115)), (4, (114, 111)), (4, (114, 97)), (4, (112, 114)), (4, (112, 101)), (4, (111, 116)), (4, (111, 109)), (4, (110, 115)), (4, (109, 97)), (4, (107, 101)), (4, (105, 111)), (4, (105, 108)), (4, (105, 100)), (4, (101, 101)), (4, (101, 44)), (4, (99, 107)), (4, (98, 108)), (4, (97, 118)), (4, (97, 98)), (4, (73, 39)), (4, (39, 115)), (4, (32, 117)), (4, (32, 99)), (3, (120, 32)), (3, (119, 101)), (3, (118, 105)), (3, (116, 119)), (3, (114, 121)), (3, (112, 32)), (3, (110, 97)), (3, (110, 46)), (3, (109, 32)), (3, (108, 46)), (3, (108, 44)), (3, (105, 120)), (3, (105, 112)), (3, (105, 107)), (3, (103, 104)), (3, (103, 101)), (3, (102, 105)), (3, (101, 116)), (3, (101, 109)), (3, (101, 99)), (3, (100, 105)), (3, (99, 104)), (3, (99, 101)), (3, (99, 97)), (3, (97, 115)), (3, (97, 105)), (3, (97, 99)), (3, (39, 114)), (3, (32, 107)), (3, (32, 101)), (2, (148, 116)), (2, (121, 46)), (2, (119, 114)), (2, (119, 104)), (2, (119, 32)), (2, (118, 117)), (2, (117, 39)), (2, (116, 114)), (2, (116, 46)), (2, (116, 44)), (2, (116, 39)), (2, (114, 116)), (2, (112, 111)), (2, (111, 119)), (2, (111, 118)), (2, (111, 98)), (2, (110, 116)), (2, (110, 110)), (2, (110, 105)), (2, (110, 44)), (2, (109, 121)), (2, (108, 117)), (2, (108, 115)), (2, (108, 110)), (2, (108, 100)), (2, (107, 32)), (2, (105, 114)), (2, (105, 103)), (2, (105, 101)), (2, (104, 116)), (2, (104, 114)), (2, (104, 111)), (2, (103, 105)), (2, (103, 97)), (2, (102, 114)), (2, (102, 97)), (2, (101, 105)), (2, (101, 97)), (2, (101, 39)), (2, (100, 46)), (2, (99, 117)), (2, (99, 116)), (2, (99, 114)), (2, (98, 97)), (2, (97, 119)), (2, (97, 107)), (2, (97, 103)), (2, (89, 111)), (2, (66, 117)), (2, (65, 110)), (2, (63, 32)), (2, (59, 32)), (2, (45, 116)), (2, (39, 109)), (2, (32, 118)), (2, (32, 89)), (2, (32, 66)), (2, (32, 65)), (1, (148, 119)), (1, (148, 112)), (1, (148, 97)), (1, (121, 115)), (1, (121, 112)), (1, (121, 39)), (1, (119, 111)), (1, (118, 111)), (1, (118, 97)), (1, (117, 112)), (1, (117, 110)), (1, (117, 105)), (1, (117, 103)), (1, (117, 101)), (1, (117, 100)), (1, (117, 99)), (1, (117, 46)), (1, (117, 44)), (1, (116, 116)), (1, (115, 121)), (1, (115, 112)), (1, (115, 108)), (1, (115, 99)), (1, (115, 97)), (1, (115, 63)), (1, (115, 46)), (1, (114, 117)), (1, (114, 110)), (1, (114, 108)), (1, (114, 45)), (1, (112, 108)), (1, (112, 105)), (1, (112, 97)), (1, (112, 46)), (1, (112, 44)), (1, (111, 112)), (1, (111, 107)), (1, (111, 105)), (1, (111, 101)), (1, (111, 100)), (1, (111, 99)), (1, (110, 121)), (1, (110, 112)), (1, (110, 108)), (1, (110, 99)), (1, (110, 98)), (1, (110, 59)), (1, (110, 45)), (1, (109, 117)), (1, (109, 112)), (1, (109, 111)), (1, (109, 98)), (1, (108, 118)), (1, (108, 116)), (1, (108, 99)), (1, (107, 44)), (1, (106, 117)), (1, (105, 98)), (1, (105, 97)), (1, (103, 226)), (1, (103, 45)), (1, (103, 44)), (1, (102, 108)), (1, (101, 121)), (1, (101, 111)), (1, (101, 103)), (1, (101, 102)), (1, (101, 63)), (1, (101, 59)), (1, (101, 45)), (1, (100, 117)), (1, (100, 44)), (1, (99, 111)), (1, (99, 32)), (1, (98, 121)), (1, (98, 118)), (1, (98, 114)), (1, (98, 105)), (1, (97, 117)), (1, (97, 112)), (1, (85, 110)), (1, (85, 83)), (1, (84, 104)), (1, (83, 116)), (1, (83, 83)), (1, (83, 82)), (1, (82, 32)), (1, (73, 116)), (1, (68, 101)), (1, (45, 111)), (1, (45, 85)), (1, (39, 118)), (1, (39, 100)), (1, (32, 106)), (1, (32, 100)), (1, (32, 85)), (1, (32, 84)), (1, (32, 83)), (1, (32, 68))]\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    \"\"\"\n",
    "    Counts occurrences of consecutive element pairs in the given list.\n",
    "    \"\"\"\n",
    "    \n",
    "    counts = {}  # Dictionary to store the count of each consecutive pair\n",
    "    \n",
    "    # Iterate through consecutive elements\n",
    "    for pair in zip(ids, ids[1:]):  \n",
    "        counts[pair] = counts.get(pair, 0) + 1  # Increment the count for the pair, initializing to 0 if not present\n",
    "    \n",
    "    return counts \n",
    "\n",
    "stats = get_stats(tokens)\n",
    "#print(stats)\n",
    "print(sorted(((value,key) for key,value in stats.items()), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('e', ' ')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(101), chr(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 111, 111, 108, 32, 109, 256, 111, 110, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 121, 111, 117, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 119, 105, 99, 101, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 116, 104, 114, 101, 256, 116, 105, 109, 101, 115, 44, 32, 116, 104, 101, 114, 101, 39, 115, 32, 116, 119, 105, 99, 256, 97, 115, 32, 109, 117, 99, 104, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 101, 46, 32, 73, 32, 99, 97, 110, 110, 111, 116, 32, 98, 101, 108, 105, 101, 118, 256, 73, 32, 97, 108, 108, 111, 119, 101, 100, 32, 121, 111, 117, 32, 116, 111, 32, 102, 111, 111, 108, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 68, 101, 102, 105, 110, 105, 116, 101, 108, 121, 32, 108, 101, 97, 114, 110, 101, 100, 32, 102, 114, 111, 109, 32, 116, 104, 256, 102, 105, 114, 115, 116, 32, 116, 105, 109, 256, 110, 111, 116, 32, 116, 111, 32, 98, 256, 102, 111, 111, 108, 101, 100, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 111, 117, 114, 32, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 98, 97, 99, 107, 32, 111, 110, 32, 121, 111, 117, 44, 32, 97, 99, 116, 117, 97, 108, 108, 121, 46, 32, 89, 111, 117, 32, 97, 114, 256, 112, 105, 99, 107, 105, 110, 103, 32, 111, 110, 32, 97, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 256, 109, 97, 110, 59, 32, 115, 111, 109, 101, 116, 104, 105, 110, 103, 32, 104, 97, 115, 32, 111, 98, 118, 105, 111, 117, 115, 108, 121, 32, 103, 111, 110, 256, 119, 114, 111, 110, 103, 32, 119, 105, 116, 104, 32, 109, 101, 46, 32, 84, 104, 105, 115, 32, 105, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 105, 110, 32, 97, 32, 119, 104, 101, 101, 108, 99, 104, 97, 105, 114, 32, 97, 116, 32, 112, 114, 105, 109, 97, 114, 121, 32, 115, 99, 104, 111, 111, 108, 46, 32, 73, 116, 39, 115, 32, 108, 105, 107, 256, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 102, 97, 116, 32, 107, 105, 100, 44, 32, 98, 117, 108, 108, 121, 105, 110, 103, 32, 116, 104, 256, 107, 105, 100, 32, 119, 105, 116, 104, 32, 97, 32, 108, 105, 109, 112, 46, 32, 70, 111, 117, 114, 32, 116, 105, 109, 101, 115, 63, 32, 89, 111, 117, 39, 114, 256, 103, 111, 110, 110, 97, 32, 102, 111, 111, 108, 32, 109, 101, 63, 32, 85, 110, 98, 101, 108, 105, 101, 118, 97, 98, 108, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 102, 105, 118, 256, 116, 105, 109, 101, 115, 44, 32, 115, 104, 97, 109, 256, 111, 110, 32, 109, 256, 97, 103, 97, 105, 110, 46, 32, 73, 32, 109, 101, 97, 110, 44, 32, 73, 39, 109, 32, 118, 117, 108, 110, 101, 114, 97, 98, 108, 101, 44, 32, 98, 117, 116, 32, 97, 116, 32, 115, 111, 109, 256, 112, 111, 105, 110, 116, 44, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 111, 32, 116, 97, 107, 256, 115, 111, 109, 256, 112, 101, 114, 115, 111, 110, 97, 108, 32, 114, 101, 115, 112, 111, 110, 115, 105, 98, 105, 108, 105, 116, 121, 44, 32, 102, 111, 114, 32, 99, 114, 121, 105, 110, 103, 32, 111, 117, 116, 32, 108, 111, 117, 100, 46, 32, 73, 39, 118, 256, 103, 111, 116, 32, 116, 119, 101, 108, 118, 256, 111, 102, 32, 116, 104, 101, 115, 101, 59, 32, 116, 104, 101, 121, 39, 114, 256, 103, 111, 111, 100, 32, 102, 111, 114, 32, 101, 105, 103, 104, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 112, 114, 111, 98, 97, 98, 108, 121, 32, 115, 105, 120, 32, 116, 105, 109, 101, 115, 226, 128, 148, 97, 32, 102, 111, 111, 108, 46, 32, 65, 110, 100, 32, 73, 32, 104, 97, 118, 256, 108, 117, 114, 101, 100, 32, 121, 111, 117, 32, 105, 110, 116, 111, 32, 109, 121, 32, 116, 114, 97, 112, 44, 32, 112, 114, 101, 116, 101, 110, 100, 105, 110, 103, 32, 116, 111, 32, 98, 256, 97, 32, 102, 111, 111, 108, 32, 115, 105, 120, 32, 99, 111, 110, 115, 101, 99, 117, 116, 105, 118, 256, 116, 105, 109, 101, 115, 32, 116, 111, 32, 103, 105, 118, 256, 121, 111, 117, 32, 97, 32, 102, 97, 108, 115, 256, 115, 101, 110, 115, 256, 111, 102, 32, 115, 101, 99, 117, 114, 105, 116, 121, 44, 32, 111, 110, 108, 121, 32, 116, 111, 32, 102, 108, 105, 112, 32, 105, 116, 44, 32, 97, 110, 100, 32, 110, 111, 119, 32, 121, 111, 117, 32, 97, 114, 256, 116, 104, 256, 102, 111, 111, 108, 44, 32, 97, 110, 100, 32, 121, 111, 117, 32, 104, 97, 118, 256, 116, 104, 256, 115, 104, 97, 109, 101, 46, 32, 70, 111, 111, 108, 32, 109, 256, 115, 101, 118, 101, 110, 32, 116, 105, 109, 101, 115, 44, 32, 121, 111, 117, 32, 115, 97, 119, 32, 116, 104, 114, 111, 117, 103, 104, 32, 109, 121, 32, 116, 114, 105, 99, 107, 44, 32, 98, 117, 116, 32, 116, 104, 101, 114, 101, 39, 115, 32, 110, 111, 32, 115, 104, 97, 109, 101, 44, 32, 98, 101, 99, 97, 117, 115, 256, 73, 39, 109, 32, 103, 101, 116, 116, 105, 110, 103, 32, 102, 111, 111, 108, 101, 100, 32, 98, 121, 32, 116, 104, 256, 98, 101, 115, 116, 46, 32, 70, 111, 111, 108, 32, 109, 256, 101, 105, 103, 104, 116, 32, 116, 105, 109, 101, 115, 44, 32, 97, 110, 100, 32, 116, 104, 105, 115, 32, 105, 115, 32, 110, 111, 32, 108, 111, 110, 103, 101, 114, 32, 102, 111, 111, 108, 105, 110, 103, 226, 128, 148, 116, 104, 105, 115, 32, 105, 115, 32, 115, 121, 115, 116, 101, 109, 97, 116, 105, 99, 32, 99, 114, 117, 101, 108, 116, 121, 46, 32, 65, 110, 100, 32, 114, 97, 116, 104, 101, 114, 32, 116, 104, 97, 110, 32, 97, 108, 108, 111, 99, 97, 116, 105, 110, 103, 32, 115, 104, 97, 109, 256, 111, 114, 32, 101, 118, 101, 110, 32, 108, 111, 111, 107, 105, 110, 103, 32, 97, 116, 32, 121, 111, 117, 32, 97, 115, 32, 97, 110, 32, 105, 110, 100, 105, 118, 105, 100, 117, 97, 108, 44, 32, 73, 39, 100, 32, 108, 105, 107, 256, 121, 111, 117, 32, 116, 111, 32, 117, 110, 112, 97, 99, 107, 32, 116, 104, 256, 110, 97, 116, 117, 114, 256, 111, 102, 32, 121, 111, 117, 114, 32, 102, 111, 111, 108, 105, 110, 103, 44, 32, 114, 101, 109, 111, 118, 256, 116, 104, 256, 102, 111, 111, 108, 32, 112, 114, 105, 118, 105, 108, 101, 103, 256, 116, 104, 97, 116, 32, 121, 111, 117, 39, 114, 256, 98, 114, 105, 110, 103, 105, 110, 103, 32, 116, 111, 32, 116, 104, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 44, 32, 97, 110, 100, 32, 98, 117, 105, 108, 100, 32, 97, 32, 102, 114, 101, 101, 114, 32, 119, 111, 114, 108, 100, 32, 102, 111, 114, 32, 117, 115, 32, 97, 108, 108, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 110, 105, 110, 256, 116, 105, 109, 101, 115, 226, 128, 148, 119, 101, 108, 108, 44, 32, 116, 104, 97, 116, 39, 115, 32, 111, 110, 256, 116, 105, 109, 256, 116, 111, 111, 32, 109, 97, 110, 121, 44, 32, 97, 110, 100, 32, 73, 32, 119, 105, 108, 108, 32, 114, 105, 115, 256, 117, 112, 32, 119, 105, 116, 104, 32, 97, 108, 108, 32, 116, 104, 256, 111, 116, 104, 101, 114, 32, 109, 101, 109, 98, 101, 114, 115, 32, 111, 102, 32, 116, 104, 256, 70, 111, 111, 108, 101, 114, 116, 97, 114, 105, 97, 116, 32, 116, 111, 32, 105, 110, 115, 116, 97, 108, 108, 32, 97, 32, 100, 105, 99, 116, 97, 116, 111, 114, 115, 104, 105, 112, 32, 111, 102, 32, 116, 104, 256, 102, 111, 111, 108, 115, 32, 97, 110, 100, 32, 119, 105, 112, 256, 111, 117, 116, 32, 116, 104, 256, 112, 101, 111, 112, 108, 256, 119, 104, 111, 32, 104, 97, 118, 256, 98, 101, 101, 110, 32, 102, 111, 111, 108, 105, 110, 103, 32, 117, 115, 46, 32, 66, 117, 116, 32, 102, 111, 111, 108, 32, 109, 256, 116, 101, 110, 32, 116, 105, 109, 101, 115, 226, 128, 148, 116, 104, 256, 114, 101, 118, 111, 108, 117, 116, 105, 111, 110, 32, 103, 111, 101, 115, 32, 97, 119, 114, 121, 44, 32, 106, 117, 115, 116, 32, 105, 110, 32, 97, 32, 115, 111, 114, 116, 32, 111, 102, 32, 83, 116, 97, 108, 105, 110, 45, 116, 97, 107, 105, 110, 103, 45, 111, 118, 101, 114, 45, 116, 104, 101, 45, 85, 83, 83, 82, 32, 116, 121, 112, 256, 115, 105, 116, 117, 97, 116, 105, 111, 110, 46]\n",
      "length: 1624\n"
     ]
    }
   ],
   "source": [
    "def merge(ids, pair, idx):\n",
    "  # in the list of ints (ids), replace all consecutive occurences of pair with the new token idx\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    # if we are not at the very last position AND the pair matches, replace it\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "top_pair = max(stats, key=stats.get)\n",
    "tokens2 = merge(tokens, top_pair, 256)\n",
    "print(tokens2)\n",
    "print(\"length:\", len(tokens2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i keep the original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merge #0 (101, 32) `e ` --> 256 `Ä€`\n",
      "Merge #1 (32, 116) ` t` --> 257 `Ä`\n",
      "Merge #2 (111, 111) `oo` --> 258 `Ä‚`\n",
      "Merge #3 (105, 110) `in` --> 259 `Äƒ`\n",
      "Merge #4 (32, 97) ` a` --> 260 `Ä„`\n",
      "Merge #5 (258, 108) `Ä‚l` --> 261 `Ä…`\n",
      "Merge #6 (32, 109) ` m` --> 262 `Ä†`\n",
      "Merge #7 (111, 117) `ou` --> 263 `Ä‡`\n",
      "Merge #8 (111, 110) `on` --> 264 `Äˆ`\n",
      "Merge #9 (46, 32) `. ` --> 265 `Ä‰`\n",
      "Merge #10 (257, 104) `Äh` --> 266 `ÄŠ`\n",
      "Merge #11 (44, 32) `, ` --> 267 `Ä‹`\n",
      "Merge #12 (100, 32) `d ` --> 268 `ÄŒ`\n",
      "Merge #13 (259, 103) `Äƒg` --> 269 `Ä`\n",
      "Merge #14 (104, 97) `ha` --> 270 `ÄŽ`\n",
      "Merge #15 (105, 109) `im` --> 271 `Ä`\n",
      "Merge #16 (101, 115) `es` --> 272 `Ä`\n",
      "Merge #17 (256, 116) `Ä€t` --> 273 `Ä‘`\n",
      "Merge #18 (102, 261) `fÄ…` --> 274 `Ä’`\n",
      "Merge #19 (121, 263) `yÄ‡` --> 275 `Ä“`\n"
     ]
    }
   ],
   "source": [
    "def get_stats(ids):\n",
    "    counts = {}\n",
    "    for pair in zip(ids, ids[1:]):\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "def merge(ids, pair, idx):\n",
    "  newids = []\n",
    "  i = 0\n",
    "  while i < len(ids):\n",
    "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
    "      newids.append(idx)\n",
    "      i += 2\n",
    "    else:\n",
    "      newids.append(ids[i])\n",
    "      i += 1\n",
    "  return newids\n",
    "\n",
    "# ---\n",
    "vocab_size = 276 # the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "ids = list(tokens) # copy so we don't destroy the original list\n",
    "\n",
    "merges = {} # (int, int) -> int\n",
    "for i in range(num_merges):\n",
    "  stats = get_stats(ids)\n",
    "  pair = max(stats, key=stats.get)\n",
    "  idx = 256 + i\n",
    "  print(f\"Merge #{i} {pair} `{chr(pair[0])}{chr(pair[1])}` --> {idx} `{chr(idx)}`\")\n",
    "  ids = merge(ids, pair, idx)\n",
    "  merges[pair] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens length: 1698\n",
      "ids length: 1243\n",
      "compression ratio: 1.37X\n"
     ]
    }
   ],
   "source": [
    "print(\"tokens length:\", len(tokens))\n",
    "print(\"ids length:\", len(ids))\n",
    "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tokenizer is a completely separate, independent module from the LLM. It has its own training dataset of text (which could be different from that of the LLM), on which you train the vocabulary using the Byte Pair Encoding (BPE) algorithm. It then translates back and forth between raw text and sequences of tokens. The LLM later only ever sees the tokens and never directly deals with any text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____\n",
    "### decoding\n",
    "\n",
    "Given a sequence of integers in the range [0, vocab_size], what is the text?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n"
     ]
    }
   ],
   "source": [
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "  # given ids (list of integers), return Python string\n",
    "  tokens = b\"\".join(vocab[idx] for idx in ids) #concatenate bytes\n",
    "  text = tokens.decode(\"utf-8\", errors=\"replace\")# errors=\"strict\"\n",
    "  return text\n",
    "\n",
    "print(decode([97]))\n",
    "\n",
    "#not every "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### encoding\n",
    "\n",
    "The other way around: Given a string, what are the tokens?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    \"\"\"\n",
    "    Encodes a given string into a list of integer tokens using a merge-based encoding scheme.\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert the input string into a list of UTF-8 byte values (tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "    # Continue merging tokens until no more valid merges exist\n",
    "    while len(tokens) >= 2:  # Need at least 2 tokens to merge\n",
    "        # Get the frequency of all consecutive token pairs\n",
    "        stats = get_stats(tokens)\n",
    "        # Find the pair with the lowest index in the `merges` dictionary (or inf if not in merges)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "\n",
    "        # If the selected pair is not in the `merges` dictionary, stop merging\n",
    "        if pair not in merges:\n",
    "            break  \n",
    "\n",
    "        # Get the index assigned to this pair from the `merges` dictionary\n",
    "        idx = merges[pair]\n",
    "\n",
    "        # Merge the selected pair into a single token\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "\n",
    "    return tokens  # Return the final list of merged tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[70, 261, 262, 256, 264, 99, 101]\n",
      "FÄ…Ä†Ä€Äˆce\n",
      "Fool me once\n"
     ]
    }
   ],
   "source": [
    "# Example usages:\n",
    "print(encode(\"Fool me once\"))  # Should return an empty list since input is empty\n",
    "print(\"\".join(chr(c) for c in encode(\"Fool me once\"))) \n",
    "print(decode(encode(\"Fool me once\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "text2 = decode(encode(text))\n",
    "print(text2 == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# trying with another bit https://www.youtube.com/watch?v=7akJJ3Ddxb0\n",
    "valtext = \"I would never call a woman crazy to her face; they hate it, and it makes them go insane. The word 'crazy' can mean too many things. When a man calls a former girlfriend crazy, it could either mean she ripped the head off a rabbit and threw it at my door, threatened to kill me, and burnt all of my things, or it could mean she got a little too upset when she found out I was sleeping with a prostitute. It could mean anything to a man. We have a term for us as well that's too broad: 'creepy.' Creepy could be a man who has trouble looking you in the eye and stands a bit too close on a bus, or it could be a man who rapes his whole family in a dungeon prison under his house. It's too broad a term. Ladies, 'he's a creepy guy'â€”does he not wash, or does he kill? It's the sort of crazy use of language we've been complaining about.\"\n",
    "valtext2 = decode(encode(valtext))\n",
    "print(valtext2 == valtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forced splits using regex patterns (GPT series)\n",
    "\n",
    "\n",
    "https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf\n",
    "\n",
    "We observed BPE including many versions of common words like ```dog``` since they occur in many variations such as ```dog.``` ```dog!```\n",
    "```dog?```. This results in a sub-optimal allocation of limited vocabulary slots and model capacity. To avoid this, we prevent BPE from merging across character categories for any byte sequence. We add an exception for spaces which significantly improves the compression efficiency while adding\n",
    "only minimal fragmentation of words across multiple vocab tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/openai/gpt-2\n",
    "\n",
    "\n",
    "``` python\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.errors = errors # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v:k for k, v in self.byte_encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 1. **Contractions**\n",
    "\n",
    "```python\n",
    "'s|'t|'re|'ve|'m|'ll|'d\n",
    "```\n",
    "\n",
    "Matches: Common English contractions like 's, 't, 're, etc.\n",
    "\n",
    "Purpose: Ensures contractions are split into separate tokens (e.g., \"don't\" â†’ [\"don\", \"'t\"]).\n",
    "\n",
    "\n",
    "#### 2. **Words**  \n",
    "```python\n",
    "?\\p{L}+\n",
    "```\n",
    "\n",
    "?: Optional leading space (matches 0 or 1 space).\n",
    "\n",
    "\\p{L}+: One or more Unicode letters.\n",
    "\n",
    "Matches: Words with or without a leading space (e.g., \"hello\" or \" hello\").\n",
    "\n",
    "#### 3. **Numbers**  \n",
    "\n",
    "```python\n",
    "?\\p{N}+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "\\p{N}+: One or more Unicode numbers.\n",
    "\n",
    "Matches: Numbers with or without a leading space (e.g., \"123\" or \" 456\").\n",
    "\n",
    "\n",
    "### 4 Symbols/Punctuation\n",
    "\n",
    "```python\n",
    "?[^\\s\\p{L}\\p{N}]+\n",
    "```\n",
    "\n",
    "?: Optional leading space.\n",
    "\n",
    "[^\\s\\p{L}\\p{N}]: Any character that is not whitespace, a letter, or a number.\n",
    "\n",
    "Matches: Symbols, punctuation, or emojis with optional leading space (e.g., \"!\", \" ðŸ˜€\").\n",
    "\n",
    "### 5 Trailing whitespace\n",
    "\n",
    "```python\n",
    "\\s+(?!\\S)\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "(?!\\S): Negative lookahead to ensure whitespace is not followed by non-whitespace.\n",
    "\n",
    "Matches: Whitespace at the end of a string or line (e.g., \" \" in \"hello \").\n",
    "\n",
    "### 6 General Whitespace\n",
    "\n",
    "```python\n",
    "\\s+\n",
    "```\n",
    "\n",
    "\\s+: One or more whitespace characters.\n",
    "\n",
    "Matches: Remaining whitespace not captured by earlier rules (e.g., spaces between words).\n",
    "\n",
    "\n",
    "____\n",
    "\n",
    "**Order Matters**: The regex engine tries alternatives from left to right. Contractions are prioritized first.\n",
    "\n",
    "**Single vs. Multiple Spaces**:\n",
    "\n",
    "A single leading space is included with a word/number/symbol (e.g., \" hello\" â†’ one token).\n",
    "\n",
    "Multiple spaces are split into separate tokens (e.g., \" hello\" â†’ [\" \", \"hello\"]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install regex\n",
    "import regex as re\n",
    "\n",
    "gpt2pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' world']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'ve\", ' eating', ' 3', ' apples']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I've eating 3 apples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'VE', ' EATING', ' 3', ' APPLES']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(gpt2pat, \"I'VE EATING 3 APPLES\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'example', ' =', ' \"', '\\n', 'for', ' i', ' in', ' range', '(', '1', ',', ' 101', '):', '\\n   ', ' if', ' i', ' %', ' 3', ' ==', ' 0', ' and', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'FizzBuzz', '\")', '\\n   ', ' elif', ' i', ' %', ' 3', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Fizz', '\")', '\\n   ', ' elif', ' i', ' %', ' 5', ' ==', ' 0', ':', '\\n       ', ' print', '(\"', 'Buzz', '\")', '\\n   ', ' else', ':', '\\n       ', ' print', '(', 'i', ')', '\\n       ', ' \"', '\\n', 'print', '(', 're', '.', 'findall', '(', 'gpt', '2', 'pat', ',', ' example', '))', '\\n']\n"
     ]
    }
   ],
   "source": [
    "example = \"\"\"\n",
    "example = \"\n",
    "for i in range(1, 101):\n",
    "    if i % 3 == 0 and i % 5 == 0:\n",
    "        print(\"FizzBuzz\")\n",
    "    elif i % 3 == 0:\n",
    "        print(\"Fizz\")\n",
    "    elif i % 5 == 0:\n",
    "        print(\"Buzz\")\n",
    "    else:\n",
    "        print(i)\n",
    "        \"\n",
    "print(re.findall(gpt2pat, example))\n",
    "\"\"\"\n",
    "print(re.findall(gpt2pat, example))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[220, 220, 220, 23748, 995, 10185]\n",
      "[262, 24748, 1917, 12340]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# GPT-2 (does not merge spaces)\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "print(enc.encode(\"    hello world!!!\"))\n",
    "\n",
    "# GPT-4 (merges spaces)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "print(enc.encode(\"    hello world!!!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download vocab.bpe and encoder.json from the gpt2 repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-11 11:29:45--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 456318 (446K) [application/octet-stream]\n",
      "Saving to: â€˜vocab.bpeâ€™\n",
      "\n",
      "vocab.bpe           100%[===================>] 445,62K   550KB/s    in 0,8s    \n",
      "\n",
      "2025-03-11 11:29:46 (550 KB/s) - â€˜vocab.bpeâ€™ saved [456318/456318]\n",
      "\n",
      "--2025-03-11 11:29:46--  https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json\n",
      "Resolving openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)... 57.150.97.129\n",
      "Connecting to openaipublic.blob.core.windows.net (openaipublic.blob.core.windows.net)|57.150.97.129|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1042301 (1018K) [application/json]\n",
      "Saving to: â€˜encoder.jsonâ€™\n",
      "\n",
      "encoder.json        100%[===================>]   1018K   802KB/s    in 1,3s    \n",
      "\n",
      "2025-03-11 11:29:48 (802 KB/s) - â€˜encoder.jsonâ€™ saved [1042301/1042301]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/vocab.bpe\n",
    "!wget https://openaipublic.blob.core.windows.net/gpt-2/models/1558M/encoder.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "with open('encoder.json', 'r') as f:\n",
    "    encoder = json.load(f) # <--- ~equivalent to our \"vocab\"\n",
    "\n",
    "with open('vocab.bpe', 'r', encoding=\"utf-8\") as f:\n",
    "    bpe_data = f.read()\n",
    "bpe_merges = [tuple(merge_str.split()) for merge_str in bpe_data.split('\\n')[1:-1]]\n",
    "# ^---- ~equivalent to our \"merges\"\n",
    "\n",
    "\n",
    "#with enconder and merges we can represent a tokenizer, encode and decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '\"': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '<': 27,\n",
       " '=': 28,\n",
       " '>': 29,\n",
       " '?': 30,\n",
       " '@': 31,\n",
       " 'A': 32,\n",
       " 'B': 33,\n",
       " 'C': 34,\n",
       " 'D': 35,\n",
       " 'E': 36,\n",
       " 'F': 37,\n",
       " 'G': 38,\n",
       " 'H': 39,\n",
       " 'I': 40,\n",
       " 'J': 41,\n",
       " 'K': 42,\n",
       " 'L': 43,\n",
       " 'M': 44,\n",
       " 'N': 45,\n",
       " 'O': 46,\n",
       " 'P': 47,\n",
       " 'Q': 48,\n",
       " 'R': 49,\n",
       " 'S': 50,\n",
       " 'T': 51,\n",
       " 'U': 52,\n",
       " 'V': 53,\n",
       " 'W': 54,\n",
       " 'X': 55,\n",
       " 'Y': 56,\n",
       " 'Z': 57,\n",
       " '[': 58,\n",
       " '\\\\': 59,\n",
       " ']': 60,\n",
       " '^': 61,\n",
       " '_': 62,\n",
       " '`': 63,\n",
       " 'a': 64,\n",
       " 'b': 65,\n",
       " 'c': 66,\n",
       " 'd': 67,\n",
       " 'e': 68,\n",
       " 'f': 69,\n",
       " 'g': 70,\n",
       " 'h': 71,\n",
       " 'i': 72,\n",
       " 'j': 73,\n",
       " 'k': 74,\n",
       " 'l': 75,\n",
       " 'm': 76,\n",
       " 'n': 77,\n",
       " 'o': 78,\n",
       " 'p': 79,\n",
       " 'q': 80,\n",
       " 'r': 81,\n",
       " 's': 82,\n",
       " 't': 83,\n",
       " 'u': 84,\n",
       " 'v': 85,\n",
       " 'w': 86,\n",
       " 'x': 87,\n",
       " 'y': 88,\n",
       " 'z': 89,\n",
       " '{': 90,\n",
       " '|': 91,\n",
       " '}': 92,\n",
       " '~': 93,\n",
       " 'Â¡': 94,\n",
       " 'Â¢': 95,\n",
       " 'Â£': 96,\n",
       " 'Â¤': 97,\n",
       " 'Â¥': 98,\n",
       " 'Â¦': 99,\n",
       " 'Â§': 100,\n",
       " 'Â¨': 101,\n",
       " 'Â©': 102,\n",
       " 'Âª': 103,\n",
       " 'Â«': 104,\n",
       " 'Â¬': 105,\n",
       " 'Â®': 106,\n",
       " 'Â¯': 107,\n",
       " 'Â°': 108,\n",
       " 'Â±': 109,\n",
       " 'Â²': 110,\n",
       " 'Â³': 111,\n",
       " 'Â´': 112,\n",
       " 'Âµ': 113,\n",
       " 'Â¶': 114,\n",
       " 'Â·': 115,\n",
       " 'Â¸': 116,\n",
       " 'Â¹': 117,\n",
       " 'Âº': 118,\n",
       " 'Â»': 119,\n",
       " 'Â¼': 120,\n",
       " 'Â½': 121,\n",
       " 'Â¾': 122,\n",
       " 'Â¿': 123,\n",
       " 'Ã€': 124,\n",
       " 'Ã': 125,\n",
       " 'Ã‚': 126,\n",
       " 'Ãƒ': 127,\n",
       " 'Ã„': 128,\n",
       " 'Ã…': 129,\n",
       " 'Ã†': 130,\n",
       " 'Ã‡': 131,\n",
       " 'Ãˆ': 132,\n",
       " 'Ã‰': 133,\n",
       " 'ÃŠ': 134,\n",
       " 'Ã‹': 135,\n",
       " 'ÃŒ': 136,\n",
       " 'Ã': 137,\n",
       " 'ÃŽ': 138,\n",
       " 'Ã': 139,\n",
       " 'Ã': 140,\n",
       " 'Ã‘': 141,\n",
       " 'Ã’': 142,\n",
       " 'Ã“': 143,\n",
       " 'Ã”': 144,\n",
       " 'Ã•': 145,\n",
       " 'Ã–': 146,\n",
       " 'Ã—': 147,\n",
       " 'Ã˜': 148,\n",
       " 'Ã™': 149,\n",
       " 'Ãš': 150,\n",
       " 'Ã›': 151,\n",
       " 'Ãœ': 152,\n",
       " 'Ã': 153,\n",
       " 'Ãž': 154,\n",
       " 'ÃŸ': 155,\n",
       " 'Ã ': 156,\n",
       " 'Ã¡': 157,\n",
       " 'Ã¢': 158,\n",
       " 'Ã£': 159,\n",
       " 'Ã¤': 160,\n",
       " 'Ã¥': 161,\n",
       " 'Ã¦': 162,\n",
       " 'Ã§': 163,\n",
       " 'Ã¨': 164,\n",
       " 'Ã©': 165,\n",
       " 'Ãª': 166,\n",
       " 'Ã«': 167,\n",
       " 'Ã¬': 168,\n",
       " 'Ã­': 169,\n",
       " 'Ã®': 170,\n",
       " 'Ã¯': 171,\n",
       " 'Ã°': 172,\n",
       " 'Ã±': 173,\n",
       " 'Ã²': 174,\n",
       " 'Ã³': 175,\n",
       " 'Ã´': 176,\n",
       " 'Ãµ': 177,\n",
       " 'Ã¶': 178,\n",
       " 'Ã·': 179,\n",
       " 'Ã¸': 180,\n",
       " 'Ã¹': 181,\n",
       " 'Ãº': 182,\n",
       " 'Ã»': 183,\n",
       " 'Ã¼': 184,\n",
       " 'Ã½': 185,\n",
       " 'Ã¾': 186,\n",
       " 'Ã¿': 187,\n",
       " 'Ä€': 188,\n",
       " 'Ä': 189,\n",
       " 'Ä‚': 190,\n",
       " 'Äƒ': 191,\n",
       " 'Ä„': 192,\n",
       " 'Ä…': 193,\n",
       " 'Ä†': 194,\n",
       " 'Ä‡': 195,\n",
       " 'Äˆ': 196,\n",
       " 'Ä‰': 197,\n",
       " 'ÄŠ': 198,\n",
       " 'Ä‹': 199,\n",
       " 'ÄŒ': 200,\n",
       " 'Ä': 201,\n",
       " 'ÄŽ': 202,\n",
       " 'Ä': 203,\n",
       " 'Ä': 204,\n",
       " 'Ä‘': 205,\n",
       " 'Ä’': 206,\n",
       " 'Ä“': 207,\n",
       " 'Ä”': 208,\n",
       " 'Ä•': 209,\n",
       " 'Ä–': 210,\n",
       " 'Ä—': 211,\n",
       " 'Ä˜': 212,\n",
       " 'Ä™': 213,\n",
       " 'Äš': 214,\n",
       " 'Ä›': 215,\n",
       " 'Äœ': 216,\n",
       " 'Ä': 217,\n",
       " 'Äž': 218,\n",
       " 'ÄŸ': 219,\n",
       " 'Ä ': 220,\n",
       " 'Ä¡': 221,\n",
       " 'Ä¢': 222,\n",
       " 'Ä£': 223,\n",
       " 'Ä¤': 224,\n",
       " 'Ä¥': 225,\n",
       " 'Ä¦': 226,\n",
       " 'Ä§': 227,\n",
       " 'Ä¨': 228,\n",
       " 'Ä©': 229,\n",
       " 'Äª': 230,\n",
       " 'Ä«': 231,\n",
       " 'Ä¬': 232,\n",
       " 'Ä­': 233,\n",
       " 'Ä®': 234,\n",
       " 'Ä¯': 235,\n",
       " 'Ä°': 236,\n",
       " 'Ä±': 237,\n",
       " 'Ä²': 238,\n",
       " 'Ä³': 239,\n",
       " 'Ä´': 240,\n",
       " 'Äµ': 241,\n",
       " 'Ä¶': 242,\n",
       " 'Ä·': 243,\n",
       " 'Ä¸': 244,\n",
       " 'Ä¹': 245,\n",
       " 'Äº': 246,\n",
       " 'Ä»': 247,\n",
       " 'Ä¼': 248,\n",
       " 'Ä½': 249,\n",
       " 'Ä¾': 250,\n",
       " 'Ä¿': 251,\n",
       " 'Å€': 252,\n",
       " 'Å': 253,\n",
       " 'Å‚': 254,\n",
       " 'Åƒ': 255,\n",
       " 'Ä t': 256,\n",
       " 'Ä a': 257,\n",
       " 'he': 258,\n",
       " 'in': 259,\n",
       " 're': 260,\n",
       " 'on': 261,\n",
       " 'Ä the': 262,\n",
       " 'er': 263,\n",
       " 'Ä s': 264,\n",
       " 'at': 265,\n",
       " 'Ä w': 266,\n",
       " 'Ä o': 267,\n",
       " 'en': 268,\n",
       " 'Ä c': 269,\n",
       " 'it': 270,\n",
       " 'is': 271,\n",
       " 'an': 272,\n",
       " 'or': 273,\n",
       " 'es': 274,\n",
       " 'Ä b': 275,\n",
       " 'ed': 276,\n",
       " 'Ä f': 277,\n",
       " 'ing': 278,\n",
       " 'Ä p': 279,\n",
       " 'ou': 280,\n",
       " 'Ä an': 281,\n",
       " 'al': 282,\n",
       " 'ar': 283,\n",
       " 'Ä to': 284,\n",
       " 'Ä m': 285,\n",
       " 'Ä of': 286,\n",
       " 'Ä in': 287,\n",
       " 'Ä d': 288,\n",
       " 'Ä h': 289,\n",
       " 'Ä and': 290,\n",
       " 'ic': 291,\n",
       " 'as': 292,\n",
       " 'le': 293,\n",
       " 'Ä th': 294,\n",
       " 'ion': 295,\n",
       " 'om': 296,\n",
       " 'll': 297,\n",
       " 'ent': 298,\n",
       " 'Ä n': 299,\n",
       " 'Ä l': 300,\n",
       " 'st': 301,\n",
       " 'Ä re': 302,\n",
       " 've': 303,\n",
       " 'Ä e': 304,\n",
       " 'ro': 305,\n",
       " 'ly': 306,\n",
       " 'Ä be': 307,\n",
       " 'Ä g': 308,\n",
       " 'Ä T': 309,\n",
       " 'ct': 310,\n",
       " 'Ä S': 311,\n",
       " 'id': 312,\n",
       " 'ot': 313,\n",
       " 'Ä I': 314,\n",
       " 'ut': 315,\n",
       " 'et': 316,\n",
       " 'Ä A': 317,\n",
       " 'Ä is': 318,\n",
       " 'Ä on': 319,\n",
       " 'im': 320,\n",
       " 'am': 321,\n",
       " 'ow': 322,\n",
       " 'ay': 323,\n",
       " 'ad': 324,\n",
       " 'se': 325,\n",
       " 'Ä that': 326,\n",
       " 'Ä C': 327,\n",
       " 'ig': 328,\n",
       " 'Ä for': 329,\n",
       " 'ac': 330,\n",
       " 'Ä y': 331,\n",
       " 'ver': 332,\n",
       " 'ur': 333,\n",
       " 'Ä u': 334,\n",
       " 'ld': 335,\n",
       " 'Ä st': 336,\n",
       " 'Ä M': 337,\n",
       " \"'s\": 338,\n",
       " 'Ä he': 339,\n",
       " 'Ä it': 340,\n",
       " 'ation': 341,\n",
       " 'ith': 342,\n",
       " 'ir': 343,\n",
       " 'ce': 344,\n",
       " 'Ä you': 345,\n",
       " 'il': 346,\n",
       " 'Ä B': 347,\n",
       " 'Ä wh': 348,\n",
       " 'ol': 349,\n",
       " 'Ä P': 350,\n",
       " 'Ä with': 351,\n",
       " 'Ä 1': 352,\n",
       " 'ter': 353,\n",
       " 'ch': 354,\n",
       " 'Ä as': 355,\n",
       " 'Ä we': 356,\n",
       " 'Ä (': 357,\n",
       " 'nd': 358,\n",
       " 'ill': 359,\n",
       " 'Ä D': 360,\n",
       " 'if': 361,\n",
       " 'Ä 2': 362,\n",
       " 'ag': 363,\n",
       " 'ers': 364,\n",
       " 'ke': 365,\n",
       " 'Ä \"': 366,\n",
       " 'Ä H': 367,\n",
       " 'em': 368,\n",
       " 'Ä con': 369,\n",
       " 'Ä W': 370,\n",
       " 'Ä R': 371,\n",
       " 'her': 372,\n",
       " 'Ä was': 373,\n",
       " 'Ä r': 374,\n",
       " 'od': 375,\n",
       " 'Ä F': 376,\n",
       " 'ul': 377,\n",
       " 'ate': 378,\n",
       " 'Ä at': 379,\n",
       " 'ri': 380,\n",
       " 'pp': 381,\n",
       " 'ore': 382,\n",
       " 'Ä The': 383,\n",
       " 'Ä se': 384,\n",
       " 'us': 385,\n",
       " 'Ä pro': 386,\n",
       " 'Ä ha': 387,\n",
       " 'um': 388,\n",
       " 'Ä are': 389,\n",
       " 'Ä de': 390,\n",
       " 'ain': 391,\n",
       " 'and': 392,\n",
       " 'Ä or': 393,\n",
       " 'igh': 394,\n",
       " 'est': 395,\n",
       " 'ist': 396,\n",
       " 'ab': 397,\n",
       " 'rom': 398,\n",
       " 'Ä N': 399,\n",
       " 'th': 400,\n",
       " 'Ä com': 401,\n",
       " 'Ä G': 402,\n",
       " 'un': 403,\n",
       " 'op': 404,\n",
       " '00': 405,\n",
       " 'Ä L': 406,\n",
       " 'Ä not': 407,\n",
       " 'ess': 408,\n",
       " 'Ä ex': 409,\n",
       " 'Ä v': 410,\n",
       " 'res': 411,\n",
       " 'Ä E': 412,\n",
       " 'ew': 413,\n",
       " 'ity': 414,\n",
       " 'ant': 415,\n",
       " 'Ä by': 416,\n",
       " 'el': 417,\n",
       " 'os': 418,\n",
       " 'ort': 419,\n",
       " 'oc': 420,\n",
       " 'qu': 421,\n",
       " 'Ä from': 422,\n",
       " 'Ä have': 423,\n",
       " 'Ä su': 424,\n",
       " 'ive': 425,\n",
       " 'ould': 426,\n",
       " 'Ä sh': 427,\n",
       " 'Ä this': 428,\n",
       " 'nt': 429,\n",
       " 'ra': 430,\n",
       " 'pe': 431,\n",
       " 'ight': 432,\n",
       " 'art': 433,\n",
       " 'ment': 434,\n",
       " 'Ä al': 435,\n",
       " 'ust': 436,\n",
       " 'end': 437,\n",
       " '--': 438,\n",
       " 'all': 439,\n",
       " 'Ä O': 440,\n",
       " 'ack': 441,\n",
       " 'Ä ch': 442,\n",
       " 'Ä le': 443,\n",
       " 'ies': 444,\n",
       " 'red': 445,\n",
       " 'ard': 446,\n",
       " 'Ã¢Ä¢': 447,\n",
       " 'out': 448,\n",
       " 'Ä J': 449,\n",
       " 'Ä ab': 450,\n",
       " 'ear': 451,\n",
       " 'iv': 452,\n",
       " 'ally': 453,\n",
       " 'our': 454,\n",
       " 'ost': 455,\n",
       " 'gh': 456,\n",
       " 'pt': 457,\n",
       " 'Ä pl': 458,\n",
       " 'ast': 459,\n",
       " 'Ä can': 460,\n",
       " 'ak': 461,\n",
       " 'ome': 462,\n",
       " 'ud': 463,\n",
       " 'The': 464,\n",
       " 'Ä his': 465,\n",
       " 'Ä do': 466,\n",
       " 'Ä go': 467,\n",
       " 'Ä has': 468,\n",
       " 'ge': 469,\n",
       " \"'t\": 470,\n",
       " 'Ä U': 471,\n",
       " 'rou': 472,\n",
       " 'Ä sa': 473,\n",
       " 'Ä j': 474,\n",
       " 'Ä but': 475,\n",
       " 'Ä wor': 476,\n",
       " 'Ä all': 477,\n",
       " 'ect': 478,\n",
       " 'Ä k': 479,\n",
       " 'ame': 480,\n",
       " 'Ä will': 481,\n",
       " 'ok': 482,\n",
       " 'Ä whe': 483,\n",
       " 'Ä they': 484,\n",
       " 'ide': 485,\n",
       " '01': 486,\n",
       " 'ff': 487,\n",
       " 'ich': 488,\n",
       " 'pl': 489,\n",
       " 'ther': 490,\n",
       " 'Ä tr': 491,\n",
       " '..': 492,\n",
       " 'Ä int': 493,\n",
       " 'ie': 494,\n",
       " 'ure': 495,\n",
       " 'age': 496,\n",
       " 'Ä ne': 497,\n",
       " 'ial': 498,\n",
       " 'ap': 499,\n",
       " 'ine': 500,\n",
       " 'ice': 501,\n",
       " 'Ä me': 502,\n",
       " 'Ä out': 503,\n",
       " 'ans': 504,\n",
       " 'one': 505,\n",
       " 'ong': 506,\n",
       " 'ions': 507,\n",
       " 'Ä who': 508,\n",
       " 'Ä K': 509,\n",
       " 'Ä up': 510,\n",
       " 'Ä their': 511,\n",
       " 'Ä ad': 512,\n",
       " 'Ä 3': 513,\n",
       " 'Ä us': 514,\n",
       " 'ated': 515,\n",
       " 'ous': 516,\n",
       " 'Ä more': 517,\n",
       " 'ue': 518,\n",
       " 'og': 519,\n",
       " 'Ä St': 520,\n",
       " 'ind': 521,\n",
       " 'ike': 522,\n",
       " 'Ä so': 523,\n",
       " 'ime': 524,\n",
       " 'per': 525,\n",
       " '.\"': 526,\n",
       " 'ber': 527,\n",
       " 'iz': 528,\n",
       " 'act': 529,\n",
       " 'Ä one': 530,\n",
       " 'Ä said': 531,\n",
       " 'Ä -': 532,\n",
       " 'are': 533,\n",
       " 'Ä your': 534,\n",
       " 'cc': 535,\n",
       " 'Ä Th': 536,\n",
       " 'Ä cl': 537,\n",
       " 'ep': 538,\n",
       " 'ake': 539,\n",
       " 'able': 540,\n",
       " 'ip': 541,\n",
       " 'Ä cont': 542,\n",
       " 'Ä which': 543,\n",
       " 'ia': 544,\n",
       " 'Ä im': 545,\n",
       " 'Ä about': 546,\n",
       " 'Ä were': 547,\n",
       " 'very': 548,\n",
       " 'ub': 549,\n",
       " 'Ä had': 550,\n",
       " 'Ä en': 551,\n",
       " 'Ä comp': 552,\n",
       " ',\"': 553,\n",
       " 'Ä In': 554,\n",
       " 'Ä un': 555,\n",
       " 'Ä ag': 556,\n",
       " 'ire': 557,\n",
       " 'ace': 558,\n",
       " 'au': 559,\n",
       " 'ary': 560,\n",
       " 'Ä would': 561,\n",
       " 'ass': 562,\n",
       " 'ry': 563,\n",
       " 'Ä Ã¢Ä¢': 564,\n",
       " 'cl': 565,\n",
       " 'ook': 566,\n",
       " 'ere': 567,\n",
       " 'so': 568,\n",
       " 'Ä V': 569,\n",
       " 'ign': 570,\n",
       " 'ib': 571,\n",
       " 'Ä off': 572,\n",
       " 'Ä te': 573,\n",
       " 'ven': 574,\n",
       " 'Ä Y': 575,\n",
       " 'ile': 576,\n",
       " 'ose': 577,\n",
       " 'ite': 578,\n",
       " 'orm': 579,\n",
       " 'Ä 201': 580,\n",
       " 'Ä res': 581,\n",
       " 'Ä man': 582,\n",
       " 'Ä per': 583,\n",
       " 'Ä other': 584,\n",
       " 'ord': 585,\n",
       " 'ult': 586,\n",
       " 'Ä been': 587,\n",
       " 'Ä like': 588,\n",
       " 'ase': 589,\n",
       " 'ance': 590,\n",
       " 'ks': 591,\n",
       " 'ays': 592,\n",
       " 'own': 593,\n",
       " 'ence': 594,\n",
       " 'Ä dis': 595,\n",
       " 'ction': 596,\n",
       " 'Ä any': 597,\n",
       " 'Ä app': 598,\n",
       " 'Ä sp': 599,\n",
       " 'int': 600,\n",
       " 'ress': 601,\n",
       " 'ations': 602,\n",
       " 'ail': 603,\n",
       " 'Ä 4': 604,\n",
       " 'ical': 605,\n",
       " 'Ä them': 606,\n",
       " 'Ä her': 607,\n",
       " 'ount': 608,\n",
       " 'Ä Ch': 609,\n",
       " 'Ä ar': 610,\n",
       " 'Ä if': 611,\n",
       " 'Ä there': 612,\n",
       " 'Ä pe': 613,\n",
       " 'Ä year': 614,\n",
       " 'av': 615,\n",
       " 'Ä my': 616,\n",
       " 'Ä some': 617,\n",
       " 'Ä when': 618,\n",
       " 'ough': 619,\n",
       " 'ach': 620,\n",
       " 'Ä than': 621,\n",
       " 'ru': 622,\n",
       " 'ond': 623,\n",
       " 'ick': 624,\n",
       " 'Ä over': 625,\n",
       " 'vel': 626,\n",
       " 'Ä qu': 627,\n",
       " 'ÄŠÄŠ': 628,\n",
       " 'Ä sc': 629,\n",
       " 'reat': 630,\n",
       " 'ree': 631,\n",
       " 'Ä It': 632,\n",
       " 'ound': 633,\n",
       " 'port': 634,\n",
       " 'Ä also': 635,\n",
       " 'Ä part': 636,\n",
       " 'fter': 637,\n",
       " 'Ä kn': 638,\n",
       " 'Ä bec': 639,\n",
       " 'Ä time': 640,\n",
       " 'ens': 641,\n",
       " 'Ä 5': 642,\n",
       " 'ople': 643,\n",
       " 'Ä what': 644,\n",
       " 'Ä no': 645,\n",
       " 'du': 646,\n",
       " 'mer': 647,\n",
       " 'ang': 648,\n",
       " 'Ä new': 649,\n",
       " '----': 650,\n",
       " 'Ä get': 651,\n",
       " 'ory': 652,\n",
       " 'ition': 653,\n",
       " 'ings': 654,\n",
       " 'Ä just': 655,\n",
       " 'Ä into': 656,\n",
       " 'Ä 0': 657,\n",
       " 'ents': 658,\n",
       " 'ove': 659,\n",
       " 'te': 660,\n",
       " 'Ä people': 661,\n",
       " 'Ä pre': 662,\n",
       " 'Ä its': 663,\n",
       " 'Ä rec': 664,\n",
       " 'Ä tw': 665,\n",
       " 'ian': 666,\n",
       " 'irst': 667,\n",
       " 'ark': 668,\n",
       " 'ors': 669,\n",
       " 'Ä work': 670,\n",
       " 'ade': 671,\n",
       " 'ob': 672,\n",
       " 'Ä she': 673,\n",
       " 'Ä our': 674,\n",
       " 'wn': 675,\n",
       " 'ink': 676,\n",
       " 'lic': 677,\n",
       " 'Ä 19': 678,\n",
       " 'Ä He': 679,\n",
       " 'ish': 680,\n",
       " 'nder': 681,\n",
       " 'ause': 682,\n",
       " 'Ä him': 683,\n",
       " 'ons': 684,\n",
       " 'Ä [': 685,\n",
       " 'Ä ro': 686,\n",
       " 'form': 687,\n",
       " 'ild': 688,\n",
       " 'ates': 689,\n",
       " 'vers': 690,\n",
       " 'Ä only': 691,\n",
       " 'oll': 692,\n",
       " 'Ä spe': 693,\n",
       " 'ck': 694,\n",
       " 'ell': 695,\n",
       " 'amp': 696,\n",
       " 'Ä acc': 697,\n",
       " 'Ä bl': 698,\n",
       " 'ious': 699,\n",
       " 'urn': 700,\n",
       " 'ft': 701,\n",
       " 'ood': 702,\n",
       " 'Ä how': 703,\n",
       " 'hed': 704,\n",
       " \"Ä '\": 705,\n",
       " 'Ä after': 706,\n",
       " 'aw': 707,\n",
       " 'Ä att': 708,\n",
       " 'ov': 709,\n",
       " 'ne': 710,\n",
       " 'Ä play': 711,\n",
       " 'erv': 712,\n",
       " 'ict': 713,\n",
       " 'Ä could': 714,\n",
       " 'itt': 715,\n",
       " 'Ä am': 716,\n",
       " 'Ä first': 717,\n",
       " 'Ä 6': 718,\n",
       " 'Ä act': 719,\n",
       " 'Ä $': 720,\n",
       " 'ec': 721,\n",
       " 'hing': 722,\n",
       " 'ual': 723,\n",
       " 'ull': 724,\n",
       " 'Ä comm': 725,\n",
       " 'oy': 726,\n",
       " 'old': 727,\n",
       " 'ces': 728,\n",
       " 'ater': 729,\n",
       " 'Ä fe': 730,\n",
       " 'Ä bet': 731,\n",
       " 'we': 732,\n",
       " 'iff': 733,\n",
       " 'Ä two': 734,\n",
       " 'ock': 735,\n",
       " 'Ä back': 736,\n",
       " ').': 737,\n",
       " 'ident': 738,\n",
       " 'Ä under': 739,\n",
       " 'rough': 740,\n",
       " 'sel': 741,\n",
       " 'xt': 742,\n",
       " 'Ä may': 743,\n",
       " 'round': 744,\n",
       " 'Ä po': 745,\n",
       " 'ph': 746,\n",
       " 'iss': 747,\n",
       " 'Ä des': 748,\n",
       " 'Ä most': 749,\n",
       " 'Ä did': 750,\n",
       " 'Ä add': 751,\n",
       " 'ject': 752,\n",
       " 'Ä inc': 753,\n",
       " 'fore': 754,\n",
       " 'Ä pol': 755,\n",
       " 'ont': 756,\n",
       " 'Ä again': 757,\n",
       " 'clud': 758,\n",
       " 'tern': 759,\n",
       " 'Ä know': 760,\n",
       " 'Ä need': 761,\n",
       " 'Ä cons': 762,\n",
       " 'Ä co': 763,\n",
       " 'Ä .': 764,\n",
       " 'Ä want': 765,\n",
       " 'Ä see': 766,\n",
       " 'Ä 7': 767,\n",
       " 'ning': 768,\n",
       " 'iew': 769,\n",
       " 'Ä This': 770,\n",
       " 'ced': 771,\n",
       " 'Ä even': 772,\n",
       " 'Ä ind': 773,\n",
       " 'ty': 774,\n",
       " 'Ä We': 775,\n",
       " 'ath': 776,\n",
       " 'Ä these': 777,\n",
       " 'Ä pr': 778,\n",
       " 'Ä use': 779,\n",
       " 'Ä because': 780,\n",
       " 'Ä fl': 781,\n",
       " 'ng': 782,\n",
       " 'Ä now': 783,\n",
       " 'Ä Ã¢Ä¢Äµ': 784,\n",
       " 'com': 785,\n",
       " 'ise': 786,\n",
       " 'Ä make': 787,\n",
       " 'Ä then': 788,\n",
       " 'ower': 789,\n",
       " 'Ä every': 790,\n",
       " 'Ä Un': 791,\n",
       " 'Ä sec': 792,\n",
       " 'oss': 793,\n",
       " 'uch': 794,\n",
       " 'Ä em': 795,\n",
       " 'Ä =': 796,\n",
       " 'Ä Re': 797,\n",
       " 'ied': 798,\n",
       " 'rit': 799,\n",
       " 'Ä inv': 800,\n",
       " 'lect': 801,\n",
       " 'Ä supp': 802,\n",
       " 'ating': 803,\n",
       " 'Ä look': 804,\n",
       " 'man': 805,\n",
       " 'pect': 806,\n",
       " 'Ä 8': 807,\n",
       " 'row': 808,\n",
       " 'Ä bu': 809,\n",
       " 'Ä where': 810,\n",
       " 'ific': 811,\n",
       " 'Ä years': 812,\n",
       " 'ily': 813,\n",
       " 'Ä diff': 814,\n",
       " 'Ä should': 815,\n",
       " 'Ä rem': 816,\n",
       " 'Th': 817,\n",
       " 'In': 818,\n",
       " 'Ä ev': 819,\n",
       " 'day': 820,\n",
       " \"'re\": 821,\n",
       " 'rib': 822,\n",
       " 'Ä rel': 823,\n",
       " 'ss': 824,\n",
       " 'Ä def': 825,\n",
       " 'Ä right': 826,\n",
       " 'Ä sy': 827,\n",
       " '),': 828,\n",
       " 'les': 829,\n",
       " '000': 830,\n",
       " 'hen': 831,\n",
       " 'Ä through': 832,\n",
       " 'Ä Tr': 833,\n",
       " '__': 834,\n",
       " 'Ä way': 835,\n",
       " 'Ä don': 836,\n",
       " 'Ä ,': 837,\n",
       " 'Ä 10': 838,\n",
       " 'ased': 839,\n",
       " 'Ä ass': 840,\n",
       " 'ublic': 841,\n",
       " 'Ä reg': 842,\n",
       " 'Ä And': 843,\n",
       " 'ix': 844,\n",
       " 'Ä very': 845,\n",
       " 'Ä includ': 846,\n",
       " 'other': 847,\n",
       " 'Ä imp': 848,\n",
       " 'oth': 849,\n",
       " 'Ä sub': 850,\n",
       " 'Ä Ã¢Ä¢Ä¶': 851,\n",
       " 'Ä being': 852,\n",
       " 'arg': 853,\n",
       " 'Ä Wh': 854,\n",
       " '==': 855,\n",
       " 'ible': 856,\n",
       " 'Ä does': 857,\n",
       " 'ange': 858,\n",
       " 'ram': 859,\n",
       " 'Ä 9': 860,\n",
       " 'ert': 861,\n",
       " 'ps': 862,\n",
       " 'ited': 863,\n",
       " 'ational': 864,\n",
       " 'Ä br': 865,\n",
       " 'Ä down': 866,\n",
       " 'Ä many': 867,\n",
       " 'aking': 868,\n",
       " 'Ä call': 869,\n",
       " 'uring': 870,\n",
       " 'ities': 871,\n",
       " 'Ä ph': 872,\n",
       " 'ics': 873,\n",
       " 'als': 874,\n",
       " 'Ä dec': 875,\n",
       " 'ative': 876,\n",
       " 'ener': 877,\n",
       " 'Ä before': 878,\n",
       " 'ility': 879,\n",
       " 'Ä well': 880,\n",
       " 'Ä much': 881,\n",
       " 'erson': 882,\n",
       " 'Ä those': 883,\n",
       " 'Ä such': 884,\n",
       " 'Ä ke': 885,\n",
       " 'Ä end': 886,\n",
       " 'Ä But': 887,\n",
       " 'ason': 888,\n",
       " 'ting': 889,\n",
       " 'Ä long': 890,\n",
       " 'ef': 891,\n",
       " 'Ä think': 892,\n",
       " 'ys': 893,\n",
       " 'Ä bel': 894,\n",
       " 'Ä sm': 895,\n",
       " 'its': 896,\n",
       " 'ax': 897,\n",
       " 'Ä own': 898,\n",
       " 'Ä prov': 899,\n",
       " 'Ä set': 900,\n",
       " 'ife': 901,\n",
       " 'ments': 902,\n",
       " 'ble': 903,\n",
       " 'ward': 904,\n",
       " 'Ä show': 905,\n",
       " 'Ä pres': 906,\n",
       " 'ms': 907,\n",
       " 'omet': 908,\n",
       " 'Ä ob': 909,\n",
       " 'Ä say': 910,\n",
       " 'Ä Sh': 911,\n",
       " 'ts': 912,\n",
       " 'ful': 913,\n",
       " 'Ä eff': 914,\n",
       " 'Ä gu': 915,\n",
       " 'Ä inst': 916,\n",
       " 'und': 917,\n",
       " 'ren': 918,\n",
       " 'cess': 919,\n",
       " 'Ä ent': 920,\n",
       " 'Ä You': 921,\n",
       " 'Ä good': 922,\n",
       " 'Ä start': 923,\n",
       " 'ince': 924,\n",
       " 'Ä made': 925,\n",
       " 'tt': 926,\n",
       " 'stem': 927,\n",
       " 'olog': 928,\n",
       " 'up': 929,\n",
       " 'Ä |': 930,\n",
       " 'ump': 931,\n",
       " 'Ä hel': 932,\n",
       " 'vern': 933,\n",
       " 'ular': 934,\n",
       " 'ually': 935,\n",
       " 'Ä ac': 936,\n",
       " 'Ä mon': 937,\n",
       " 'Ä last': 938,\n",
       " 'Ä 200': 939,\n",
       " '10': 940,\n",
       " 'Ä stud': 941,\n",
       " 'ures': 942,\n",
       " 'Ä Ar': 943,\n",
       " 'self': 944,\n",
       " 'ars': 945,\n",
       " 'meric': 946,\n",
       " 'ues': 947,\n",
       " 'cy': 948,\n",
       " 'Ä min': 949,\n",
       " 'ollow': 950,\n",
       " 'Ä col': 951,\n",
       " 'io': 952,\n",
       " 'Ä mod': 953,\n",
       " 'Ä count': 954,\n",
       " 'Ä Com': 955,\n",
       " 'hes': 956,\n",
       " 'Ä fin': 957,\n",
       " 'air': 958,\n",
       " 'ier': 959,\n",
       " 'Ã¢Ä¢Ä¶': 960,\n",
       " 'read': 961,\n",
       " 'ank': 962,\n",
       " 'atch': 963,\n",
       " 'ever': 964,\n",
       " 'Ä str': 965,\n",
       " 'Ä point': 966,\n",
       " 'ork': 967,\n",
       " 'Ä New': 968,\n",
       " 'Ä sur': 969,\n",
       " 'ool': 970,\n",
       " 'alk': 971,\n",
       " 'ement': 972,\n",
       " 'Ä used': 973,\n",
       " 'ract': 974,\n",
       " 'ween': 975,\n",
       " 'Ä same': 976,\n",
       " 'oun': 977,\n",
       " 'Ä Al': 978,\n",
       " 'ci': 979,\n",
       " 'Ä differe': 980,\n",
       " 'Ä while': 981,\n",
       " '--------': 982,\n",
       " 'Ä game': 983,\n",
       " 'cept': 984,\n",
       " 'Ä sim': 985,\n",
       " '...': 986,\n",
       " 'Ä inter': 987,\n",
       " 'ek': 988,\n",
       " 'Ä report': 989,\n",
       " 'Ä produ': 990,\n",
       " 'Ä still': 991,\n",
       " 'led': 992,\n",
       " 'ah': 993,\n",
       " 'Ä here': 994,\n",
       " 'Ä world': 995,\n",
       " 'Ä though': 996,\n",
       " 'Ä num': 997,\n",
       " 'arch': 998,\n",
       " 'imes': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤\n",
      "Ä =================================================================\n",
      "Ä ----------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤\n",
      "................................................................\n",
      "================================================================\n",
      "________________________________________________________________\n",
      "--------------------------------------------------------\n",
      "Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶\n",
      "------------------------------------------------\n",
      "Ä =================================\n",
      "Ä --------------------------------\n",
      "Ä ********************************\n",
      "--------------------------------\n",
      "................................\n",
      "================================\n",
      "________________________________\n",
      "ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤ÃƒÄ¥ÃƒÄ¤\n",
      "********************************\n"
     ]
    }
   ],
   "source": [
    "# Find the 10 longest tokens in the encoder\n",
    "longest_tokens = sorted(encoder.keys(), key=len, reverse=True)[:20]\n",
    "\n",
    "# Print the longest tokens\n",
    "for token in longest_tokens:\n",
    "    print(f\"{token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder['<|endoftext|>'] # special token in use for the GPT-2 base model. used to separate different texts in the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentencepiece\n",
    "\n",
    "Commonly used because (unlike tiktoken) it can efficiently both train and inference BPE tokenizers. It is used in both **Llama** and **Mistral** series.\n",
    "\n",
    "[sentencepiece on Github link](https://github.com/google/sentencepiece).\n",
    "\n",
    "**The big difference**: sentencepiece runs BPE on the Unicode code points directly! It then has an option `character_coverage` for what to do with very very rare codepoints that appear very few times, and it either maps them onto an UNK token, or if `byte_fallback` is turned on, it encodes them with utf-8 and then encodes the raw bytes instead.\n",
    "\n",
    "TLDR:\n",
    "\n",
    "- tiktoken encodes to utf-8 and then BPEs bytes\n",
    "- sentencepiece BPEs the code points and optionally falls back to utf-8 bytes for rare code points (rarity is determined by character_coverage hyperparameter), which then get translated to byte tokens.\n",
    "\n",
    "(Personally I think the tiktoken way is a lot cleaner...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training with the start of Don Quijote\n",
    "with open(\"quixote_sentencepiece.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"En un lugar de la Mancha, de cuyo nombre no quiero acordarme, no ha mucho tiempo que vivÃ­a un hidalgo de los de lanza en astillero, adarga antigua, rocÃ­n flaco y galgo corredor. Una olla de algo mÃ¡s vaca que carnero, salpicÃ³n las mÃ¡s noches, duelos y quebrantos los sÃ¡bados, lantejas los viernes, algÃºn palomino de aÃ±adidura los domingos, consumÃ­an las tres partes de su hacienda. El resto della concluÃ­an sayo de velarte, calzas de velludo para las fiestas, con sus pantuflos de lo mesmo, y los dÃ­as de entresemana se honraba con su vellorÃ­ de lo mÃ¡s fino. TenÃ­a en su casa una ama que pasaba de los cuarenta, y una sobrina que no llegaba a los veinte, y un mozo de campo y plaza, que asÃ­ ensillaba el rocÃ­n como tomaba la podadera. Frisaba la edad de nuestro hidalgo con los cincuenta aÃ±os; era de complexiÃ³n recia, seco de carnes, enjuto de rostro, gran madrugador y amigo de la caza. Quieren decir que tenÃ­a el sobrenombre de Quijada, o Quesada, que en esto hay alguna diferencia en los autores que deste caso escriben; aunque, por conjeturas verosÃ­miles, se deja entender que se llamaba Quejana. Pero esto importa poco a nuestro cuento; basta que en la narraciÃ³n dÃ©l no se salga un punto de la verdad. Es, pues, de saber que este sobredicho hidalgo, los ratos que estaba ocioso, que eran los mÃ¡s del aÃ±o, se daba a leer libros de caballerÃ­as, con tanta aficiÃ³n y gusto, que olvidÃ³ casi de todo punto el ejercicio de la caza, y aun la administraciÃ³n de su hacienda. Y llegÃ³ a tanto su curiosidad y desatino en esto, que vendiÃ³ muchas hanegas de tierra de sembradura para comprar libros de caballerÃ­as en que leer, y asÃ­, llevÃ³ a su casa todos cuantos pudo haber dellos; y de todos, ningunos le parecÃ­an tan bien como los que compuso el famoso Feliciano de Silva, porque la claridad de su prosa y aquellas entricadas razones suyas le parecÃ­an de perlas, y mÃ¡s cuando llegaba a leer aquellos requiebros y cartas de desafÃ­os, donde en muchas partes hallaba escrito: La razÃ³n de la sinrazÃ³n que a mi razÃ³n se hace, de tal manera mi razÃ³n enflaquece, que con razÃ³n me quejo de la vuestra fermosura. Y tambiÃ©n cuando leÃ­a: ...los altos cielos que de vuestra divinidad divinamente con las estrellas os fortifican, y os hacen merecedora del merecimiento que merece la vuestra grandeza. Con estas razones perdÃ­a el pobre caballero el juicio, y desvelÃ¡base por entenderlas y desentraÃ±arles el sentido, que no se lo sacara ni las entendiera el mesmo AristÃ³teles, si resucitara para sÃ³lo ello. No estaba muy bien con las heridas que don BelianÃ­s daba y recebÃ­a, porque se imaginaba que, por grandes maestros que le hubiesen curado, no dejarÃ­a de tener el rostro y todo el cuerpo lleno de cicatrices y seÃ±ales. Pero, con todo, alababa en su autor aquel acabar su libro con la promesa de aquella inacabable aventura, y muchas veces le vino deseo de tomar la pluma y dalle fin al pie de la letra, como allÃ­ se promete; y sin duda alguna lo hiciera, y aun saliera con ello, si otros mayores y continuos pensamientos no se lo estorbaran. Tuvo muchas veces competencia con el cura de su lugar -que era hombre docto, graduado en SigÃ¼enza-, sobre cuÃ¡l habÃ­a sido mejor caballero: PalmerÃ­n de Ingalaterra o AmadÃ­s de Gaula; mas maese NicolÃ¡s, barbero del mesmo pueblo, decÃ­a que ninguno llegaba al Caballero del Febo, y que si alguno se le podÃ­a comparar, era don Galaor, hermano de AmadÃ­s de Gaula, porque tenÃ­a muy acomodada condiciÃ³n para todo; que no era caballero melindroso, ni tan llorÃ³n como su hermano, y que en lo de la valentÃ­a no le iba en zaga. En resoluciÃ³n, Ã©l se enfrascÃ³ tanto en su letura, que se le pasaban las noches leyendo de claro en claro, y los dÃ­as de turbio en turbio; y asÃ­, del poco dormir y del mucho leer, se le secÃ³ el celebro, de manera que vino a perder el juicio. LlenÃ³sele la fantasÃ­a de todo aquello que leÃ­a en los libros, asÃ­ de encantamentos como de pendencias, batallas, desafÃ­os, heridas, requiebros, amores, tormentas y disparates imposibles; y asentÃ³sele de tal modo en la imaginaciÃ³n que era verdad toda aquella mÃ¡quina de aquellas sonadas soÃ±adas invenciones que leÃ­a, que para Ã©l no habÃ­a otra historia mÃ¡s cierta en el mundo. DecÃ­a Ã©l que el Cid Ruy DÃ­az habÃ­a sido muy buen caballero, pero que no tenÃ­a que ver con el Caballero de la Ardiente Espada, que de sÃ³lo un revÃ©s habÃ­a partido por medio dos fieros y descomunales gigantes. Mejor estaba con Bernardo del Carpio, porque en Roncesvalles habÃ­a muerto a RoldÃ¡n el encantado, valiÃ©ndose de la industria de HÃ©rcules, cuando ahogÃ³ a Anteo, el hijo de la Tierra, entre los brazos. DecÃ­a mucho bien del gigante Morgante, porque, con ser de aquella generaciÃ³n gigantea, que todos son soberbios y descomedidos, Ã©l solo era afable y bien criado. Pero, sobre todos, estaba bien con Reinaldos de MontalbÃ¡n, y mÃ¡s cuando le veÃ­a salir de su castillo y robar cuantos topaba, y cuando en allende robÃ³ aquel Ã­dolo de Mahoma que era todo de oro, segÃºn dice su historia. Diera Ã©l, por dar una mano de coces al traidor de GalalÃ³n, al ama que tenÃ­a, y aun a su sobrina de aÃ±adidura. En efeto, rematado ya su juicio, vino a dar en el mÃ¡s estraÃ±o pensamiento que jamÃ¡s dio loco en el mundo; y fue que le pareciÃ³ convenible y necesario, asÃ­ para el aumento de su honra como para el servicio de su repÃºblica, hacerse caballero andante, y irse por todo el mundo con sus armas y caballo a buscar las aventuras y a ejercitarse en todo aquello que Ã©l habÃ­a leÃ­do que los caballeros andantes se ejercitaban, deshaciendo todo gÃ©nero de agravio, y poniÃ©ndose en ocasiones y peligros donde, acabÃ¡ndolos, cobrase eterno nombre y fama...\")\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SentencePiece Training Options ðŸ”¤âœ‚ï¸\n",
    "\n",
    "The training options for `spm_train` can be listed using `spm_train --help`. Since the standard pip install of sentencepiece does not always include `spm_train`, here's a comprehensive list with explanations:\n",
    "\n",
    "## Basic Usage ðŸš€\n",
    "```\n",
    "Usage: spm_train [options] files\n",
    "```\n",
    "\n",
    "## Essential Options â­\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--input` | Comma-separated list of input text files | \"\" |\n",
    "| `--model_prefix` | Output model name prefix (required!) | \"\" |\n",
    "| `--model_type` | Algorithm: `unigram`, `bpe`, `word` or `char` | \"unigram\" |\n",
    "| `--vocab_size` | Size of vocabulary to create | 8000 |\n",
    "\n",
    "## Input Processing Options ðŸ“„\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--input_format` | Format: `text` or `tsv` | \"\" |\n",
    "| `--input_sentence_size` | Maximum number of sentences to load (0 = all) | 0 |\n",
    "| `--shuffle_input_sentence` | Randomly sample sentences (when using `--input_sentence_size`) | true |\n",
    "| `--max_sentence_length` | Maximum sentence length in bytes | 4192 |\n",
    "\n",
    "## Tokenization Control ðŸ§©\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--character_coverage` | % of characters to cover (higher = larger vocab) | 0.9995 |\n",
    "| `--max_sentencepiece_length` | Maximum length of each token | 16 |\n",
    "| `--split_by_unicode_script` | Split by Unicode script boundaries | true |\n",
    "| `--split_by_number` | Split tokens at numbers | true |\n",
    "| `--split_by_whitespace` | Use whitespace to split tokens | true |\n",
    "| `--split_digits` | Split each digit into separate pieces | false |\n",
    "| `--byte_fallback` | Decompose unknown characters into UTF-8 bytes | false |\n",
    "\n",
    "## Special Tokens & Symbols ðŸ·ï¸\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--control_symbols` | Comma-separated list of control symbols | \"\" |\n",
    "| `--user_defined_symbols` | Comma-separated list of user-defined symbols | \"\" |\n",
    "| `--required_chars` | Characters always included regardless of coverage | \"\" |\n",
    "| `--unk_id` | ID for unknown token (`<unk>`) | 0 |\n",
    "| `--bos_id` | ID for beginning of sentence (`<s>`) | 1 |\n",
    "| `--eos_id` | ID for end of sentence (`</s>`) | 2 |\n",
    "| `--pad_id` | ID for padding (`<pad>`) | -1 |\n",
    "\n",
    "## Text Normalization ðŸ§¹\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--normalization_rule_name` | Rule for text normalization: `nfkc` or `identity` | \"nmt_nfkc\" |\n",
    "| `--add_dummy_prefix` | Add dummy space at text beginning | true |\n",
    "| `--remove_extra_whitespaces` | Remove extra whitespace (leading, trailing, duplicate) | true |\n",
    "\n",
    "## Advanced Training Options âš™ï¸\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--num_threads` | Number of training threads | 16 |\n",
    "| `--num_sub_iterations` | Number of EM sub-iterations | 2 |\n",
    "| `--seed_sentencepiece_size` | Size of seed sentencepieces | 1000000 |\n",
    "| `--shrinking_factor` | Keep top pieces based on loss | 0.75 |\n",
    "| `--hard_vocab_limit` | Strictly enforce vocab size limit | true |\n",
    "| `--random_seed` | Random seed value | 4294967295 |\n",
    "\n",
    "## Privacy Options ðŸ”’\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--enable_differential_privacy` | Use differential privacy (DP) in training | false |\n",
    "| `--differential_privacy_noise_level` | Amount of noise for DP | 0 |\n",
    "| `--differential_privacy_clipping_threshold` | Threshold for clipping counts in DP | 0 |\n",
    "\n",
    "## Help and Debugging ðŸ”\n",
    "\n",
    "| Option | Description | Default |\n",
    "|--------|-------------|---------|\n",
    "| `--help` | Show help message | false |\n",
    "| `--version` | Show version information | false |\n",
    "| `--minloglevel` | Minimum level for logging messages | 0 |\n",
    "| `--self_test_sample_size` | Size of self-test samples | 0 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: quixote_sentencepiece.txt\n",
      "  input_format: text\n",
      "  model_prefix: tok400\n",
      "  model_type: BPE\n",
      "  vocab_size: 400\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.99995\n",
      "  input_sentence_size: 200000000\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 8384\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 1\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 1\n",
      "  required_chars: \n",
      "  byte_fallback: 1\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  â‡ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: identity\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 0\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: quixote_sentencepiece.txt\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x00>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x01>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x02>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x03>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x04>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x05>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x06>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x07>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x08>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x09>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x0F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x10>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x11>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x12>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x13>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x14>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x15>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x16>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x17>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x18>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x19>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x1F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x20>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x21>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x22>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x23>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x24>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x25>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x26>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x27>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x28>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x29>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x2F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x30>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x31>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x32>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x33>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x34>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x35>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x36>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x37>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x38>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x39>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x3F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x40>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x41>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x42>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x43>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x44>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x45>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x46>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x47>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x48>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x49>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x4F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x50>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x51>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x52>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x53>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x54>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x55>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x56>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x57>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x58>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x59>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x5F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x60>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x61>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x62>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x63>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x64>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x65>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x66>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x67>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x68>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x69>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x6F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x70>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x71>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x72>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x73>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x74>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x75>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x76>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x77>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x78>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x79>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x7F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x80>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x81>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x82>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x83>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x84>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x85>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x86>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x87>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x88>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x89>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x8F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x90>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x91>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x92>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x93>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x94>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x95>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x96>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x97>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x98>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x99>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9A>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9B>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9C>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9D>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9E>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0x9F>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xA9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xAF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xB9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xBF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xC9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xCF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xD9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xDF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xE9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xED>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xEF>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF0>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF1>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF2>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF3>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF4>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF5>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF6>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF7>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF8>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xF9>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFA>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFB>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFC>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFD>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFE>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <0xFF>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=5592\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=56\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 507\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=121 min_freq=1\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=43 size=20 all=675 active=619 piece=an\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=21 size=40 all=960 active=904 piece=â–le\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13 size=60 all=1106 active=1050 piece=â–no\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9 size=80 all=1199 active=1143 piece=â–caball\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: tok400.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: tok400.vocab\n"
     ]
    }
   ],
   "source": [
    "# train a sentencepiece model on it\n",
    "# the settings here are (best effort) those used for training Llama 2\n",
    "import os\n",
    "\n",
    "options = dict(\n",
    "  # input spec\n",
    "  input=\"quixote_sentencepiece.txt\",\n",
    "  input_format=\"text\",\n",
    "  # output spec\n",
    "  model_prefix=\"tok400\", # output filename prefix\n",
    "  # algorithm spec\n",
    "  # BPE alg\n",
    "  model_type=\"bpe\",\n",
    "  vocab_size=400,\n",
    "  # normalization\n",
    "  normalization_rule_name=\"identity\", # ew, turn off normalization\n",
    "  remove_extra_whitespaces=False,\n",
    "  input_sentence_size=200000000, # max number of training sentences\n",
    "  max_sentence_length=8384, # max number of bytes per sentence\n",
    "  seed_sentencepiece_size=1000000,\n",
    "  shuffle_input_sentence=True,\n",
    "  # rare word treatment\n",
    "  character_coverage=0.99995,\n",
    "  byte_fallback=True,\n",
    "  # merge rules\n",
    "  split_digits=True,\n",
    "  split_by_unicode_script=True,\n",
    "  split_by_whitespace=True,\n",
    "  split_by_number=True,\n",
    "  max_sentencepiece_length=16,\n",
    "  add_dummy_prefix=True,\n",
    "  allow_whitespace_only_pieces=True,\n",
    "  # special tokens\n",
    "  unk_id=0, # the UNK token MUST exist\n",
    "  bos_id=1, # the others are optional, set to -1 to turn off\n",
    "  eos_id=2,\n",
    "  pad_id=-1,\n",
    "  # systems\n",
    "  num_threads=os.cpu_count(), # use ~all system resources\n",
    ")\n",
    "\n",
    "# Check if the input file exists and is not empty\n",
    "if os.path.exists(options['input']) and os.path.getsize(options['input']) > 0:\n",
    "    spm.SentencePieceTrainer.train(**options)\n",
    "else:\n",
    "    print(f\"Error: The input file {options['input']} does not exist or is empty.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<unk>', 0],\n",
       " ['<s>', 1],\n",
       " ['</s>', 2],\n",
       " ['<0x00>', 3],\n",
       " ['<0x01>', 4],\n",
       " ['<0x02>', 5],\n",
       " ['<0x03>', 6],\n",
       " ['<0x04>', 7],\n",
       " ['<0x05>', 8],\n",
       " ['<0x06>', 9],\n",
       " ['<0x07>', 10],\n",
       " ['<0x08>', 11],\n",
       " ['<0x09>', 12],\n",
       " ['<0x0A>', 13],\n",
       " ['<0x0B>', 14],\n",
       " ['<0x0C>', 15],\n",
       " ['<0x0D>', 16],\n",
       " ['<0x0E>', 17],\n",
       " ['<0x0F>', 18],\n",
       " ['<0x10>', 19],\n",
       " ['<0x11>', 20],\n",
       " ['<0x12>', 21],\n",
       " ['<0x13>', 22],\n",
       " ['<0x14>', 23],\n",
       " ['<0x15>', 24],\n",
       " ['<0x16>', 25],\n",
       " ['<0x17>', 26],\n",
       " ['<0x18>', 27],\n",
       " ['<0x19>', 28],\n",
       " ['<0x1A>', 29],\n",
       " ['<0x1B>', 30],\n",
       " ['<0x1C>', 31],\n",
       " ['<0x1D>', 32],\n",
       " ['<0x1E>', 33],\n",
       " ['<0x1F>', 34],\n",
       " ['<0x20>', 35],\n",
       " ['<0x21>', 36],\n",
       " ['<0x22>', 37],\n",
       " ['<0x23>', 38],\n",
       " ['<0x24>', 39],\n",
       " ['<0x25>', 40],\n",
       " ['<0x26>', 41],\n",
       " ['<0x27>', 42],\n",
       " ['<0x28>', 43],\n",
       " ['<0x29>', 44],\n",
       " ['<0x2A>', 45],\n",
       " ['<0x2B>', 46],\n",
       " ['<0x2C>', 47],\n",
       " ['<0x2D>', 48],\n",
       " ['<0x2E>', 49],\n",
       " ['<0x2F>', 50],\n",
       " ['<0x30>', 51],\n",
       " ['<0x31>', 52],\n",
       " ['<0x32>', 53],\n",
       " ['<0x33>', 54],\n",
       " ['<0x34>', 55],\n",
       " ['<0x35>', 56],\n",
       " ['<0x36>', 57],\n",
       " ['<0x37>', 58],\n",
       " ['<0x38>', 59],\n",
       " ['<0x39>', 60],\n",
       " ['<0x3A>', 61],\n",
       " ['<0x3B>', 62],\n",
       " ['<0x3C>', 63],\n",
       " ['<0x3D>', 64],\n",
       " ['<0x3E>', 65],\n",
       " ['<0x3F>', 66],\n",
       " ['<0x40>', 67],\n",
       " ['<0x41>', 68],\n",
       " ['<0x42>', 69],\n",
       " ['<0x43>', 70],\n",
       " ['<0x44>', 71],\n",
       " ['<0x45>', 72],\n",
       " ['<0x46>', 73],\n",
       " ['<0x47>', 74],\n",
       " ['<0x48>', 75],\n",
       " ['<0x49>', 76],\n",
       " ['<0x4A>', 77],\n",
       " ['<0x4B>', 78],\n",
       " ['<0x4C>', 79],\n",
       " ['<0x4D>', 80],\n",
       " ['<0x4E>', 81],\n",
       " ['<0x4F>', 82],\n",
       " ['<0x50>', 83],\n",
       " ['<0x51>', 84],\n",
       " ['<0x52>', 85],\n",
       " ['<0x53>', 86],\n",
       " ['<0x54>', 87],\n",
       " ['<0x55>', 88],\n",
       " ['<0x56>', 89],\n",
       " ['<0x57>', 90],\n",
       " ['<0x58>', 91],\n",
       " ['<0x59>', 92],\n",
       " ['<0x5A>', 93],\n",
       " ['<0x5B>', 94],\n",
       " ['<0x5C>', 95],\n",
       " ['<0x5D>', 96],\n",
       " ['<0x5E>', 97],\n",
       " ['<0x5F>', 98],\n",
       " ['<0x60>', 99],\n",
       " ['<0x61>', 100],\n",
       " ['<0x62>', 101],\n",
       " ['<0x63>', 102],\n",
       " ['<0x64>', 103],\n",
       " ['<0x65>', 104],\n",
       " ['<0x66>', 105],\n",
       " ['<0x67>', 106],\n",
       " ['<0x68>', 107],\n",
       " ['<0x69>', 108],\n",
       " ['<0x6A>', 109],\n",
       " ['<0x6B>', 110],\n",
       " ['<0x6C>', 111],\n",
       " ['<0x6D>', 112],\n",
       " ['<0x6E>', 113],\n",
       " ['<0x6F>', 114],\n",
       " ['<0x70>', 115],\n",
       " ['<0x71>', 116],\n",
       " ['<0x72>', 117],\n",
       " ['<0x73>', 118],\n",
       " ['<0x74>', 119],\n",
       " ['<0x75>', 120],\n",
       " ['<0x76>', 121],\n",
       " ['<0x77>', 122],\n",
       " ['<0x78>', 123],\n",
       " ['<0x79>', 124],\n",
       " ['<0x7A>', 125],\n",
       " ['<0x7B>', 126],\n",
       " ['<0x7C>', 127],\n",
       " ['<0x7D>', 128],\n",
       " ['<0x7E>', 129],\n",
       " ['<0x7F>', 130],\n",
       " ['<0x80>', 131],\n",
       " ['<0x81>', 132],\n",
       " ['<0x82>', 133],\n",
       " ['<0x83>', 134],\n",
       " ['<0x84>', 135],\n",
       " ['<0x85>', 136],\n",
       " ['<0x86>', 137],\n",
       " ['<0x87>', 138],\n",
       " ['<0x88>', 139],\n",
       " ['<0x89>', 140],\n",
       " ['<0x8A>', 141],\n",
       " ['<0x8B>', 142],\n",
       " ['<0x8C>', 143],\n",
       " ['<0x8D>', 144],\n",
       " ['<0x8E>', 145],\n",
       " ['<0x8F>', 146],\n",
       " ['<0x90>', 147],\n",
       " ['<0x91>', 148],\n",
       " ['<0x92>', 149],\n",
       " ['<0x93>', 150],\n",
       " ['<0x94>', 151],\n",
       " ['<0x95>', 152],\n",
       " ['<0x96>', 153],\n",
       " ['<0x97>', 154],\n",
       " ['<0x98>', 155],\n",
       " ['<0x99>', 156],\n",
       " ['<0x9A>', 157],\n",
       " ['<0x9B>', 158],\n",
       " ['<0x9C>', 159],\n",
       " ['<0x9D>', 160],\n",
       " ['<0x9E>', 161],\n",
       " ['<0x9F>', 162],\n",
       " ['<0xA0>', 163],\n",
       " ['<0xA1>', 164],\n",
       " ['<0xA2>', 165],\n",
       " ['<0xA3>', 166],\n",
       " ['<0xA4>', 167],\n",
       " ['<0xA5>', 168],\n",
       " ['<0xA6>', 169],\n",
       " ['<0xA7>', 170],\n",
       " ['<0xA8>', 171],\n",
       " ['<0xA9>', 172],\n",
       " ['<0xAA>', 173],\n",
       " ['<0xAB>', 174],\n",
       " ['<0xAC>', 175],\n",
       " ['<0xAD>', 176],\n",
       " ['<0xAE>', 177],\n",
       " ['<0xAF>', 178],\n",
       " ['<0xB0>', 179],\n",
       " ['<0xB1>', 180],\n",
       " ['<0xB2>', 181],\n",
       " ['<0xB3>', 182],\n",
       " ['<0xB4>', 183],\n",
       " ['<0xB5>', 184],\n",
       " ['<0xB6>', 185],\n",
       " ['<0xB7>', 186],\n",
       " ['<0xB8>', 187],\n",
       " ['<0xB9>', 188],\n",
       " ['<0xBA>', 189],\n",
       " ['<0xBB>', 190],\n",
       " ['<0xBC>', 191],\n",
       " ['<0xBD>', 192],\n",
       " ['<0xBE>', 193],\n",
       " ['<0xBF>', 194],\n",
       " ['<0xC0>', 195],\n",
       " ['<0xC1>', 196],\n",
       " ['<0xC2>', 197],\n",
       " ['<0xC3>', 198],\n",
       " ['<0xC4>', 199],\n",
       " ['<0xC5>', 200],\n",
       " ['<0xC6>', 201],\n",
       " ['<0xC7>', 202],\n",
       " ['<0xC8>', 203],\n",
       " ['<0xC9>', 204],\n",
       " ['<0xCA>', 205],\n",
       " ['<0xCB>', 206],\n",
       " ['<0xCC>', 207],\n",
       " ['<0xCD>', 208],\n",
       " ['<0xCE>', 209],\n",
       " ['<0xCF>', 210],\n",
       " ['<0xD0>', 211],\n",
       " ['<0xD1>', 212],\n",
       " ['<0xD2>', 213],\n",
       " ['<0xD3>', 214],\n",
       " ['<0xD4>', 215],\n",
       " ['<0xD5>', 216],\n",
       " ['<0xD6>', 217],\n",
       " ['<0xD7>', 218],\n",
       " ['<0xD8>', 219],\n",
       " ['<0xD9>', 220],\n",
       " ['<0xDA>', 221],\n",
       " ['<0xDB>', 222],\n",
       " ['<0xDC>', 223],\n",
       " ['<0xDD>', 224],\n",
       " ['<0xDE>', 225],\n",
       " ['<0xDF>', 226],\n",
       " ['<0xE0>', 227],\n",
       " ['<0xE1>', 228],\n",
       " ['<0xE2>', 229],\n",
       " ['<0xE3>', 230],\n",
       " ['<0xE4>', 231],\n",
       " ['<0xE5>', 232],\n",
       " ['<0xE6>', 233],\n",
       " ['<0xE7>', 234],\n",
       " ['<0xE8>', 235],\n",
       " ['<0xE9>', 236],\n",
       " ['<0xEA>', 237],\n",
       " ['<0xEB>', 238],\n",
       " ['<0xEC>', 239],\n",
       " ['<0xED>', 240],\n",
       " ['<0xEE>', 241],\n",
       " ['<0xEF>', 242],\n",
       " ['<0xF0>', 243],\n",
       " ['<0xF1>', 244],\n",
       " ['<0xF2>', 245],\n",
       " ['<0xF3>', 246],\n",
       " ['<0xF4>', 247],\n",
       " ['<0xF5>', 248],\n",
       " ['<0xF6>', 249],\n",
       " ['<0xF7>', 250],\n",
       " ['<0xF8>', 251],\n",
       " ['<0xF9>', 252],\n",
       " ['<0xFA>', 253],\n",
       " ['<0xFB>', 254],\n",
       " ['<0xFC>', 255],\n",
       " ['<0xFD>', 256],\n",
       " ['<0xFE>', 257],\n",
       " ['<0xFF>', 258],\n",
       " ['â–d', 259],\n",
       " ['â–de', 260],\n",
       " ['en', 261],\n",
       " ['â–l', 262],\n",
       " ['â–c', 263],\n",
       " ['er', 264],\n",
       " ['ue', 265],\n",
       " ['â–a', 266],\n",
       " ['â–s', 267],\n",
       " ['os', 268],\n",
       " ['que', 269],\n",
       " ['es', 270],\n",
       " ['ra', 271],\n",
       " ['â–p', 272],\n",
       " ['as', 273],\n",
       " ['â–y', 274],\n",
       " ['â–que', 275],\n",
       " ['ab', 276],\n",
       " ['â–m', 277],\n",
       " ['an', 278],\n",
       " ['to', 279],\n",
       " ['do', 280],\n",
       " ['el', 281],\n",
       " ['al', 282],\n",
       " ['on', 283],\n",
       " ['â–en', 284],\n",
       " ['ar', 285],\n",
       " ['â–h', 286],\n",
       " ['ci', 287],\n",
       " ['in', 288],\n",
       " ['Ã­a', 289],\n",
       " ['or', 290],\n",
       " ['â–el', 291],\n",
       " ['om', 292],\n",
       " ['un', 293],\n",
       " ['â–n', 294],\n",
       " ['â–v', 295],\n",
       " ['â–con', 296],\n",
       " ['â–la', 297],\n",
       " ['â–le', 298],\n",
       " ['â–su', 299],\n",
       " ['ad', 300],\n",
       " ['â–se', 301],\n",
       " ['â–t', 302],\n",
       " ['br', 303],\n",
       " ['aba', 304],\n",
       " ['â–to', 305],\n",
       " ['ll', 306],\n",
       " ['Ã³n', 307],\n",
       " ['â–r', 308],\n",
       " ['ero', 309],\n",
       " ['â–los', 310],\n",
       " ['all', 311],\n",
       " ['era', 312],\n",
       " ['â–cu', 313],\n",
       " ['ant', 314],\n",
       " ['â–es', 315],\n",
       " ['id', 316],\n",
       " ['ent', 317],\n",
       " ['â–no', 318],\n",
       " ['tr', 319],\n",
       " ['ara', 320],\n",
       " ['ch', 321],\n",
       " ['â–b', 322],\n",
       " ['â–f', 323],\n",
       " ['â–g', 324],\n",
       " ['â–al', 325],\n",
       " ['â–mu', 326],\n",
       " ['â–com', 327],\n",
       " ['â–por', 328],\n",
       " ['aball', 329],\n",
       " ['am', 330],\n",
       " ['ec', 331],\n",
       " ['Ã¡s', 332],\n",
       " ['â–des', 333],\n",
       " ['â–aque', 334],\n",
       " ['le', 335],\n",
       " ['â–so', 336],\n",
       " ['â–todo', 337],\n",
       " ['â–caball', 338],\n",
       " ['ic', 339],\n",
       " ['oc', 340],\n",
       " ['ri', 341],\n",
       " ['ui', 342],\n",
       " ['bre', 343],\n",
       " ['â–', 344],\n",
       " ['e', 345],\n",
       " ['a', 346],\n",
       " ['o', 347],\n",
       " ['s', 348],\n",
       " ['n', 349],\n",
       " ['l', 350],\n",
       " ['r', 351],\n",
       " ['d', 352],\n",
       " ['u', 353],\n",
       " ['i', 354],\n",
       " ['c', 355],\n",
       " ['t', 356],\n",
       " ['m', 357],\n",
       " [',', 358],\n",
       " ['b', 359],\n",
       " ['q', 360],\n",
       " ['p', 361],\n",
       " ['y', 362],\n",
       " ['Ã­', 363],\n",
       " ['g', 364],\n",
       " ['h', 365],\n",
       " ['v', 366],\n",
       " ['Ã³', 367],\n",
       " ['.', 368],\n",
       " ['f', 369],\n",
       " ['j', 370],\n",
       " ['z', 371],\n",
       " ['Ã¡', 372],\n",
       " ['Ã©', 373],\n",
       " [';', 374],\n",
       " ['Ã±', 375],\n",
       " ['E', 376],\n",
       " ['A', 377],\n",
       " ['C', 378],\n",
       " ['M', 379],\n",
       " ['D', 380],\n",
       " ['G', 381],\n",
       " ['P', 382],\n",
       " ['Q', 383],\n",
       " ['R', 384],\n",
       " [':', 385],\n",
       " ['F', 386],\n",
       " ['T', 387],\n",
       " ['Ãº', 388],\n",
       " ['-', 389],\n",
       " ['B', 390],\n",
       " ['L', 391],\n",
       " ['N', 392],\n",
       " ['S', 393],\n",
       " ['Y', 394],\n",
       " ['H', 395],\n",
       " ['I', 396],\n",
       " ['U', 397],\n",
       " ['x', 398],\n",
       " ['Ã¼', 399]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('tok400.model')\n",
    "vocab = [[sp.id_to_piece(idx), idx] for idx in range(sp.get_piece_size())]\n",
    "vocab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# makemore: part 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8134 words\n",
      "['Alegr√≠a-Dulantzi']\n",
      "['alegria-dulantzi']\n"
     ]
    }
   ],
   "source": [
    "#Data load. No changes here\n",
    "\n",
    "#https://datos.gob.es/es/catalogo/a09002970-municipios-de-espana\n",
    "# We will instead be using names of villages/cities in Spain. Only 8k data\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV data\n",
    "df = pd.read_csv(\"Municipis_d_Espanya.csv\", sep=\",\")\n",
    "\n",
    "# Function to clean the names\n",
    "def clean_name(name):\n",
    "    # If there's a slash, take the first part\n",
    "    name = name.split('/')[0]\n",
    "    # If it's in \"Last, First\" format, swap it to \"First Last\"\n",
    "    if ',' in name:\n",
    "        parts = name.split(', ')\n",
    "        if len(parts) == 2:\n",
    "            name = f\"{parts[1]} {parts[0]}\"\n",
    "    return name\n",
    "\n",
    "# Apply the function to clean names\n",
    "df[\"Nom\"] = df[\"Nom\"].apply(clean_name)\n",
    "\n",
    "# Extract only the 'Territorio' column as a list\n",
    "words = df[\"Nom\"].tolist()\n",
    "\n",
    "print(f\"{len(words)} words\")\n",
    "\n",
    "#Simplifying the problem (lowercase and no accents)\n",
    "import unidecode\n",
    "import re\n",
    "\n",
    "print(words[:1])\n",
    "words = [re.sub(r'[\\(\\)\\'\"]', '', unidecode.unidecode(word).lower()) for word in words]\n",
    "print(words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: ' ', 2: '-', 3: 'a', 4: 'b', 5: 'c', 6: 'd', 7: 'e', 8: 'f', 9: 'g', 10: 'h', 11: 'i', 12: 'j', 13: 'k', 14: 'l', 15: 'm', 16: 'n', 17: 'o', 18: 'p', 19: 'q', 20: 'r', 21: 's', 22: 't', 23: 'u', 24: 'v', 25: 'w', 26: 'x', 27: 'y', 28: 'z', 0: '.'}\n",
      "29\n"
     ]
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "vocab_size = len(itos)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([85032, 3]) torch.Size([85032])\n",
      "torch.Size([10606, 3]) torch.Size([10606])\n",
      "torch.Size([10768, 3]) torch.Size([10768])\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one?\n",
    "\n",
    "def build_dataset(words):  \n",
    "  X, Y = [], []\n",
    "  \n",
    "  for w in words:\n",
    "    context = [0] * block_size\n",
    "    for ch in w + '.':\n",
    "      ix = stoi[ch]\n",
    "      X.append(context)\n",
    "      Y.append(ix)\n",
    "      context = context[1:] + [ix] # crop and append\n",
    "\n",
    "  X = torch.tensor(X)\n",
    "  Y = torch.tensor(Y)\n",
    "  print(X.shape, Y.shape)\n",
    "  return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr,  Ytr  = build_dataset(words[:n1])     # 80%\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])   # 10%\n",
    "Xte,  Yte  = build_dataset(words[n2:])     # 10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No changes until here\n",
    "_____________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
    "def compare_to_pytorch(label, manual_calculated, tensor):\n",
    "    # Check if the values are exactly the same\n",
    "    is_exact = torch.all(manual_calculated == tensor.grad).item()\n",
    "\n",
    "    # Check if the values are approximately the same\n",
    "    is_approximate = torch.allclose(manual_calculated, tensor.grad)\n",
    "\n",
    "    # Calculate the maximum difference between the expected values and the gradients\n",
    "    max_difference = (manual_calculated - tensor.grad).abs().max().item()\n",
    "\n",
    "    # Print the results\n",
    "    print(f'{label:15s} | exactly equal: {str(is_exact):5s} | approximatly equal: {str(is_approximate):5s} | larges difference: {max_difference}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18619\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 64 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C  = torch.randn((vocab_size, n_embd),            generator=g)\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3)/((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden,                        generator=g) * 0.1 # using b1 just for fun, it's useless because of BN\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size),          generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size,                      generator=g) * 0.1\n",
    "# BatchNorm parameters\n",
    "bngain = torch.randn((1, n_hidden))*0.1 + 1.0\n",
    "bnbias = torch.randn((1, n_hidden))*0.1\n",
    "\n",
    "# Note: I am initializating many of these parameters in non-standard ways\n",
    "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
    "# implementation of the backward pass.\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# construct a minibatch\n",
    "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = Xtr[ix], Ytr[ix] # batch X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0208, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Embedding the input characters into vectors\n",
    "embeddings = C[Xb]  # C is the embedding matrix, Xb are the input indices\n",
    "flattened_embeddings = embeddings.view(embeddings.shape[0], -1)  # Flatten embeddings to 2D: (batch_size, embedding_dim)\n",
    "\n",
    "# Step 2: Linear Layer 1 (pre-activation)\n",
    "hidden_pre_bn = flattened_embeddings @ W1 + b1  # Linear transformation (batch_size, hidden_size)\n",
    "\n",
    "# Step 3: Batch Normalization (BN)\n",
    "batch_mean = 1 / batch_size * hidden_pre_bn.sum(0, keepdim=True)  # Calculate batch mean (1, hidden_size)\n",
    "centered_bn = hidden_pre_bn - batch_mean  # Subtract the mean from each hidden pre-activation value\n",
    "\n",
    "batch_variance = 1 / (batch_size - 1) * (centered_bn**2).sum(0, keepdim=True)  # Calculate batch variance (1, hidden_size)\n",
    "batch_variance_inv = (batch_variance + 1e-5)**-0.5  # Inverse of the standard deviation (1, hidden_size)\n",
    "normalized_bn = centered_bn * batch_variance_inv  # Normalize the batch (batch_size, hidden_size)\n",
    "\n",
    "# Step 4: Apply Scale and Shift (Gamma and Beta) to Batch Normalization\n",
    "hidden_pre_activation = bngain * normalized_bn + bnbias  # Apply learnable scaling (gamma) and shifting (beta)\n",
    "\n",
    "# Step 5: Non-linearity (activation)\n",
    "hidden = torch.tanh(hidden_pre_activation)  # Apply the tanh activation function\n",
    "\n",
    "# Step 6: Linear Layer 2 (output layer)\n",
    "logits = hidden @ W2 + b2  # Final linear transformation to produce logits (batch_size, output_dim)\n",
    "\n",
    "# Step 7: Cross Entropy Loss (manual calculation)\n",
    "logit_max = logits.max(1, keepdim=True).values  # For numerical stability, subtract the max logit\n",
    "stable_logits = logits - logit_max  # Subtract max logits to prevent overflow during exponentiation\n",
    "\n",
    "# Calculate probabilities (softmax)\n",
    "exp_logits = stable_logits.exp()  # Exponentiate the logits to get unnormalized probabilities\n",
    "logits_sum = exp_logits.sum(1, keepdim=True)  # Sum of exponentiated logits (batch_size, 1)\n",
    "inv_logits_sum = logits_sum**-1  # Inverse of the sum for normalization (batch_size, 1)\n",
    "probabilities = exp_logits * inv_logits_sum  # Normalize to get the actual probabilities (batch_size, output_dim)\n",
    "\n",
    "# Compute log probabilities\n",
    "log_probabilities = probabilities.log()  # Logarithm of probabilities (batch_size, output_dim)\n",
    "\n",
    "# Calculate the cross entropy loss\n",
    "loss = -log_probabilities[range(batch_size), Yb].mean()  # Average the negative log likelihood for the true labels\n",
    "\n",
    "# Step 8: Backward Pass\n",
    "for param in parameters:\n",
    "    param.grad = None  # Clear previous gradients\n",
    "\n",
    "# Retain gradients for intermediate variables for debugging or inspection\n",
    "for tensor in [log_probabilities, probabilities, exp_logits, logits_sum, inv_logits_sum,\n",
    "               stable_logits, logit_max, logits, hidden, hidden_pre_activation, normalized_bn,\n",
    "               batch_variance_inv, batch_variance, centered_bn, batch_mean, hidden_pre_bn, flattened_embeddings, embeddings]:\n",
    "    tensor.retain_grad()  # Retain gradients for inspection if needed\n",
    "\n",
    "# Perform backpropagation to compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Return the final loss value\n",
    "loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "## backprop through the whole thing manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "log_probabilities\n",
    "probabilities\n",
    "inv_logits_sum\n",
    "logits_sum\n",
    "exp_logits\n",
    "stable_logits\n",
    "logit_max\n",
    "logits\n",
    "hidden\n",
    "hidden_pre_activation\n",
    "normalized_bn\n",
    "batch_variance_inv\n",
    "batch_variance\n",
    "centered_bn\n",
    "batch_mean\n",
    "flattened_embeddings\n",
    "embeddings\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 29])\n",
      "log_probabilities | exactly equal: True  | approximatly equal: True  | larges difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "#loss = -logprobabilities[range(batch_size), Yb].mean()\n",
    "#loss = -1*(sum(elements))/n_elements\n",
    "#dloss= -1/n_elements\n",
    "\n",
    "print(log_probabilities.shape)\n",
    "\n",
    "d_log_probabilities=torch.zeros_like(log_probabilities)\n",
    "d_log_probabilities[range(batch_size), Yb] = -1/batch_size\n",
    "\n",
    "compare_to_pytorch('log_probabilities', d_log_probabilities, log_probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probabilities   | exactly equal: True  | approximatly equal: True  | larges difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "#log_probabilities = probabilities.log()\n",
    "#d/dx(log(x)) = 1/x\n",
    "\n",
    "d_probabilities=(1/probabilities)*d_log_probabilities #examples with low prob --> boost grad\n",
    "\n",
    "compare_to_pytorch('probabilities', d_probabilities, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 29])\n",
      "torch.Size([64, 1])\n",
      "torch.Size([64, 29])\n",
      "Inv logits sum  | exactly equal: True  | approximatly equal: True  | larges difference: 0.0\n"
     ]
    }
   ],
   "source": [
    "#probabilities = exp_logits * inv_logits_sum  # Normalize to get the actual probabilities (batch_size, output_dim)\n",
    "#2 ops really\n",
    "#   replicate column tensor\n",
    "#   multiplication\n",
    "\n",
    "\n",
    "print(exp_logits.shape)\n",
    "print(inv_logits_sum.shape)\n",
    "\n",
    "# c = a * b, but with tensors:\n",
    "# a[3x3] * b[3,1]  ---->\n",
    "# a11*b1  a12*b1  a13*b1\n",
    "# a21*b2  a22*b2  a23*b2\n",
    "# a31*b3  a32*b3  a33*b3\n",
    "# c[3x3]\n",
    "\n",
    "\n",
    "print(d_probabilities.shape)\n",
    "\n",
    "d_inv_logis_sum=(exp_logits*d_probabilities).sum(1,keepdim=True)\n",
    "\n",
    "\n",
    "compare_to_pytorch('Inv logits sum', d_inv_logis_sum, inv_logits_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exp_logits = stable_logits.exp()  # Exponentiate the logits to get unnormalized probabilities\n",
    "#logits_sum = exp_logits.sum(1, keepdim=True)  # Sum of exponentiated logits (batch_size, 1)\n",
    "#inv_logits_sum = logits_sum**-1  # Inverse of the sum for normalization (batch_size, 1)\n",
    "#probabilities = exp_logits * inv_logits_sum  # Normalize to get the actual probabilities (batch_size, output_dim)\n",
    "\n",
    "#Exp logits its used twice\n",
    "\n",
    "\n",
    "d_exp_logitcs=(inv_logits_sum*d_probabilities)+\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
